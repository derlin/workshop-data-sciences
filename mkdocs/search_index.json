{
    "docs": [
        {
            "location": "/", 
            "text": "D\u00e9j\u00e0 entendu parl\u00e9 de Hadoop, Spark, Flink ou HDFS\u00a0? Curieux de savoir enfin de quoi il retourne\u00a0?\n\n\n\n\nThis workshop takes place at  UNIFR / HEIA-FR the 19\nth\n and 20\nth\n of October 2017. It is organised by IT-Valley in collaboration with the DAPLAB. It is intended for any developer willing to familiarize with the Big Data and Data Sciences technologies.\n\n\nIt will be given in French, with support in English.\n\n\nSchedule\n\n\nDay 1\n:\n\n\n\n\nIntroduction: Big Data, Hadoop, HDFS, what are they ?\n\n\nHDFS: filesystem and command line usage\n\n\nMapReduce:\n\n\ntheory\n: what is MapReduce ?\n\n\npractice\n: My first MapReduce application (java)\n\n\n\n\n\n\nHive:\n\n\ntheory\n: what is Hive?\n\n\npractice\n: Querying and manipulating data using Hive\n\n\n\n\n\n\n\n\nDay 2\n:\n\n\n\n\nSpark and Zeppelin:\n\n\ngetting started with Zeppelin (python + pyspark)\n\n\nSpark SQL: quick data discovery and visualisation (python + pyspark)\n\n\n\n\n\n\nLSA, \nLatent Semantic Analysis\n:\n\n\ntheory\n: what is it and what is it for ?\n\n\npractice\n: implementing a document search engine using LDA, \nLatent Dirichlet Allocation\n (python + pyspark)\n\n\n\n\n\n\nif there is some time left, a little tour of the DAPLAB\n\n\n\n\nRequirements\n\n\nFor the workshop, you will need the following:\n\n\n\n\na laptop (Recommended: Mac or Linux)\n\n\nthe Java JDK version 8 or above\n\n\na java IDE: \n\n\nif you don't already have one, please install the \nIntelliJ Community Edition\n\n\n\n\n\n\nMaven:\n\n\nIf you have IntelliJ, you can skip this step as it already ships with an embedded Maven\n\n\nIf you want Maven available from the command line as well, follow the installation instructions on the \nMaven website\n\n\n\n\n\n\nDocker:\n\n\nWindows\n installation guide\n\n\nMac\n installation guide\n\n\n\n\n\n\nSnorkel:\n\n\nfollow the instruction at \nhttps://github.com/Sqooba/snorkel\n or have a look at the section below.\n\n\n\n\n\n\n\n\nSetting up Snorkel\n\n\n\n\nYou need Docker running on your machine. \n\n\n\n\nSnorkel is a docker container allowing you to run Zeppelin locally.\n\n\n\n\n\n\nDownload snorkel: \n\n\n\n\nIf you have git installed, clone the following repository: \n  \ngit clone https://github.com/Sqooba/snorkel.git\n\n\n\nIf you don't have git installed, go to \nhttps://github.com/Sqooba/snorkel\n \n  and select \nClone or download \n Download ZIP\n, then unzip.\n\n\n\n\n\n\n\n\nopen a command line and navigate inside the snorkel folder;\n\n\n\n\n\n\nbuild the zeppelin image:\n\n\n\n\nOn Mac/Linux:\n    \n./build-images.sh\n\n\n\nOn Windows (from the command prompt or the powershell):\n    \ncd zeppelin\ndocker build . -t sqooba/zeppelin-starter\ncd ..\n\n\n\n\n\n\n\n\n\nstart zeppelin:\n\n\n\n\nOn Mac/Linux:\n    \n./start-zeppelin.sh\n\n\n\nOn Windows (from the command prompt or the powershell):\n    \n./start-zeppelin.bat\n\n\n\n\n\n\n\n\n\ncheck that zeppelin is running: \n\n\n\n\ngo to \nhttp://localhost:8080/\n, you should have a zeppelin welcome screen;\n\n\n\n\n\n\n\n\nstop zeppelin:\n\n\n\n\nOn Mac/Linux:\n    \n./stop-zeppelin.sh\n\n\n\nOn Windows (from the command prompt or the powershell):\n    \n./stop-zeppelin.bat", 
            "title": "Home"
        }, 
        {
            "location": "/#schedule", 
            "text": "Day 1 :   Introduction: Big Data, Hadoop, HDFS, what are they ?  HDFS: filesystem and command line usage  MapReduce:  theory : what is MapReduce ?  practice : My first MapReduce application (java)    Hive:  theory : what is Hive?  practice : Querying and manipulating data using Hive     Day 2 :   Spark and Zeppelin:  getting started with Zeppelin (python + pyspark)  Spark SQL: quick data discovery and visualisation (python + pyspark)    LSA,  Latent Semantic Analysis :  theory : what is it and what is it for ?  practice : implementing a document search engine using LDA,  Latent Dirichlet Allocation  (python + pyspark)    if there is some time left, a little tour of the DAPLAB", 
            "title": "Schedule"
        }, 
        {
            "location": "/#requirements", 
            "text": "For the workshop, you will need the following:   a laptop (Recommended: Mac or Linux)  the Java JDK version 8 or above  a java IDE:   if you don't already have one, please install the  IntelliJ Community Edition    Maven:  If you have IntelliJ, you can skip this step as it already ships with an embedded Maven  If you want Maven available from the command line as well, follow the installation instructions on the  Maven website    Docker:  Windows  installation guide  Mac  installation guide    Snorkel:  follow the instruction at  https://github.com/Sqooba/snorkel  or have a look at the section below.", 
            "title": "Requirements"
        }, 
        {
            "location": "/#setting-up-snorkel", 
            "text": "You need Docker running on your machine.    Snorkel is a docker container allowing you to run Zeppelin locally.    Download snorkel:    If you have git installed, clone the following repository: \n   git clone https://github.com/Sqooba/snorkel.git  If you don't have git installed, go to  https://github.com/Sqooba/snorkel  \n  and select  Clone or download   Download ZIP , then unzip.     open a command line and navigate inside the snorkel folder;    build the zeppelin image:   On Mac/Linux:\n     ./build-images.sh  On Windows (from the command prompt or the powershell):\n     cd zeppelin\ndocker build . -t sqooba/zeppelin-starter\ncd ..     start zeppelin:   On Mac/Linux:\n     ./start-zeppelin.sh  On Windows (from the command prompt or the powershell):\n     ./start-zeppelin.bat     check that zeppelin is running:    go to  http://localhost:8080/ , you should have a zeppelin welcome screen;     stop zeppelin:   On Mac/Linux:\n     ./stop-zeppelin.sh  On Windows (from the command prompt or the powershell):\n     ./stop-zeppelin.bat", 
            "title": "Setting up Snorkel"
        }, 
        {
            "location": "/hdfs/", 
            "text": "In order to use HADOOP, it is crucial that you understand the basic functioning of HDFS, as well as some of its constraints.\nAfter a brief introduction of core HDFS concepts, this page presents \ncopy-paste\n-like tutorial to familiarize with\n\nHDFS commands\n.\nIt mainly focuses on user commands (uploading and downloading data into HDFS).\n\n\nResources\n\n\nWhile the source of truth for HDFS commands is the code source, the \ndocumentation page describing the \nhdfs dfs\n commands\n is really useful.\n\n\nA good and simpler cheat sheet is also available \nhere\n.\n\n\nIntroduction\n\n\nHDFS (\nHadoop Distributed File System\n) is one of the core components of HADOOP.\n\n\nThe HDFS is a distributed file system designed to run on commodity hardware. Very powerful,\nit should ensure that data are replicated across a wide variety of nodes, making the system\nfault tolerant and suitable for large data sets and gives high throughput.\n\n\n\n\nTip\n\n\nTo have a better understanding of how HDFS works, we strongly encourage you to check out \nthe HDFS Architecture Guide\n.\n\n\n\n\nSome remarks on HDFS\n\n\nHDFS uses a \nsimple coherency model\n: applications mostly need a \nwrite-once-read-many\n access model for files. As a result, a file once created, written to and closed becomes read-only. It is possible to append to an HDFS file only if the system was explicitly configured to.\n\n\nHDFS is tuned to deal with \nlarge files\n. A typical file in HDFS is gigabytes to terabytes in size. As a result, try to avoid scattering your data in numerous small files.\n\n\nHDFS is designed more for \nbatch processing\n rather than interactive use (high throughput versus low latency), and provides only sequential access of data. If your application has other needs, check out tools like \nHBase\n, \nHive\n, \nApache Spark\n, etc.\n\n\n\n\n\u201cMoving Computation is Cheaper than Moving Data\u201d\n\n\n\n\nHDFS architecture\n\n\nAs the \nthe HDFS Architecture Guide\n explains, HDFS has a \nmaster/slave architecture\n.\n\n\nAn HDFS cluster consists of a single \nNameNode\n, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of \nDataNodes\n, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system\u2019s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.\n\n\n\n\n\n\nBasic Manipulations\n\n\n\n\nInfo\n\n\nIn HDFS, user's folders are stored in \n/user\n and not \n/home\n like traditional Unix/Linux filesystems.\n\n\n\n\nListing your home folder\n\n\n $ hdfs dfs -ls\n Found \n28\n items\n ...\n -rw-r--r--   \n3\n llinder  daplab_users   \n6398990\n \n2015\n-03-13 \n11\n:01 data.csv\n ...\n ^^^^^^^^^^   ^ ^^^^^^^^ ^^^^^^^^^^^^   ^^^^^^^ ^^^^^^^^^^ ^^^^^ ^^^^^^^^\n          \n1\n   \n2\n        \n3\n            \n4\n         \n5\n          \n6\n     \n7\n        \n8\n\n\n\n\n\nColumns, as numbered below, represent:\n\n\n\n\nPermissions, in \ntraditional unix permission\n syntax;\n\n\nReplication factor, \nRF\n in short. The RF default to 3 for a file and 0 for a directory; \n\n\nOwner (you!);\n\n\nGroup owning the file;\n\n\nSize of the file, in bytes. Note that to compute the physical space used, this number should be multiplied by the RF;\n\n\nModification date. As HDFS being mostly a ''\nwrite-once-read-many\n'' filesystem,\nthis date often means creation date;\n\n\nModification time. Same as date;\n\n\nFilename, within the listed folder.\n\n\n\n\nListing the \n/tmp\n folder\n\n\nThe only change is that you specify an absolute path from the root of the filesystem (\n/\n):\n\nhdfs dfs -ls /tmp\n\n\n\nUploading a resource\n\n\nTo put a file to HDFS, you have two choices. You can use \nhdfs\n with the \n-put\n option or with the \n-copyFromLocal\n option:\n\n\n# uploading a file\n\nhdfs dfs -put localfile.txt /tmp\nhdfs dfs -copyFromLocal localfile.txt /tmp/\n\n# uploading a directory \n\nhdfs dfs -put localdir /tmp\nhdfs dfs -copyFromLocal localdir /tmp/\n\n\n\n\nThe first arguments after \n-copyFromLocal\n or \n-put\n point to local files or folders, while the last argument is a file (if only one file listed as source) or directory in HDFS. Note that you can use wildcards and also rename files and folders when copying, exactly as you would do in a linux shell:\n\n\n# uploading all files in the current directory with the .txt extension\n\nhdfs dfs -put *.txt /tmp\nhdfs dfs -copyFromLocal *.txt /tmp/\n\n# uploading a directory and renaming it hdfsdir\n\nhdfs dfs -put localdir /tmp/hdfsdir\nhdfs dfs -copyFromLocal localdir /tmp/hdfsdir\n\n\n\n\nBoth options are doing about the same thing, but \n-copyFromLocal\n is more explicit when you're uploading a local file and thus preferred.\n\n\nDownloading a resource\n\n\nDownload is the same as uploading, but \n-put\n becomes \n-get\n and \n-copyFromLocal\n becomes \n-copyToLocal\n:\n\n\nhdfs dfs -get /tmp/remotefile.txt .\nhdfs dfs -copyToLocal /tmp/remotefile.txt .\n\n\n\n\nCreating a folder\n\n\nTo create a folder, use \n-mkdir\n\n\n# create a folder in your hdfs home\n\nhdfs dfs -mkdir dummy-folder\n\n# create a folder in /tmp\n\nhdfs dfs -mkdir /tmp/dummy-folder\n\n\n\n\nNote that relative paths points to your home folder,  \n/user/llinder\n for instance.\n\n\nRemoving resources\n\n\nTo remove individual files, use the \n-rm\n option:\n\n\nhdfs dfs -rm /tmp/somefile.txt\n\n\n\n\nTo remove a folder, the option is \n-rmdir\n for an empty directory and \n-rm -r\n for a non-empty one. The \n-r\n in \n-rm -r\n means \nrecursive\n: it removes the folder and all its children recursively:\n\n\n# remove the dummy-folder in your home\n\nhdfs dfs -rmdir dummy-folder\nrmdir: \n/tmp/lala\n: Directory is not empty\n\n# oups, the directory is not empty... use -rm -r\n\nhdfs dfs -rm -r dummy-folder\n\n\n\n\nAdvanced Manipulations\n\n\nthe \nhdfs dfs\n command support several actions that any linux user is already familiar with. Most of their parameters are the same, but note that the collapsing of options (\n-rf\n instead of \n-r -f\n for example) are not supported. Here is a non-exhaustive list:\n\n\n\n\n-rm [-r] [-f]\n: remove a file or directory;\n\n\n-cp [-r]\n: copy a file or directory;\n\n\n-mv\n: move/rename a file or directory;\n\n\n-cat\n: display the content of a file;\n\n\n-chmod\n: manipulate file permissions;\n\n\n-chown\n: manipulate file ownership;\n\n\n-tail|-touch|\netc.\n\n\n\n\nOther useful commands include:\n\n\n\n\n-moveFromLocal|-moveToLocal\n: same as \n-copyFromLocal|-copyToLocal\n, but remove the source;\n\n\n-stat\n: display information about the specified path;\n\n\n-count\n: counts the number of directories, files, and bytes under the paths;\n\n\n-du\n: display the size of the specified file, or the sizes of files and directories that are contained in the specified directory;\n\n\n-dus\n: display a summary of the file sizes;\n\n\n-getmerge\n: concatenate the files in src and writes the result to the specified local destination file. To add a newline character at the end of each file, specify the \naddnl\n option: \nhdfs dfs -getmerge \nsrc\n \nlocaldst\n [addnl]\n\n\n-setrep [-R]\n: change the replication factor for a specified file or directory;", 
            "title": "HDFS"
        }, 
        {
            "location": "/hdfs/#resources", 
            "text": "While the source of truth for HDFS commands is the code source, the  documentation page describing the  hdfs dfs  commands  is really useful.  A good and simpler cheat sheet is also available  here .", 
            "title": "Resources"
        }, 
        {
            "location": "/hdfs/#introduction", 
            "text": "HDFS ( Hadoop Distributed File System ) is one of the core components of HADOOP.  The HDFS is a distributed file system designed to run on commodity hardware. Very powerful,\nit should ensure that data are replicated across a wide variety of nodes, making the system\nfault tolerant and suitable for large data sets and gives high throughput.   Tip  To have a better understanding of how HDFS works, we strongly encourage you to check out  the HDFS Architecture Guide .", 
            "title": "Introduction"
        }, 
        {
            "location": "/hdfs/#some-remarks-on-hdfs", 
            "text": "HDFS uses a  simple coherency model : applications mostly need a  write-once-read-many  access model for files. As a result, a file once created, written to and closed becomes read-only. It is possible to append to an HDFS file only if the system was explicitly configured to.  HDFS is tuned to deal with  large files . A typical file in HDFS is gigabytes to terabytes in size. As a result, try to avoid scattering your data in numerous small files.  HDFS is designed more for  batch processing  rather than interactive use (high throughput versus low latency), and provides only sequential access of data. If your application has other needs, check out tools like  HBase ,  Hive ,  Apache Spark , etc.   \u201cMoving Computation is Cheaper than Moving Data\u201d", 
            "title": "Some remarks on HDFS"
        }, 
        {
            "location": "/hdfs/#hdfs-architecture", 
            "text": "As the  the HDFS Architecture Guide  explains, HDFS has a  master/slave architecture .  An HDFS cluster consists of a single  NameNode , a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of  DataNodes , usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system\u2019s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.", 
            "title": "HDFS architecture"
        }, 
        {
            "location": "/hdfs/#basic-manipulations", 
            "text": "Info  In HDFS, user's folders are stored in  /user  and not  /home  like traditional Unix/Linux filesystems.", 
            "title": "Basic Manipulations"
        }, 
        {
            "location": "/hdfs/#listing-your-home-folder", 
            "text": "$ hdfs dfs -ls\n Found  28  items\n ...\n -rw-r--r--    3  llinder  daplab_users    6398990   2015 -03-13  11 :01 data.csv\n ...\n ^^^^^^^^^^   ^ ^^^^^^^^ ^^^^^^^^^^^^   ^^^^^^^ ^^^^^^^^^^ ^^^^^ ^^^^^^^^\n           1     2          3              4           5            6       7          8   Columns, as numbered below, represent:   Permissions, in  traditional unix permission  syntax;  Replication factor,  RF  in short. The RF default to 3 for a file and 0 for a directory;   Owner (you!);  Group owning the file;  Size of the file, in bytes. Note that to compute the physical space used, this number should be multiplied by the RF;  Modification date. As HDFS being mostly a '' write-once-read-many '' filesystem,\nthis date often means creation date;  Modification time. Same as date;  Filename, within the listed folder.", 
            "title": "Listing your home folder"
        }, 
        {
            "location": "/hdfs/#listing-the-tmp-folder", 
            "text": "The only change is that you specify an absolute path from the root of the filesystem ( / ): hdfs dfs -ls /tmp", 
            "title": "Listing the /tmp folder"
        }, 
        {
            "location": "/hdfs/#uploading-a-resource", 
            "text": "To put a file to HDFS, you have two choices. You can use  hdfs  with the  -put  option or with the  -copyFromLocal  option:  # uploading a file \nhdfs dfs -put localfile.txt /tmp\nhdfs dfs -copyFromLocal localfile.txt /tmp/ # uploading a directory  \nhdfs dfs -put localdir /tmp\nhdfs dfs -copyFromLocal localdir /tmp/  The first arguments after  -copyFromLocal  or  -put  point to local files or folders, while the last argument is a file (if only one file listed as source) or directory in HDFS. Note that you can use wildcards and also rename files and folders when copying, exactly as you would do in a linux shell:  # uploading all files in the current directory with the .txt extension \nhdfs dfs -put *.txt /tmp\nhdfs dfs -copyFromLocal *.txt /tmp/ # uploading a directory and renaming it hdfsdir \nhdfs dfs -put localdir /tmp/hdfsdir\nhdfs dfs -copyFromLocal localdir /tmp/hdfsdir  Both options are doing about the same thing, but  -copyFromLocal  is more explicit when you're uploading a local file and thus preferred.", 
            "title": "Uploading a resource"
        }, 
        {
            "location": "/hdfs/#downloading-a-resource", 
            "text": "Download is the same as uploading, but  -put  becomes  -get  and  -copyFromLocal  becomes  -copyToLocal :  hdfs dfs -get /tmp/remotefile.txt .\nhdfs dfs -copyToLocal /tmp/remotefile.txt .", 
            "title": "Downloading a resource"
        }, 
        {
            "location": "/hdfs/#creating-a-folder", 
            "text": "To create a folder, use  -mkdir  # create a folder in your hdfs home \nhdfs dfs -mkdir dummy-folder # create a folder in /tmp \nhdfs dfs -mkdir /tmp/dummy-folder  Note that relative paths points to your home folder,   /user/llinder  for instance.", 
            "title": "Creating a folder"
        }, 
        {
            "location": "/hdfs/#removing-resources", 
            "text": "To remove individual files, use the  -rm  option:  hdfs dfs -rm /tmp/somefile.txt  To remove a folder, the option is  -rmdir  for an empty directory and  -rm -r  for a non-empty one. The  -r  in  -rm -r  means  recursive : it removes the folder and all its children recursively:  # remove the dummy-folder in your home \nhdfs dfs -rmdir dummy-folder\nrmdir:  /tmp/lala : Directory is not empty # oups, the directory is not empty... use -rm -r \nhdfs dfs -rm -r dummy-folder", 
            "title": "Removing resources"
        }, 
        {
            "location": "/hdfs/#advanced-manipulations", 
            "text": "the  hdfs dfs  command support several actions that any linux user is already familiar with. Most of their parameters are the same, but note that the collapsing of options ( -rf  instead of  -r -f  for example) are not supported. Here is a non-exhaustive list:   -rm [-r] [-f] : remove a file or directory;  -cp [-r] : copy a file or directory;  -mv : move/rename a file or directory;  -cat : display the content of a file;  -chmod : manipulate file permissions;  -chown : manipulate file ownership;  -tail|-touch| etc.   Other useful commands include:   -moveFromLocal|-moveToLocal : same as  -copyFromLocal|-copyToLocal , but remove the source;  -stat : display information about the specified path;  -count : counts the number of directories, files, and bytes under the paths;  -du : display the size of the specified file, or the sizes of files and directories that are contained in the specified directory;  -dus : display a summary of the file sizes;  -getmerge : concatenate the files in src and writes the result to the specified local destination file. To add a newline character at the end of each file, specify the  addnl  option:  hdfs dfs -getmerge  src   localdst  [addnl]  -setrep [-R] : change the replication factor for a specified file or directory;", 
            "title": "Advanced Manipulations"
        }, 
        {
            "location": "/mapreduce/", 
            "text": "Coming soon", 
            "title": "MapReduce"
        }, 
        {
            "location": "/hive/", 
            "text": "Coming soon", 
            "title": "Hive"
        }, 
        {
            "location": "/zeppelin/", 
            "text": "Apache Zeppelin is an online notebook that lets you interact with a HADOOP cluster (or any other hadoop/spark installation) through many languages and technology backends. \n\n\nIn this workshop, we will use Zeppelin to explore data with Spark. \n\n\nEnvironment setup\n\n\nTo simplify, we will use a local Zeppelin running on a Docker container on your local machine. \n\n\nWhat is Docker?\nDocker is a tool designed to make it easier to create, deploy, and run applications by using \ncontainers\n.\nContainers are a way to package software in a format that can run isolated on a shared operating system. Unlike VMs, containers do not bundle a full operating system - only libraries and settings required to make the software work are needed. This makes for efficient, lightweight, self-contained systems and guarantees that software will always run the same, regardless of where it\u2019s deployed.\nBy now, you should have Docker running on your machine as well as snorkel set up. Simply start zeppelin (see \nhow to setup Snorkel ?\n) and navigate to \nhttp://localhost:8080/\n.\n\n\nGetting Started\n\n\nCreating a new notebook\n\n\nOn the home page or on the notebook menu, select \"\ncreate new...\n\". Once the notebook is opened, give it a new name.\n\n\n\n\nCreating folders\n\n\nUsing slashes (\n/\n) in the notebook name will automatically create and/or move the notebook into folders.\n\n\n\n\nBasic concepts\n\n\nA notebook is made of \ncells\n, also called \nparagraphs\n. A cell has an \ninterpreter\n that tells Zeppelin which langage/backend to use to run the cell.\n\n\nThe interpreter is configured by writing \n%\ninterpreter name\n at the top of the cell. Without it, Zeppelin will use the default interpreter, which you can configure by clicking on \n \n interpreters\n at the top right of the notebook (drag-drop to re-order them, the first one being the default).\n\n\nYou can run cells by clicking on the \n icon on the right or using the shortcut \nshift+enter\n. \n\n\n\n\nshortcuts\n\n\nMany useful shortcuts exist in edit mode. Click on \n the at the top right of a notebook to display them.\n\n\n\n\nThe first time you run a cell of a given type, an new \ninstance\n of the interpreter is started (it might take a moment). This instance will then be used for all subsequent run of any cell configured with the same language. This is nice, because it means you can share variables and code between cells. \n\n\n\n\nIn case of trouble\n\n\nIf something goes wrong, you can restart an interpreter any time using the \n \n interpreters\n and then clicking on the \n icon alongside the interpreter name.\n\n\n\n\nList of interpreter prefixes\n\n\nInterpreters and prefixes may vary between the installations. On your local Zeppelin, the following interpreters are available: \n\n\n\n\n\n\n\n\nPrefix\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n%spark\n\n\nSpark (scala)\n\n\n\n\n\n\n%pyspark\n\n\nSpark (python)\n\n\n\n\n\n\n%sql\n\n\nSpark SQL\n\n\n\n\n\n\n%dep\n\n\nspecial syntax to load external dependencies\n\n\n\n\n\n\n%md\n\n\nMarkDown cell\n\n\n\n\n\n\n%sh\n\n\nshell script (bash)\n\n\n\n\n\n\n%python\n\n\n\"regular\" python\n\n\n\n\n\n\n\n\nNote\n: \nspark\n is Spark 1.6, \nspark2\n is Spark 2.1.0.\n\n\nA first example\n\n\nTo test Zeppelin, create a new notebook. \n\n\nShell cell\n: \n\n\nOn the first cell, enter the snippet below:\n\n\n%sh\n\necho\n \nThe working directory is \n$(\npwd\n)\n, it contains:\n\nls\n\n\n\n\nAs you can see, this is a regular shell script that will run inside the docker container. The working directory is \n/zeppelin\n, which contains some folders that are also available from your filesystem:\n\n\n\n\n\n\n\n\nZeppelin path\n\n\nLocal path\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n/zeppelin/data\n\n\nsnorkel/zeppelin/data\n\n\nEmpty directory you can use to store input and output data\n\n\n\n\n\n\n/zeppelin/logs\n\n\nsnorkel/zeppelin/logs\n\n\nZeppelin logs\n\n\n\n\n\n\n/zeppelin/notebooks\n\n\nsnorkel/zeppelin/notebooks\n\n\nWhere Zeppelin stores the notebooks (in a special \n.json\n format)... Don't erase it !\n\n\n\n\n\n\n/zeppelin/spark-warehouse\n\n\nsnorkel/zeppelin/spark-warehouse\n\n\nA special directory used by Spark for storing temp tables and such\n\n\n\n\n\n\n\n\nMarkdown cell\n:\n\n\nOn a new cell, type some markdown and press \nshift+enter\n:\n\n\n%md\n\n#\n Title\n\n##\n Subtitle\nI am testing the \n_Zeppelin_\n functionalities, specifically the \n`%markdown`\n \ninterpreter. I can even do some nice \n__latex math__\n using the \\$\\$ delimiters!\nExample:\n\n$$ \\frac{N}{4} * log(t) $$\n\n\n\n\nLet's do some python:\n\n\nOn a new cell, type the following:\n\n\n%\npython\n\n\nrange\n(\n0\n,\n10\n)\n\n\n\n\n\nYou should have the result \n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n displayed. Now, store the result into a variable:\n\n\n%\npython\n\n\nlst\n \n=\n \nrange\n(\n0\n,\n10\n)\n\n\n\n\n\nAs you can see, no output is shown. This is because the last statement, here an assignment, does not return anything. To still view the result, add \nprint(lst)\n at the end of the cell.\n\n\nInside a new cell, let's define a function:\n\n\n%\npython\n\n\ndef\n \nadd_lists\n(\nlst_a\n,\n \nlst_b\n):\n\n    \n do an addition term by term between two lists \n\n    \nreturn\n \n[\n \na\n \n+\n \nb\n \nfor\n \n(\na\n,\n \nb\n)\n \nin\n \nzip\n(\nlst_a\n,\n \nlst_b\n)\n \n]\n\n\n\n\n\nOnce you ran the cell, the function exists in the current context, so you can use it anywhere in your notebook. In case you need to make a change to the function, simply rerun the cell with the updated code.\n\n\n\n\nWarning\n\n\nWhen you stop the docker container or reload an interpreter, the current context is lost and you need to rerun the cells.\n\n\n\n\n\n\nBattling with pyspark\n\n\nLet's explore the \nbattling.csv\n. It is a csv file containing information on baseball games. The columns are:\n\n\n\n\n\n\n\n\nColumn name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nplayerID\n\n\nPlayer ID code\n\n\n\n\n\n\nyearID\n\n\nYear\n\n\n\n\n\n\nstint\n\n\nplayer's stint (order of appearances within a season)\n\n\n\n\n\n\nteamID\n\n\nTeam\n\n\n\n\n\n\nlgID\n\n\nLeague\n\n\n\n\n\n\nG\n\n\nGames\n\n\n\n\n\n\nG_batting\n\n\nGame as batter\n\n\n\n\n\n\nAB\n\n\nAt Bats\n\n\n\n\n\n\nR\n\n\nRuns\n\n\n\n\n\n\nH\n\n\nHits\n\n\n\n\n\n\n2B\n\n\nDoubles\n\n\n\n\n\n\n3B\n\n\nTriples\n\n\n\n\n\n\nHR\n\n\nHomeruns\n\n\n\n\n\n\nRBI\n\n\nRuns Batted In\n\n\n\n\n\n\nSB\n\n\nStolen Bases\n\n\n\n\n\n\nCS\n\n\nCaught Stealing\n\n\n\n\n\n\nBB\n\n\nBase on Balls\n\n\n\n\n\n\nSO\n\n\nStrikeouts\n\n\n\n\n\n\nIBB\n\n\nIntentional walks\n\n\n\n\n\n\nHBP\n\n\nHit by pitch\n\n\n\n\n\n\nSH\n\n\nSacrifice hits\n\n\n\n\n\n\nSF\n\n\nSacrifice flies\n\n\n\n\n\n\nGIDP\n\n\nGrounded into double plays\n\n\n\n\n\n\n\n\nSpark Context\n\n\nTo interact with Spark, we need a \nSpark Context\n. \n\n\nIn Zeppelin, the context is created for us and available through the \nspark\n variable. Likewise, the \nSpark SQL Context\n is stored in the \nsqlContext\n variable.\n\n\n%\npyspark\n\n\nprint\n(\nspark\n.\nversion\n)\n\n\nprint\n(\nsqlContext\n)\n\n\n\n\n\nLoading the data\n\n\nDownload the \nbattling.csv\n file and save it in \nsnorkel/zeppelin/data\n. \n\n\nTo read CSV data into a \nSpark Dataframe\n, nothing is more easy:\n\n\n%\npysark\n\n\nbattingFile\n \n=\n \ndata/Batting.csv\n\n\nbatting\n \n=\n \nspark\n.\nread\n.\ncsv\n(\nbattingFile\n,\n \n    \nheader\n=\nTrue\n,\n \nmode\n=\nDROPMALFORMED\n,\n \ninferSchema\n=\nTrue\n)\n\n\n\n\n\nVisualising the data\n\n\nA \nSparkDataframe\n has two interesting methods to visualize the content:\n\n\n\n\ndescribe()\n: prints the dataframe \nschema\n, i.e. the columns and their types\n\n\nshow()\n or \nshow(numRows, truncate=truncate)\n: prints the content of dataframe as a table. By default, only the first 20 rows are shown and the content of the cells might be truncated from better readability;\n\n\n\n\n%pyspark\n\n\nbatting\n.\ndescribe\n()\n\n\nbatting\n.\nshow\n()\n\n\n\n\n\nTo have an even better view, we can use the Zeppelin method \nz.show()\n (\nz\n is a global variable used to interact with Zeppelin):\n\n%pyspark\n\n\nz\n.\nshow\n(\nbatting\n)\n\n\n\n\nSimple aggregation\n\n\nA \nSpark Dataframe\n is like a table, with rows and columns. It is possible to do most of the operations you would do on an SQL table.\n\n\nFirst, let's compute some statistics per year:\n\n\n%\npyspark\n\n\n\n# import the sum function \n\n\nfrom\n \npyspark.sql.functions\n \nimport\n \nsum\n \n\n\n# compute aggregations. In SQL, this would be written:\n\n\nstatsPerYear\n \n=\n \nbatting\n\\\n    \n.\ngroupBy\n(\nyearID\n)\n\\\n    \n.\nagg\n(\n\n        \nsum\n(\nR\n)\n.\nalias\n(\ntotal runs\n),\n \n        \nsum\n(\nH\n)\n.\nalias\n(\ntotal hits\n),\n \n        \nsum\n(\nG\n)\n.\nalias\n(\ntotal games\n))\n\\\n    \n.\norderBy\n(\nyearID\n)\n\n\n\nz\n.\nshow\n(\nstatsPerYear\n)\n\n\n\n\n\nSQL equivalent\nIn SQL syntax, this query would look like:\nELECT\n \nR\n \nas\n \ntotal runs\n,\n \nH\n \nas\n \ntotal hits\n,\n \nG\n \nas\n \ntotal games\n\n\nFROM\n \nbatting\n\n\nGROUP\n \nBY\n \nyearID\n\n\nORDER\n \nBY\n \nyearID\n\n\n\n\nOn the interface, select the line chart \n or area chart \n and then click on \nsettings\n. Drag-and-drop the statistics into the \nValues\n area:\n\n\n\n\nInteractive queries\n\n\nZeppelin offers methods to create simple forms. The basic syntax is:\n\n\nz\n.\ninput\n(\ninput name\n,\n \ndefault\n \nvalue\n)\n\n\n\n\n\nLet's use an input form to display the hit by pitch per team for a given year:\n\n\n%\npyspark\n\n\nfrom\n \npyspark.sql.functions\n \nimport\n \navg\n\n\n\n# get user input\n\n\nyear\n \n=\n \nz\n.\ninput\n(\nyear\n,\n \n1894\n)\n\n\n\n# do the query\n\n\nhbp_results\n \n=\n \nbatting\n\\\n    \n.\nfilter\n(\nbatting\n.\nyearID\n \n==\n \nyear\n)\n\\\n    \n.\ngroupBy\n(\nteamID\n)\n\\\n    \n.\nagg\n(\navg\n(\nHBP\n)\n\\\n    \n.\nalias\n(\naverage hit by pitch\n))\n\n\n\n# display the results\n\n\nz\n.\nshow\n(\nhbp_results\n)\n\n\n\n\n\nAs you can see, the query is rerun everytime the input changes.\n\n\nz.input\n creates a simple input text, but you can also use \nz.select(\ninput title\n, labels, values)\n for a dropdown and \nz.checkbox(\ninput title\n, default_value)\n for multiple choices. \n\n\nFor example, we could create a dropdown for all teams like this:\n\n\n%\npyspark\n\n\n# get all team names\n\n\nall_teams\n \n=\n \nbatting\n.\nselect\n(\nteamID\n)\n.\ndistinct\n()\n.\ncollect\n()\n\n\n# we get a list of Row(teamsId). Get rid of the Row wrapper!\n\n\nall_teams\n \n=\n \n[\nr\n[\n0\n]\n \nfor\n \nr\n \nin\n \nall_teams\n]\n\n\n\n# create and show a dropdown form\n\n\nteam\n \n=\n \nz\n.\nselect\n(\nselected team\n,\n \nsorted\n(\nzip\n(\nall_teams\n,\n \nall_teams\n)))\n\n\n\n# go something with the results, for example listing the years when the team played\n\n\nyears_played\n \n=\n \nbatting\n\\\n    \n.\nselect\n(\nbatting\n.\nyearID\n.\nalias\n(\nyears played\n))\n\\\n    \n.\nwhere\n(\nbatting\n.\nteamID\n \n==\n \nteam\n)\n\\\n    \n.\ndistinct\n()\n\\\n    \n.\norderBy\n(\nyears played\n)\n\n\n\nz\n.\nshow\n(\nyears_played\n)\n\n\n\n\n\nHave a look at the \nZeppelin documentation\n for more information and examples.\n\n\n\n\nBattling with Spark SQL\n\n\nFrom the code so far, you might have noticed how similar to SQL queries our code were. What if we could get rid of the difficult python syntax and use a declarative language instead ? Well, we can. \n\n\nRegistering a table\n\n\nTo use the SQL syntax directly, we first need to register our dataframe as a table and give it name:\n\n\n%\npyspark\n\n\nbatting\n.\ncreateOrReplaceTempView\n(\nbatting\n)\n \n\n# or registerTempTable(\nbatting\n) for older spark versions\n\n\n\n\n\nThe name can be anything; we will be used in SQL queries, for example in a \nFROM\n clause, to refer to the \nbatting\n dataframe.\n\n\nSimple aggregation\n\n\nFrom now on, we can run SQL-like queries using the \nsqlContext\n. For example (the triple quotes in python are for multi-line strings):\n\n\n%\npyspark\n\n\nresult\n \n=\n \nsqlContext\n.\nsql\n(\n\n    \n\n\n    SELECT teamID, max(H) as `max hits` \n\n\n    FROM batting \n\n\n    GROUP BY teamID \n\n\n    ORDER BY(`max hits`) \n\n\n    DESC LIMIT 10\n\n\n    \n)\n\n\nz\n.\nshow\n(\nresult\n)\n\n\n\n\n\n\n\nColumn aliases with spaces or special characters must be enclosed in \nbackticks\n and not \nstraight quotes\n !\n\n\n\n\nBut this is still python. Thanks to Zeppelin, it is possible to run Spark SQL queries and display results directly using a \nSpark SQL\n cell. So our query now looks like:\n\n\n%\nsql\n  \n-- tell Zeppelin to use the Spark SQL interpreter\n\n\nSELECT\n \nteamID\n,\n \nmax\n(\nH\n)\n \nas\n \n`\nmax\n \nhits\n`\n \n\nFROM\n \nbatting\n \n\nGROUP\n \nBY\n \nteamID\n \n\nORDER\n \nBY\n(\n`\nmax\n \nhits\n`\n)\n \n\nDESC\n \nLIMIT\n \n10\n\n\n\n\n\nInteractive queries\n\n\nForms fields can also be used in an SQL cell, but the syntax is a bit different:\n\n\n\n\nsimple form: \n${\ninput_name\n=\ndefault_value\n}\n\n\ndropdown: \n${\ninput_name\n=\ndefault_value\n,\nvalue1\n|\nvalue2\n|\nvalueX\n}\n\n\n\n\nFor example, using an input:\n\n\n%\nsql\n\n\nselect\n \nteamID\n,\n \navg\n(\nHBP\n)\n \nas\n \n`\naverage\n \nhit\n \nper\n \npitch\n`\n\n\nfrom\n \nbatting\n\n\nwhere\n \nyearID\n \n=\n \n${\nyear\n=\n1984\n}\n \n-- let the user choose the year\n\n\ngroup\n \nby\n \nteamID\n\n\n\n\n\nusing a dropdown:\n\n\n%\nsql\n\n\nselect\n \ndistinct\n(\nyearID\n)\n \nas\n \n`\nyears\n \nplayed\n`\n\n\nfrom\n \nbatting\n\n\nwhere\n \nteamID\n \n=\n \n${team=OAK,OAK|SFN|SDN|PHI}\n \n-- let the user choose one 4 teams\n\n\norder\n \nby\n \n`\nyears\n \nplayed\n`\n\n\n\n.", 
            "title": "Hello Zeppelin"
        }, 
        {
            "location": "/zeppelin/#environment-setup", 
            "text": "To simplify, we will use a local Zeppelin running on a Docker container on your local machine.   What is Docker? Docker is a tool designed to make it easier to create, deploy, and run applications by using  containers . Containers are a way to package software in a format that can run isolated on a shared operating system. Unlike VMs, containers do not bundle a full operating system - only libraries and settings required to make the software work are needed. This makes for efficient, lightweight, self-contained systems and guarantees that software will always run the same, regardless of where it\u2019s deployed. By now, you should have Docker running on your machine as well as snorkel set up. Simply start zeppelin (see  how to setup Snorkel ? ) and navigate to  http://localhost:8080/ .", 
            "title": "Environment setup"
        }, 
        {
            "location": "/zeppelin/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/zeppelin/#creating-a-new-notebook", 
            "text": "On the home page or on the notebook menu, select \" create new... \". Once the notebook is opened, give it a new name.   Creating folders  Using slashes ( / ) in the notebook name will automatically create and/or move the notebook into folders.", 
            "title": "Creating a new notebook"
        }, 
        {
            "location": "/zeppelin/#basic-concepts", 
            "text": "A notebook is made of  cells , also called  paragraphs . A cell has an  interpreter  that tells Zeppelin which langage/backend to use to run the cell.  The interpreter is configured by writing  % interpreter name  at the top of the cell. Without it, Zeppelin will use the default interpreter, which you can configure by clicking on     interpreters  at the top right of the notebook (drag-drop to re-order them, the first one being the default).  You can run cells by clicking on the   icon on the right or using the shortcut  shift+enter .    shortcuts  Many useful shortcuts exist in edit mode. Click on   the at the top right of a notebook to display them.   The first time you run a cell of a given type, an new  instance  of the interpreter is started (it might take a moment). This instance will then be used for all subsequent run of any cell configured with the same language. This is nice, because it means you can share variables and code between cells.    In case of trouble  If something goes wrong, you can restart an interpreter any time using the     interpreters  and then clicking on the   icon alongside the interpreter name.", 
            "title": "Basic concepts"
        }, 
        {
            "location": "/zeppelin/#list-of-interpreter-prefixes", 
            "text": "Interpreters and prefixes may vary between the installations. On your local Zeppelin, the following interpreters are available:      Prefix  Description      %spark  Spark (scala)    %pyspark  Spark (python)    %sql  Spark SQL    %dep  special syntax to load external dependencies    %md  MarkDown cell    %sh  shell script (bash)    %python  \"regular\" python     Note :  spark  is Spark 1.6,  spark2  is Spark 2.1.0.", 
            "title": "List of interpreter prefixes"
        }, 
        {
            "location": "/zeppelin/#a-first-example", 
            "text": "To test Zeppelin, create a new notebook.   Shell cell :   On the first cell, enter the snippet below:  %sh echo   The working directory is  $( pwd ) , it contains: \nls  As you can see, this is a regular shell script that will run inside the docker container. The working directory is  /zeppelin , which contains some folders that are also available from your filesystem:     Zeppelin path  Local path  Description      /zeppelin/data  snorkel/zeppelin/data  Empty directory you can use to store input and output data    /zeppelin/logs  snorkel/zeppelin/logs  Zeppelin logs    /zeppelin/notebooks  snorkel/zeppelin/notebooks  Where Zeppelin stores the notebooks (in a special  .json  format)... Don't erase it !    /zeppelin/spark-warehouse  snorkel/zeppelin/spark-warehouse  A special directory used by Spark for storing temp tables and such     Markdown cell :  On a new cell, type some markdown and press  shift+enter :  %md #  Title ##  Subtitle\nI am testing the  _Zeppelin_  functionalities, specifically the  `%markdown`  \ninterpreter. I can even do some nice  __latex math__  using the \\$\\$ delimiters!\nExample:\n\n$$ \\frac{N}{4} * log(t) $$  Let's do some python:  On a new cell, type the following:  % python  range ( 0 , 10 )   You should have the result  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  displayed. Now, store the result into a variable:  % python  lst   =   range ( 0 , 10 )   As you can see, no output is shown. This is because the last statement, here an assignment, does not return anything. To still view the result, add  print(lst)  at the end of the cell.  Inside a new cell, let's define a function:  % python  def   add_lists ( lst_a ,   lst_b ): \n      do an addition term by term between two lists  \n     return   [   a   +   b   for   ( a ,   b )   in   zip ( lst_a ,   lst_b )   ]   Once you ran the cell, the function exists in the current context, so you can use it anywhere in your notebook. In case you need to make a change to the function, simply rerun the cell with the updated code.   Warning  When you stop the docker container or reload an interpreter, the current context is lost and you need to rerun the cells.", 
            "title": "A first example"
        }, 
        {
            "location": "/zeppelin/#battling-with-pyspark", 
            "text": "Let's explore the  battling.csv . It is a csv file containing information on baseball games. The columns are:     Column name  Description      playerID  Player ID code    yearID  Year    stint  player's stint (order of appearances within a season)    teamID  Team    lgID  League    G  Games    G_batting  Game as batter    AB  At Bats    R  Runs    H  Hits    2B  Doubles    3B  Triples    HR  Homeruns    RBI  Runs Batted In    SB  Stolen Bases    CS  Caught Stealing    BB  Base on Balls    SO  Strikeouts    IBB  Intentional walks    HBP  Hit by pitch    SH  Sacrifice hits    SF  Sacrifice flies    GIDP  Grounded into double plays", 
            "title": "Battling with pyspark"
        }, 
        {
            "location": "/zeppelin/#spark-context", 
            "text": "To interact with Spark, we need a  Spark Context .   In Zeppelin, the context is created for us and available through the  spark  variable. Likewise, the  Spark SQL Context  is stored in the  sqlContext  variable.  % pyspark  print ( spark . version )  print ( sqlContext )", 
            "title": "Spark Context"
        }, 
        {
            "location": "/zeppelin/#loading-the-data", 
            "text": "Download the  battling.csv  file and save it in  snorkel/zeppelin/data .   To read CSV data into a  Spark Dataframe , nothing is more easy:  % pysark  battingFile   =   data/Batting.csv  batting   =   spark . read . csv ( battingFile ,  \n     header = True ,   mode = DROPMALFORMED ,   inferSchema = True )", 
            "title": "Loading the data"
        }, 
        {
            "location": "/zeppelin/#visualising-the-data", 
            "text": "A  SparkDataframe  has two interesting methods to visualize the content:   describe() : prints the dataframe  schema , i.e. the columns and their types  show()  or  show(numRows, truncate=truncate) : prints the content of dataframe as a table. By default, only the first 20 rows are shown and the content of the cells might be truncated from better readability;   %pyspark  batting . describe ()  batting . show ()   To have an even better view, we can use the Zeppelin method  z.show()  ( z  is a global variable used to interact with Zeppelin): %pyspark  z . show ( batting )", 
            "title": "Visualising the data"
        }, 
        {
            "location": "/zeppelin/#simple-aggregation", 
            "text": "A  Spark Dataframe  is like a table, with rows and columns. It is possible to do most of the operations you would do on an SQL table.  First, let's compute some statistics per year:  % pyspark  # import the sum function   from   pyspark.sql.functions   import   sum   # compute aggregations. In SQL, this would be written:  statsPerYear   =   batting \\\n     . groupBy ( yearID ) \\\n     . agg ( \n         sum ( R ) . alias ( total runs ),  \n         sum ( H ) . alias ( total hits ),  \n         sum ( G ) . alias ( total games )) \\\n     . orderBy ( yearID )  z . show ( statsPerYear )   SQL equivalent In SQL syntax, this query would look like: ELECT   R   as   total runs ,   H   as   total hits ,   G   as   total games  FROM   batting  GROUP   BY   yearID  ORDER   BY   yearID   On the interface, select the line chart   or area chart   and then click on  settings . Drag-and-drop the statistics into the  Values  area:", 
            "title": "Simple aggregation"
        }, 
        {
            "location": "/zeppelin/#interactive-queries", 
            "text": "Zeppelin offers methods to create simple forms. The basic syntax is:  z . input ( input name ,   default   value )   Let's use an input form to display the hit by pitch per team for a given year:  % pyspark  from   pyspark.sql.functions   import   avg  # get user input  year   =   z . input ( year ,   1894 )  # do the query  hbp_results   =   batting \\\n     . filter ( batting . yearID   ==   year ) \\\n     . groupBy ( teamID ) \\\n     . agg ( avg ( HBP ) \\\n     . alias ( average hit by pitch ))  # display the results  z . show ( hbp_results )   As you can see, the query is rerun everytime the input changes.  z.input  creates a simple input text, but you can also use  z.select( input title , labels, values)  for a dropdown and  z.checkbox( input title , default_value)  for multiple choices.   For example, we could create a dropdown for all teams like this:  % pyspark  # get all team names  all_teams   =   batting . select ( teamID ) . distinct () . collect ()  # we get a list of Row(teamsId). Get rid of the Row wrapper!  all_teams   =   [ r [ 0 ]   for   r   in   all_teams ]  # create and show a dropdown form  team   =   z . select ( selected team ,   sorted ( zip ( all_teams ,   all_teams )))  # go something with the results, for example listing the years when the team played  years_played   =   batting \\\n     . select ( batting . yearID . alias ( years played )) \\\n     . where ( batting . teamID   ==   team ) \\\n     . distinct () \\\n     . orderBy ( years played )  z . show ( years_played )   Have a look at the  Zeppelin documentation  for more information and examples.", 
            "title": "Interactive queries"
        }, 
        {
            "location": "/zeppelin/#battling-with-spark-sql", 
            "text": "From the code so far, you might have noticed how similar to SQL queries our code were. What if we could get rid of the difficult python syntax and use a declarative language instead ? Well, we can.", 
            "title": "Battling with Spark SQL"
        }, 
        {
            "location": "/zeppelin/#registering-a-table", 
            "text": "To use the SQL syntax directly, we first need to register our dataframe as a table and give it name:  % pyspark  batting . createOrReplaceTempView ( batting )   # or registerTempTable( batting ) for older spark versions   The name can be anything; we will be used in SQL queries, for example in a  FROM  clause, to refer to the  batting  dataframe.", 
            "title": "Registering a table"
        }, 
        {
            "location": "/zeppelin/#simple-aggregation_1", 
            "text": "From now on, we can run SQL-like queries using the  sqlContext . For example (the triple quotes in python are for multi-line strings):  % pyspark  result   =   sqlContext . sql ( \n          SELECT teamID, max(H) as `max hits`       FROM batting       GROUP BY teamID       ORDER BY(`max hits`)       DESC LIMIT 10       )  z . show ( result )    Column aliases with spaces or special characters must be enclosed in  backticks  and not  straight quotes  !   But this is still python. Thanks to Zeppelin, it is possible to run Spark SQL queries and display results directly using a  Spark SQL  cell. So our query now looks like:  % sql    -- tell Zeppelin to use the Spark SQL interpreter  SELECT   teamID ,   max ( H )   as   ` max   hits `   FROM   batting   GROUP   BY   teamID   ORDER   BY ( ` max   hits ` )   DESC   LIMIT   10", 
            "title": "Simple aggregation"
        }, 
        {
            "location": "/zeppelin/#interactive-queries_1", 
            "text": "Forms fields can also be used in an SQL cell, but the syntax is a bit different:   simple form:  ${ input_name = default_value }  dropdown:  ${ input_name = default_value , value1 | value2 | valueX }   For example, using an input:  % sql  select   teamID ,   avg ( HBP )   as   ` average   hit   per   pitch `  from   batting  where   yearID   =   ${ year = 1984 }   -- let the user choose the year  group   by   teamID   using a dropdown:  % sql  select   distinct ( yearID )   as   ` years   played `  from   batting  where   teamID   =   ${team=OAK,OAK|SFN|SDN|PHI}   -- let the user choose one 4 teams  order   by   ` years   played `  \n.", 
            "title": "Interactive queries"
        }, 
        {
            "location": "/spark/", 
            "text": "In this workshop, we will create a search engine for BBC articles using pyspark and the Spark ML library. \n\n\n\n\npyspark documentation\n\n\nML guide\n\n\n\n\nIntroduction\n\n\nWhat we will do\n\n\nTo create the search engine, we will do the following steps:\n\n\n\n\nWhat we will use\n\n\nLSA, \nlatent semantic anlysis\n is\n\n\n\n\na technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms\n (source: wikipedia)\n\n\n\n\nIn summary, we want to establish underlying relationships between documents in a collection by linking documents and terms through \nconcepts\n. Those concepts are deduced or constructed through a statistical or mathematical algorithm that forms a \nmodel\n. Once we have the model, we can begin using it for search queries.\n\n\nAvailable algorithms\n\n\nThere are two well-known methods for discovering underlying topics:\n\n\n\n\n\n\nSVD\n, \nSingular Value Decomposition\n: is a mathematical method for decomposing a matrix into a product of smaller matrices. It has the advantage of being \nmathematically correct\n: computing the model, hence  the decomposed matrices, takes only one pass and is deterministic: the same data will always give the same result (as long as the same parameters are used).\n\n\n\n\n\n\nLDA\n, \nLatent Dirichlet Allocation\n, is a generative probabilistic model. Based on statistics, many iterations are necessary to get a good-enough model and every run could give a different result. The result is also highly dependant on the parameters we use for the training. \n\n\n\n\n\n\nIn this workshop, we will use the LDA technique.\n\n\nLDA Basics\n\n\nLDA is like a clustering algorithm where :\n\n\n\n\nthe \ncluster centers\n are the topics;\n\n\nthe \ntraining examples\n are documents;\n\n\nthe \nfeatures\n are the word frequencies;\n\n\nthe distance is \neuclidean\n based on a statistical model (Bayesian inference rules / Dirichlet distribution)\n\n\n\n\nAs in clustering, we need to give some informations (\nparameters\n) to the model. The most important one is \nthe number of topics (k)\n we think there is (exactly as we need to specify the number of clusters to find).\n\n\nDuring training, the model will first assign each document to a random topic. Then, on each iteration, it computes how well the actual topic distribution describes/predicts each document, makes adjustments and try again. Most of the time, the programmer will set in advance the \nmaximum number of iterations\n to do.\n\n\nIn the end, the model outputs a topic distribution over document (how much a document is important to a given topic) and over terms (how much the term describes the topic). Documents with similar topic distributions are likely to be similar, even if they don't use the same words.\n\n\nLDA with pyspark\n\n\nGetting the dataset\n\n\ncoming soon", 
            "title": "Spark"
        }, 
        {
            "location": "/spark/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/spark/#what-we-will-do", 
            "text": "To create the search engine, we will do the following steps:", 
            "title": "What we will do"
        }, 
        {
            "location": "/spark/#what-we-will-use", 
            "text": "LSA,  latent semantic anlysis  is   a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms  (source: wikipedia)   In summary, we want to establish underlying relationships between documents in a collection by linking documents and terms through  concepts . Those concepts are deduced or constructed through a statistical or mathematical algorithm that forms a  model . Once we have the model, we can begin using it for search queries.", 
            "title": "What we will use"
        }, 
        {
            "location": "/spark/#available-algorithms", 
            "text": "There are two well-known methods for discovering underlying topics:    SVD ,  Singular Value Decomposition : is a mathematical method for decomposing a matrix into a product of smaller matrices. It has the advantage of being  mathematically correct : computing the model, hence  the decomposed matrices, takes only one pass and is deterministic: the same data will always give the same result (as long as the same parameters are used).    LDA ,  Latent Dirichlet Allocation , is a generative probabilistic model. Based on statistics, many iterations are necessary to get a good-enough model and every run could give a different result. The result is also highly dependant on the parameters we use for the training.     In this workshop, we will use the LDA technique.", 
            "title": "Available algorithms"
        }, 
        {
            "location": "/spark/#lda-basics", 
            "text": "LDA is like a clustering algorithm where :   the  cluster centers  are the topics;  the  training examples  are documents;  the  features  are the word frequencies;  the distance is  euclidean  based on a statistical model (Bayesian inference rules / Dirichlet distribution)   As in clustering, we need to give some informations ( parameters ) to the model. The most important one is  the number of topics (k)  we think there is (exactly as we need to specify the number of clusters to find).  During training, the model will first assign each document to a random topic. Then, on each iteration, it computes how well the actual topic distribution describes/predicts each document, makes adjustments and try again. Most of the time, the programmer will set in advance the  maximum number of iterations  to do.  In the end, the model outputs a topic distribution over document (how much a document is important to a given topic) and over terms (how much the term describes the topic). Documents with similar topic distributions are likely to be similar, even if they don't use the same words.", 
            "title": "LDA Basics"
        }, 
        {
            "location": "/spark/#lda-with-pyspark", 
            "text": "", 
            "title": "LDA with pyspark"
        }, 
        {
            "location": "/spark/#getting-the-dataset", 
            "text": "coming soon", 
            "title": "Getting the dataset"
        }, 
        {
            "location": "/lsa/", 
            "text": "Coming soon", 
            "title": "LSA"
        }
    ]
}