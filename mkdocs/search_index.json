{
    "docs": [
        {
            "location": "/", 
            "text": "D\u00e9j\u00e0 entendu parl\u00e9 de Hadoop, Spark, Flink ou HDFS\u00a0? Curieux de savoir enfin de quoi il retourne\u00a0?\n\n\n\n\nThis workshop takes place at  UNIFR / HEIA-FR the 19\nth\n and 20\nth\n of October 2017. It is organised by IT-Valley in collaboration with the DAPLAB. It is intended for any developer willing to familiarize with the Big Data and Data Sciences technologies.\n\n\nLocation:\n\n\n\n\nRoom C00.11\n\n\nHaute \u00e9cole d'ing\u00e9nierie et d'architecture de Fribourg\n\n\nBd de P\u00e9rolles 80, 1705 Fribourg\n\n\n\n\nIt will be given in French, with support in English.\n\n\nSchedule\n\n\nDay 1\n:\n\n\n\n\nIntroduction: Big Data, Hadoop, HDFS, what are they ?\n\n\nCheck out the slides: \nBig Data: an Introduction\n\n\n\n\n\n\nHDFS: filesystem and command line usage\n\n\nMapReduce:\n\n\ntheory\n: what is MapReduce ?\n\n\npractice\n: My first MapReduce application (java)\n\n\n\n\n\n\nHive:\n\n\ntheory\n: what is Hive?\n\n\npractice\n: Querying and manipulating data using Hive\n\n\n\n\n\n\n\n\nDay 2\n:\n\n\n\n\nSpark and Zeppelin:\n\n\ngetting started with Zeppelin (python + pyspark)\n\n\nSpark SQL: quick data discovery and visualisation (python + pyspark)\n\n\n\n\n\n\nLSA, \nLatent Semantic Analysis\n:\n\n\ntheory\n: what is it and what is it for ?\n\n\npractice\n: implementing a document search engine using LDA, \nLatent Dirichlet Allocation\n (python + pyspark)\n\n\n\n\n\n\nif there is some time left, a little tour of the DAPLAB\n\n\n\n\nRequirements\n\n\nFor the workshop, you will need the following:\n\n\n\n\na laptop (Recommended: Mac or Linux)\n\n\nthe Java JDK version 8 or above\n\n\na java IDE: \n\n\nif you don't already have one, please install the \nIntelliJ Community Edition\n\n\n\n\n\n\nMaven:\n\n\nIf you have IntelliJ, you can skip this step as it already ships with an embedded Maven\n\n\nIf you want Maven available from the command line as well, follow the installation instructions on the \nMaven website\n\n\n\n\n\n\nDocker:\n\n\nWindows\n installation guide\n\n\nMac\n installation guide\n\n\n\n\n\n\nSnorkel:\n\n\n\n\nfollow the instruction at \nhttps://github.com/Sqooba/snorkel\n or have a look at the section below. \n\n\n\n\nFor Windows, I added some scripts at \nhttps://github.com/derlin/snorkel\n, the pull request is still in review. Use mine for now.\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up Snorkel\n\n\n\n\nYou need Docker running on your machine. \n\n\n\n\nSnorkel is a docker container allowing you to run Zeppelin locally.\n\n\n\n\n\n\nDownload snorkel: \n\n\n\n\nIf you have git installed, clone the following repository: \n  \ngit clone https://github.com/Sqooba/snorkel.git\n\n\n\nIf you don't have git installed, go to \nhttps://github.com/Sqooba/snorkel\n \n  and select \nClone or download \n Download ZIP\n, then unzip.\n\n\n\n\n\n\n\n\nopen a command line and navigate inside the snorkel folder;\n\n\n\n\n\n\nbuild the zeppelin image:\n\n\n\n\nOn Mac/Linux:\n    \n./build-images.sh\n\n\n\nOn Windows:\n    \n./build-images.cmd\n\n\n\n\n\n\n\n\n\nstart zeppelin:\n\n\n\n\nOn Mac/Linux:\n    \n./zeppelin.sh --start\n\n\n\nOn Windows (from the command prompt or the powershell):\n    \n./start-zeppelin.cmd\n\n\n\n\n\n\n\n\n\ncheck that zeppelin is running: \n\n\n\n\ngo to \nhttp://localhost:8080/\n, you should have a zeppelin welcome screen;\n\n\n\n\n\n\n\n\nstop zeppelin:\n\n\n\n\nOn Mac/Linux:\n    \n./zeppelin.sh --stop\n\n\n\nOn Windows (from the command prompt or the powershell):\n    \n./stop-zeppelin.cmd", 
            "title": "Home"
        }, 
        {
            "location": "/#schedule", 
            "text": "Day 1 :   Introduction: Big Data, Hadoop, HDFS, what are they ?  Check out the slides:  Big Data: an Introduction    HDFS: filesystem and command line usage  MapReduce:  theory : what is MapReduce ?  practice : My first MapReduce application (java)    Hive:  theory : what is Hive?  practice : Querying and manipulating data using Hive     Day 2 :   Spark and Zeppelin:  getting started with Zeppelin (python + pyspark)  Spark SQL: quick data discovery and visualisation (python + pyspark)    LSA,  Latent Semantic Analysis :  theory : what is it and what is it for ?  practice : implementing a document search engine using LDA,  Latent Dirichlet Allocation  (python + pyspark)    if there is some time left, a little tour of the DAPLAB", 
            "title": "Schedule"
        }, 
        {
            "location": "/#requirements", 
            "text": "For the workshop, you will need the following:   a laptop (Recommended: Mac or Linux)  the Java JDK version 8 or above  a java IDE:   if you don't already have one, please install the  IntelliJ Community Edition    Maven:  If you have IntelliJ, you can skip this step as it already ships with an embedded Maven  If you want Maven available from the command line as well, follow the installation instructions on the  Maven website    Docker:  Windows  installation guide  Mac  installation guide    Snorkel:   follow the instruction at  https://github.com/Sqooba/snorkel  or have a look at the section below.    For Windows, I added some scripts at  https://github.com/derlin/snorkel , the pull request is still in review. Use mine for now.", 
            "title": "Requirements"
        }, 
        {
            "location": "/#setting-up-snorkel", 
            "text": "You need Docker running on your machine.    Snorkel is a docker container allowing you to run Zeppelin locally.    Download snorkel:    If you have git installed, clone the following repository: \n   git clone https://github.com/Sqooba/snorkel.git  If you don't have git installed, go to  https://github.com/Sqooba/snorkel  \n  and select  Clone or download   Download ZIP , then unzip.     open a command line and navigate inside the snorkel folder;    build the zeppelin image:   On Mac/Linux:\n     ./build-images.sh  On Windows:\n     ./build-images.cmd     start zeppelin:   On Mac/Linux:\n     ./zeppelin.sh --start  On Windows (from the command prompt or the powershell):\n     ./start-zeppelin.cmd     check that zeppelin is running:    go to  http://localhost:8080/ , you should have a zeppelin welcome screen;     stop zeppelin:   On Mac/Linux:\n     ./zeppelin.sh --stop  On Windows (from the command prompt or the powershell):\n     ./stop-zeppelin.cmd", 
            "title": "Setting up Snorkel"
        }, 
        {
            "location": "/hdfs/", 
            "text": "In order to use HADOOP, it is crucial that you understand the basic functioning of HDFS, as well as some of its constraints.\nAfter a brief introduction of core HDFS concepts, this page presents \ncopy-paste\n-like tutorial to familiarize with\n\nHDFS commands\n.\nIt mainly focuses on user commands (uploading and downloading data into HDFS).\n\n\nResources\n\n\nWhile the source of truth for HDFS commands is the code source, the \ndocumentation page describing the \nhdfs dfs\n commands\n is really useful.\n\n\nA good and simpler cheat sheet is also available \nhere\n.\n\n\nIntroduction\n\n\nHDFS (\nHadoop Distributed File System\n) is one of the core components of HADOOP.\n\n\nThe HDFS is a distributed file system designed to run on commodity hardware. Very powerful,\nit should ensure that data are replicated across a wide variety of nodes, making the system\nfault tolerant and suitable for large data sets and gives high throughput.\n\n\n\n\nTip\n\n\nTo have a better understanding of how HDFS works, we strongly encourage you to check out \nthe HDFS Architecture Guide\n.\n\n\n\n\nSome remarks on HDFS\n\n\nHDFS uses a \nsimple coherency model\n: applications mostly need a \nwrite-once-read-many\n access model for files. As a result, a file once created, written to and closed becomes read-only. It is possible to append to an HDFS file only if the system was explicitly configured to.\n\n\nHDFS is tuned to deal with \nlarge files\n. A typical file in HDFS is gigabytes to terabytes in size. As a result, try to avoid scattering your data in numerous small files.\n\n\nHDFS is designed more for \nbatch processing\n rather than interactive use (high throughput versus low latency), and provides only sequential access of data. If your application has other needs, check out tools like \nHBase\n, \nHive\n, \nApache Spark\n, etc.\n\n\n\n\n\u201cMoving Computation is Cheaper than Moving Data\u201d\n\n\n\n\nHDFS architecture\n\n\nAs the \nthe HDFS Architecture Guide\n explains, HDFS has a \nmaster/slave architecture\n.\n\n\nAn HDFS cluster consists of a single \nNameNode\n, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of \nDataNodes\n, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system\u2019s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.\n\n\n\n\n\n\nBasic Manipulations\n\n\nTo interact with HDFS you should use the \ndfs\n module.\nThe dfs module, also known as \"FS shell\", provides basic file manipulation operations.\n\n\n\n\nInfo\n\n\nIn HDFS, user's folders are stored in \n/user\n and not \n/home\n like traditional Unix/Linux filesystems.\n\n\n\n\nListing your home folder\n\n\n $ hdfs dfs -ls /user/\nyour_user\n\n Found \n28\n items\n ...\n -rw-r--r--   \n3\n llinder  daplab_users   \n6398990\n \n2015\n-03-13 \n11\n:01 data.csv\n ...\n ^^^^^^^^^^   ^ ^^^^^^^^ ^^^^^^^^^^^^   ^^^^^^^ ^^^^^^^^^^ ^^^^^ ^^^^^^^^\n          \n1\n   \n2\n        \n3\n            \n4\n         \n5\n          \n6\n     \n7\n        \n8\n\n\n\n\n\n\n\nInfo\n\n\nRelative paths points to your home folder, using : \nhdfs dfs -ls /user/\nyour_user\n is the same as \nhdfs dfs -ls\n\n\n\n\nColumns, as numbered below, represent:\n\n\n\n\nPermissions, in \ntraditional unix permission\n syntax;\n\n\nReplication factor, \nRF\n in short. The RF default to 3 for a file and 0 for a directory; \n\n\nOwner (you!);\n\n\nGroup owning the file;\n\n\nSize of the file, in bytes. Note that to compute the physical space used, this number should be multiplied by the RF;\n\n\nModification date. As HDFS being mostly a ''\nwrite-once-read-many\n'' filesystem,\nthis date often means creation date;\n\n\nModification time. Same as date;\n\n\nFilename, within the listed folder.\n\n\n\n\nUploading a resource\n\n\nTo put a file to HDFS, you have two choices. You can use \nhdfs\n with the \n-put\n option or with the \n-copyFromLocal\n option:\n\n\n# uploading a file\n\nhdfs dfs -put localfile.txt /user/\nyour_user\n\nhdfs dfs -copyFromLocal localfile.txt /user/\nyour_user\n\n\n# uploading a directory \n\nhdfs dfs -put localdir /user/\nyour_user\n\nhdfs dfs -copyFromLocal localdir /user/\nyour_user\n\n\n\n\n\nThe first arguments after \n-copyFromLocal\n or \n-put\n point to local files or folders, while the last argument is a file (if only one file listed as source) or directory in HDFS. Note that you can rename files and folders when copying, exactly as you would do in a linux shell:\n\n\n# uploading all files in the current directory with the .txt extension\n\nhdfs dfs -put *.txt /user/\nyour_user\n\nhdfs dfs -copyFromLocal *.txt /user/\nyour_user\n\n\n# uploading a directory and renaming it hdfsdir\n\nhdfs dfs -put localdir /user/\nyour_user\n/hdfsdir\nhdfs dfs -copyFromLocal localdir /user/\nyour_user\n/hdfsdir\n\n\n\n\nUploading data for next sessions:\n\n\n\n\nNow lets upload the data that we will use for MapReduce and Hive:\n\n\n\n\n1) Bible Shakespear Data:\n\n\n\n\nDownload \nbible_shakespear\n data set\n\n\nDecompress and upload to HDFS:\n\n\n\n\nhdfs dfs -put bible_shakes.nopunc /user/\nyour_user\n\n\n\n2) Batting Data:\n\n\n\n\nDownload the csv file \nBatting.csv\n\n\nUpload to HDFS:\n\n\n\n\nhdfs dfs -put Batting.csv /user/\nyour_user\n\n\n\n\n\nDownloading a resource\n\n\nDownload is the same as uploading, but \n-put\n becomes \n-get\n and \n-copyFromLocal\n becomes \n-copyToLocal\n:\n\n\nhdfs dfs -get /user/\nyour_user\n/remotefile.txt .\nhdfs dfs -copyToLocal /user/\nyour_user\n/remotefile.txt .\n\n\n\n\nCreating a folder\n\n\nTo create a folder, use \n-mkdir\n\n\n# create a folder in your hdfs home\n\nhdfs dfs -mkdir dummy-folder\n\n\n\n\nRemoving resources\n\n\nTo remove individual files, use the \n-rm\n option:\n\n\nhdfs dfs -rm /user/\nyour_user\n/somefile.txt\n\n\n\n\nTo remove a folder, the option is \n-rmdir\n for an empty directory and \n-rm -r\n for a non-empty one. The \n-r\n in \n-rm -r\n means \nrecursive\n: it removes the folder and all its children recursively:\n\n\n# remove the dummy-folder in your home\n\nhdfs dfs -rmdir dummy-folder\nrmdir: \n/tmp/lala\n: Directory is not empty\n\n# oups, the directory is not empty... use -rm -r\n\nhdfs dfs -rm -r dummy-folder\n\n\n\n\nAdvanced Manipulations\n\n\nthe \nhdfs dfs\n command support several actions that any linux user is already familiar with. Most of their parameters are the same, but note that the collapsing of options (\n-rf\n instead of \n-r -f\n for example) are not supported. Here is a non-exhaustive list:\n\n\n\n\n-rm [-r] [-f]\n: remove a file or directory;\n\n\n-cp [-r]\n: copy a file or directory;\n\n\n-mv\n: move/rename a file or directory;\n\n\n-cat\n: display the content of a file;\n\n\n-chmod\n: manipulate file permissions;\n\n\n-chown\n: manipulate file ownership;\n\n\n-tail|-touch|\netc.\n\n\n\n\nOther useful commands include:\n\n\n\n\n-moveFromLocal|-moveToLocal\n: same as \n-copyFromLocal|-copyToLocal\n, but remove the source;\n\n\n-stat\n: display information about the specified path;\n\n\n-count\n: counts the number of directories, files, and bytes under the paths;\n\n\n-du\n: display the size of the specified file, or the sizes of files and directories that are contained in the specified directory;\n\n\n-dus\n: display a summary of the file sizes;\n\n\n-getmerge\n: concatenate the files in src and writes the result to the specified local destination file. To add a newline character at the end of each file, specify the \naddnl\n option: \nhdfs dfs -getmerge \nsrc\n \nlocaldst\n [addnl]\n\n\n-setrep [-R]\n: change the replication factor for a specified file or directory;", 
            "title": "HDFS"
        }, 
        {
            "location": "/hdfs/#resources", 
            "text": "While the source of truth for HDFS commands is the code source, the  documentation page describing the  hdfs dfs  commands  is really useful.  A good and simpler cheat sheet is also available  here .", 
            "title": "Resources"
        }, 
        {
            "location": "/hdfs/#introduction", 
            "text": "HDFS ( Hadoop Distributed File System ) is one of the core components of HADOOP.  The HDFS is a distributed file system designed to run on commodity hardware. Very powerful,\nit should ensure that data are replicated across a wide variety of nodes, making the system\nfault tolerant and suitable for large data sets and gives high throughput.   Tip  To have a better understanding of how HDFS works, we strongly encourage you to check out  the HDFS Architecture Guide .", 
            "title": "Introduction"
        }, 
        {
            "location": "/hdfs/#some-remarks-on-hdfs", 
            "text": "HDFS uses a  simple coherency model : applications mostly need a  write-once-read-many  access model for files. As a result, a file once created, written to and closed becomes read-only. It is possible to append to an HDFS file only if the system was explicitly configured to.  HDFS is tuned to deal with  large files . A typical file in HDFS is gigabytes to terabytes in size. As a result, try to avoid scattering your data in numerous small files.  HDFS is designed more for  batch processing  rather than interactive use (high throughput versus low latency), and provides only sequential access of data. If your application has other needs, check out tools like  HBase ,  Hive ,  Apache Spark , etc.   \u201cMoving Computation is Cheaper than Moving Data\u201d", 
            "title": "Some remarks on HDFS"
        }, 
        {
            "location": "/hdfs/#hdfs-architecture", 
            "text": "As the  the HDFS Architecture Guide  explains, HDFS has a  master/slave architecture .  An HDFS cluster consists of a single  NameNode , a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of  DataNodes , usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system\u2019s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.", 
            "title": "HDFS architecture"
        }, 
        {
            "location": "/hdfs/#basic-manipulations", 
            "text": "To interact with HDFS you should use the  dfs  module.\nThe dfs module, also known as \"FS shell\", provides basic file manipulation operations.   Info  In HDFS, user's folders are stored in  /user  and not  /home  like traditional Unix/Linux filesystems.", 
            "title": "Basic Manipulations"
        }, 
        {
            "location": "/hdfs/#listing-your-home-folder", 
            "text": "$ hdfs dfs -ls /user/ your_user \n Found  28  items\n ...\n -rw-r--r--    3  llinder  daplab_users    6398990   2015 -03-13  11 :01 data.csv\n ...\n ^^^^^^^^^^   ^ ^^^^^^^^ ^^^^^^^^^^^^   ^^^^^^^ ^^^^^^^^^^ ^^^^^ ^^^^^^^^\n           1     2          3              4           5            6       7          8    Info  Relative paths points to your home folder, using :  hdfs dfs -ls /user/ your_user  is the same as  hdfs dfs -ls   Columns, as numbered below, represent:   Permissions, in  traditional unix permission  syntax;  Replication factor,  RF  in short. The RF default to 3 for a file and 0 for a directory;   Owner (you!);  Group owning the file;  Size of the file, in bytes. Note that to compute the physical space used, this number should be multiplied by the RF;  Modification date. As HDFS being mostly a '' write-once-read-many '' filesystem,\nthis date often means creation date;  Modification time. Same as date;  Filename, within the listed folder.", 
            "title": "Listing your home folder"
        }, 
        {
            "location": "/hdfs/#uploading-a-resource", 
            "text": "To put a file to HDFS, you have two choices. You can use  hdfs  with the  -put  option or with the  -copyFromLocal  option:  # uploading a file \nhdfs dfs -put localfile.txt /user/ your_user \nhdfs dfs -copyFromLocal localfile.txt /user/ your_user  # uploading a directory  \nhdfs dfs -put localdir /user/ your_user \nhdfs dfs -copyFromLocal localdir /user/ your_user   The first arguments after  -copyFromLocal  or  -put  point to local files or folders, while the last argument is a file (if only one file listed as source) or directory in HDFS. Note that you can rename files and folders when copying, exactly as you would do in a linux shell:  # uploading all files in the current directory with the .txt extension \nhdfs dfs -put *.txt /user/ your_user \nhdfs dfs -copyFromLocal *.txt /user/ your_user  # uploading a directory and renaming it hdfsdir \nhdfs dfs -put localdir /user/ your_user /hdfsdir\nhdfs dfs -copyFromLocal localdir /user/ your_user /hdfsdir", 
            "title": "Uploading a resource"
        }, 
        {
            "location": "/hdfs/#uploading-data-for-next-sessions", 
            "text": "Now lets upload the data that we will use for MapReduce and Hive:   1) Bible Shakespear Data:   Download  bible_shakespear  data set  Decompress and upload to HDFS:   hdfs dfs -put bible_shakes.nopunc /user/ your_user  \n2) Batting Data:   Download the csv file  Batting.csv  Upload to HDFS:   hdfs dfs -put Batting.csv /user/ your_user", 
            "title": "Uploading data for next sessions:"
        }, 
        {
            "location": "/hdfs/#downloading-a-resource", 
            "text": "Download is the same as uploading, but  -put  becomes  -get  and  -copyFromLocal  becomes  -copyToLocal :  hdfs dfs -get /user/ your_user /remotefile.txt .\nhdfs dfs -copyToLocal /user/ your_user /remotefile.txt .", 
            "title": "Downloading a resource"
        }, 
        {
            "location": "/hdfs/#creating-a-folder", 
            "text": "To create a folder, use  -mkdir  # create a folder in your hdfs home \nhdfs dfs -mkdir dummy-folder", 
            "title": "Creating a folder"
        }, 
        {
            "location": "/hdfs/#removing-resources", 
            "text": "To remove individual files, use the  -rm  option:  hdfs dfs -rm /user/ your_user /somefile.txt  To remove a folder, the option is  -rmdir  for an empty directory and  -rm -r  for a non-empty one. The  -r  in  -rm -r  means  recursive : it removes the folder and all its children recursively:  # remove the dummy-folder in your home \nhdfs dfs -rmdir dummy-folder\nrmdir:  /tmp/lala : Directory is not empty # oups, the directory is not empty... use -rm -r \nhdfs dfs -rm -r dummy-folder", 
            "title": "Removing resources"
        }, 
        {
            "location": "/hdfs/#advanced-manipulations", 
            "text": "the  hdfs dfs  command support several actions that any linux user is already familiar with. Most of their parameters are the same, but note that the collapsing of options ( -rf  instead of  -r -f  for example) are not supported. Here is a non-exhaustive list:   -rm [-r] [-f] : remove a file or directory;  -cp [-r] : copy a file or directory;  -mv : move/rename a file or directory;  -cat : display the content of a file;  -chmod : manipulate file permissions;  -chown : manipulate file ownership;  -tail|-touch| etc.   Other useful commands include:   -moveFromLocal|-moveToLocal : same as  -copyFromLocal|-copyToLocal , but remove the source;  -stat : display information about the specified path;  -count : counts the number of directories, files, and bytes under the paths;  -du : display the size of the specified file, or the sizes of files and directories that are contained in the specified directory;  -dus : display a summary of the file sizes;  -getmerge : concatenate the files in src and writes the result to the specified local destination file. To add a newline character at the end of each file, specify the  addnl  option:  hdfs dfs -getmerge  src   localdst  [addnl]  -setrep [-R] : change the replication factor for a specified file or directory;", 
            "title": "Advanced Manipulations"
        }, 
        {
            "location": "/mapreduce/", 
            "text": "MapReduce is a programming model for distributed data processing across multiple nodes. The programs are designed to compute large volumes of data in a parallel fashion. MapReduce works by breaking the processing into two main phases: Map phase and Reduce phase.\n\n\nMapReduce execution framework allows the following:\n\n\n\n\nAutomatic parallelization and distribution\n\n\nFault tolerance\n\n\nAbstraction for programmers\n\n\n\n\nRefer to \nMapReduce Tutorial\n for more details.\n\n\nMapReduce Framework\n\n\n\n\nMapReduce Phases\n\n\nMap phase\n\n\n\n\nMap phase is usually for filtering or transforming data. Each mapper performs a user-defined operation on a single HDFS block\n\n\nMap tasks run on the node where the data block is stored\n\n\nInput: key/value pair\n\n\nOutput: A list of zero or more key value/pairs\n\n\n\n\nPartition - Shuffle and sort phase\n\n\n\n\nPartitioner: Guarantees all pairs with the same key go to the same Reduce\n\n\nShuffling: Moving map outputs to the reducers\n\n\nSorting: The set of intermediate keys on a single node is sorted before passing to the reducer\n\n\n\n\nReduce phase\n\n\n\n\nRead all results for each key and performs a user-defined operation, for example: aggregations\n\n\nThe Reducer outputs zero or more final key/value pairs to the HDFS\n\n\n\n\nCoding with MapReduce\n\n\nData Types and Formats\n\n\n\n\nKeys and values are java objects\n\n\nWritable interface for serialization\n\n\nValues implement Writable interface\n\n\nKeys implement WritableComparable interface\n\n\nIntWritable \u2013 int\n\n\nLongWritable \u2013 Long\n\n\nFloatWritable \u2013 Float\n\n\nDoubleWritable - Double\n\n\nText \u2013 String values\n\n\n\n\nLet\u2019s examine our first MapReduce program\n\n\nA typical MapReduce program consists of three main parts: Driver, Mapper and Reducer code.\n\n\nDriver Code\n\n\n\n\nContains Job configuration and submission details\n\n\nConfigurations not explicitly set in your driver code will be read from your Hadoop configuration files (/etc/hadoop/conf)\n\n\n\n\nExamine the below sample for Driver Code of word count example:\n\n\npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n \nargs\n[])\n \nthrows\n \nException\n \n{\n\n    \nConfiguration\n \nconf\n \n=\n \nnew\n \nConfiguration\n();\n\n    \nJob\n \njob\n \n=\n \nnew\n \nJob\n(\nconf\n,\nword count\n);\n\n\n    \njob\n.\nsetInputFormatClass\n(\nTextInputFormat\n.\nclass\n);\n\n    \njob\n.\nsetJarByClass\n(\nWordCount\n.\nclass\n);\n\n\n    \njob\n.\nsetMapperClass\n(\nWCMapper\n.\nclass\n);\n\n    \njob\n.\nsetReducerClass\n(\nWCReducer\n.\nclass\n);\n\n\n    \njob\n.\nsetMapOutputKeyClass\n(\nText\n.\nclass\n);\n\n    \njob\n.\nsetMapOutputValueClass\n(\nIntWritable\n.\nclass\n);\n\n    \njob\n.\nsetOutputKeyClass\n(\nText\n.\nclass\n);\n\n    \njob\n.\nsetOutputValueClass\n(\nIntWritable\n.\nclass\n);\n\n\n    \njob\n.\nsetOutputFormatClass\n(\nTextOutputFormat\n.\nclass\n);\n\n\n    \nFileInputFormat\n.\naddInputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n0\n]));\n\n    \nFileOutputFormat\n.\nsetOutputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n1\n]));\n\n    \nSystem\n.\nexit\n(\njob\n.\nwaitForCompletion\n(\ntrue\n)\n \n?\n \n0\n \n:\n \n1\n);\n\n\n}\n\n\n\n\nMapper Code\n\n\n\n\nExtend Mapper base class\n\n\nOverride map()\n\n\nInput: key/value\n\n\nOutput: key/value\n\n\n\n\nSample for Map class for word count example:\n\n\nstatic\n \nclass\n \nWCMapper\n \nextends\n \nMapper\nObject\n,\n \nText\n,\n \nText\n,\n \nIntWritable\n \n{\n\n    \npublic\n \nfinal\n \nstatic\n \nIntWritable\n \none\n \n=\n \nnew\n \nIntWritable\n(\n1\n);\n\n    \nprivate\n \nText\n \nword\n \n=\n \nnew\n \nText\n();\n\n\n    \npublic\n \nvoid\n \nmap\n(\nObject\n \nkey\n,\n \nText\n \nvalue\n,\n \nContext\n \ncontext\n)\n\n    \nthrows\n \nIOException\n,\n \nInterruptedException\n \n{\n\n        \nSystem\n.\nout\n.\nprintln\n(\nkey\n);\n\n        \nSystem\n.\nout\n.\nprintln\n(\nvalue\n);\n\n        \nStringTokenizer\n \nitr\n \n=\n \nnew\n \nStringTokenizer\n(\nvalue\n.\ntoString\n());\n\n        \nwhile\n \n(\nitr\n.\nhasMoreTokens\n())\n \n{\n\n            \nword\n.\nset\n(\nitr\n.\nnextToken\n());\n\n            \ncontext\n.\nwrite\n(\nword\n,\n \none\n);\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\nReducer Code\n\n\n\n\nExtend Reducer base class\n\n\nOverride reduce()\n\n\nInput: key/collection of values\n\n\nOutput: key/value\n\n\n\n\nSample for Reduce class for word count example:\n\n\nstatic\n \nclass\n \nWCReducer\n \nextends\n \nReducer\nText\n,\n \nIntWritable\n,\n \nText\n,\n \nIntWritable\n \n{\n\n    \nprivate\n \nIntWritable\n \nresult\n \n=\n \nnew\n \nIntWritable\n();\n\n\n    \nprotected\n \nvoid\n \nreduce\n(\nText\n \nkey\n,\n \nIterable\nIntWritable\n \nvalues\n,\n \nContext\n \ncontext\n)\n\n    \nthrows\n \nIOException\n,\n \nInterruptedException\n \n{\n\n        \nint\n \nsum\n \n=\n \n0\n;\n\n        \nfor\n \n(\nIntWritable\n \nvalue\n \n:\n \nvalues\n)\n\n            \nsum\n \n+=\n \nvalue\n.\nget\n();\n\n        \nresult\n.\nset\n(\nsum\n);\n\n        \ncontext\n.\nwrite\n(\nkey\n,\n \nresult\n);\n\n    \n}\n\n\n}\n\n\n\n\n\nRunning the program\n\n\n\n\nDownload \nbible_shakespear\n data set\n\n\nDecompress and move to HDFS\n\n\n\n\nhdfs dfs -put bible_shakes.nopunc /user/\nyour_user\n\n\n\n\n\n\n\nDownload source code for \nword count example\n\n\nCompile the code and create a jar (You can download this \nwordcount.jar\n for testing)\n\n\nTo execute a MapReduce job:\n\n\n\n\n/bin/hadoop jar \njar-name\n \nmain-class\n \ninput-path\n \noutput-path\n\n\n\n\n\n\n\nUse the following command to run our example:\n\n\n\n\n/bin/hadoop jar wordcount.jar  exascale.info.lab.wordcount.WordCount /user/\nyour_user\n/bible_shakes.nopunc /user/\nyour_user\n/\noutput_folder\n\n\n\n\n\n\n\nMonitor the job status and examine the logs:\n\n\n\n\nhttps://hadoop-rm.daplab.ch/cluster\n\n\n\n\nExamine the output in HDFS\n\n\n\n\nImplementation Pointers\n\n\nNumber of Mappers and Reducers\n\n\n\n\nThe number of mappers typically depends on the input data and the block size\n\n\nThe number of reducers can be set in your driver code, as follows:\n\n\n\n\njob\n.\nsetNumReduceTasks\n(\nnumReducers\n);\n\n\n\n\n\n\n\nHaving only one reducer for large input data can cause a bottleneck\n\n\nTo many reducers will create extensive network traffic\n\n\n\n\nCreating Map only jobs\n\n\nSome jobs only require a map task. For example, file conversions, and data sampling.\n\n\nTo create a map only job:\n\n\n\n\nSet the number of reducers to 0:\n\n\n\n\njob\n.\nsetNumReduceTasks\n(\n0\n);\n\n\n\n\n\n\n\nSpecify output types:\n\n\n\n\njob\n.\nsetOutputKeyClass\n(\nText\n.\nclass\n);\n\n\njob\n.\nsetOutputValueClass\n(\nIntWritable\n.\nclass\n);\n\n\n\n\n\nCombiners\n\n\n\n\n\n\nWe can introduce an additional step for optimizing bandwidth usage in our MapReduce job.\n\n\n\n\n\n\nThe combiner is a \u201cmini reduce\u201d process which operates on data generated by one node to reduce the network traffic\n\n\n\n\n\n\nTo add a combiner in our word count example, add the following line in the driver code:\n\n\n\n\n\n\njob\n.\nsetReducerClass\n(\nWCReducer\n.\nclass\n);\n\n\n\n\n\nExample 2 - Bigrams count\n\n\n\n\nUse the \nbible_shakespear\n data set again\n\n\nWrite a MapReduce program to count the co-occurrences of pairs of words\n\n\nFor simplicity, you can use this \nbigrams_Helper.zip\n program structure and build upon it\n\n\nMonitor the progress of your job\n\n\nExamine the output\n\n\nUse the combiner class to optimize your job\n\n\n\n\nSolution\nMapper class:\nFirst we need to convert the Text value recieved by each map to String[]:\n\npublic\n \nstatic\n \nString\n[]\n \ntextToString\n(\nText\n \nvalue\n)\n \n{\n\n    \nString\n \ntext\n \n=\n \nvalue\n.\ntoString\n();\n\n    \ntext\n \n=\n \ntext\n.\ntoLowerCase\n();\n\n    \ntext\n \n=\n \ntext\n.\nreplaceAll\n(\n[^a-z]+\n,\n \n \n);\n\n    \ntext\n \n=\n \ntext\n.\nreplaceAll\n(\n^\\\\s+\n,\n \n);\n\n    \nStringTokenizer\n \nitr\n \n=\n \nnew\n \nStringTokenizer\n(\ntext\n);\n\n    \nArrayList\nString\n \nresult\n \n=\n \nnew\n \nArrayList\nString\n();\n\n    \nwhile\n \n(\nitr\n.\nhasMoreTokens\n())\n\n        \nresult\n.\nadd\n(\nitr\n.\nnextToken\n());\n\n    \nreturn\n \nArrays\n.\ncopyOf\n(\nresult\n.\ntoArray\n(),\nresult\n.\nsize\n(),\nString\n[].\nclass\n);}\n\n\nThen we can loop over the String[] object and extract bigrams:\n\nfor\n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \nTokens\n.\nlength\n \n-\n \n1\n;\n \ni\n++)\n \n{\n\n    \nword\n.\nset\n(\nTokens\n[\ni\n]\n \n+\n \n \n \n+\n  \nTokens\n[\ni\n+\n1\n]);\n\n    \ncontext\n.\nwrite\n(\nword\n,\n \none\n);}\n\n\nReducer class:\nWe can use the same reducer as in the word count example, because we need to sum the values of the received keys (in this case the keys emitted by the mappers are bigrams):\n\nint\n \nsum\n \n=\n \n0\n;\n\n\nfor\n \n(\nIntWritable\n \nvalue\n \n:\n \nvalues\n)\n\n    \nsum\n \n+=\n \nvalue\n.\nget\n();\n\n\nresult\n.\nset\n(\nsum\n);\n\n\ncontext\n.\nwrite\n(\nkey\n,\n \nresult\n);\n\n\nFurther exercises\n\n\n\n\nLength Histogram: Write a MapReduce program to count the occurrences for ranges of lengths of words.", 
            "title": "MapReduce"
        }, 
        {
            "location": "/mapreduce/#mapreduce-framework", 
            "text": "", 
            "title": "MapReduce Framework"
        }, 
        {
            "location": "/mapreduce/#mapreduce-phases", 
            "text": "Map phase   Map phase is usually for filtering or transforming data. Each mapper performs a user-defined operation on a single HDFS block  Map tasks run on the node where the data block is stored  Input: key/value pair  Output: A list of zero or more key value/pairs   Partition - Shuffle and sort phase   Partitioner: Guarantees all pairs with the same key go to the same Reduce  Shuffling: Moving map outputs to the reducers  Sorting: The set of intermediate keys on a single node is sorted before passing to the reducer   Reduce phase   Read all results for each key and performs a user-defined operation, for example: aggregations  The Reducer outputs zero or more final key/value pairs to the HDFS", 
            "title": "MapReduce Phases"
        }, 
        {
            "location": "/mapreduce/#coding-with-mapreduce", 
            "text": "", 
            "title": "Coding with MapReduce"
        }, 
        {
            "location": "/mapreduce/#data-types-and-formats", 
            "text": "Keys and values are java objects  Writable interface for serialization  Values implement Writable interface  Keys implement WritableComparable interface  IntWritable \u2013 int  LongWritable \u2013 Long  FloatWritable \u2013 Float  DoubleWritable - Double  Text \u2013 String values", 
            "title": "Data Types and Formats"
        }, 
        {
            "location": "/mapreduce/#lets-examine-our-first-mapreduce-program", 
            "text": "A typical MapReduce program consists of three main parts: Driver, Mapper and Reducer code.  Driver Code   Contains Job configuration and submission details  Configurations not explicitly set in your driver code will be read from your Hadoop configuration files (/etc/hadoop/conf)   Examine the below sample for Driver Code of word count example:  public   static   void   main ( String   args [])   throws   Exception   { \n     Configuration   conf   =   new   Configuration (); \n     Job   job   =   new   Job ( conf , word count ); \n\n     job . setInputFormatClass ( TextInputFormat . class ); \n     job . setJarByClass ( WordCount . class ); \n\n     job . setMapperClass ( WCMapper . class ); \n     job . setReducerClass ( WCReducer . class ); \n\n     job . setMapOutputKeyClass ( Text . class ); \n     job . setMapOutputValueClass ( IntWritable . class ); \n     job . setOutputKeyClass ( Text . class ); \n     job . setOutputValueClass ( IntWritable . class ); \n\n     job . setOutputFormatClass ( TextOutputFormat . class ); \n\n     FileInputFormat . addInputPath ( job ,   new   Path ( args [ 0 ])); \n     FileOutputFormat . setOutputPath ( job ,   new   Path ( args [ 1 ])); \n     System . exit ( job . waitForCompletion ( true )   ?   0   :   1 );  }   Mapper Code   Extend Mapper base class  Override map()  Input: key/value  Output: key/value   Sample for Map class for word count example:  static   class   WCMapper   extends   Mapper Object ,   Text ,   Text ,   IntWritable   { \n     public   final   static   IntWritable   one   =   new   IntWritable ( 1 ); \n     private   Text   word   =   new   Text (); \n\n     public   void   map ( Object   key ,   Text   value ,   Context   context ) \n     throws   IOException ,   InterruptedException   { \n         System . out . println ( key ); \n         System . out . println ( value ); \n         StringTokenizer   itr   =   new   StringTokenizer ( value . toString ()); \n         while   ( itr . hasMoreTokens ())   { \n             word . set ( itr . nextToken ()); \n             context . write ( word ,   one ); \n         } \n     }  }   Reducer Code   Extend Reducer base class  Override reduce()  Input: key/collection of values  Output: key/value   Sample for Reduce class for word count example:  static   class   WCReducer   extends   Reducer Text ,   IntWritable ,   Text ,   IntWritable   { \n     private   IntWritable   result   =   new   IntWritable (); \n\n     protected   void   reduce ( Text   key ,   Iterable IntWritable   values ,   Context   context ) \n     throws   IOException ,   InterruptedException   { \n         int   sum   =   0 ; \n         for   ( IntWritable   value   :   values ) \n             sum   +=   value . get (); \n         result . set ( sum ); \n         context . write ( key ,   result ); \n     }  }", 
            "title": "Let\u2019s examine our first MapReduce program"
        }, 
        {
            "location": "/mapreduce/#running-the-program", 
            "text": "Download  bible_shakespear  data set  Decompress and move to HDFS   hdfs dfs -put bible_shakes.nopunc /user/ your_user    Download source code for  word count example  Compile the code and create a jar (You can download this  wordcount.jar  for testing)  To execute a MapReduce job:   /bin/hadoop jar  jar-name   main-class   input-path   output-path    Use the following command to run our example:   /bin/hadoop jar wordcount.jar  exascale.info.lab.wordcount.WordCount /user/ your_user /bible_shakes.nopunc /user/ your_user / output_folder    Monitor the job status and examine the logs:   https://hadoop-rm.daplab.ch/cluster   Examine the output in HDFS", 
            "title": "Running the program"
        }, 
        {
            "location": "/mapreduce/#implementation-pointers", 
            "text": "", 
            "title": "Implementation Pointers"
        }, 
        {
            "location": "/mapreduce/#number-of-mappers-and-reducers", 
            "text": "The number of mappers typically depends on the input data and the block size  The number of reducers can be set in your driver code, as follows:   job . setNumReduceTasks ( numReducers );    Having only one reducer for large input data can cause a bottleneck  To many reducers will create extensive network traffic", 
            "title": "Number of Mappers and Reducers"
        }, 
        {
            "location": "/mapreduce/#creating-map-only-jobs", 
            "text": "Some jobs only require a map task. For example, file conversions, and data sampling.  To create a map only job:   Set the number of reducers to 0:   job . setNumReduceTasks ( 0 );    Specify output types:   job . setOutputKeyClass ( Text . class );  job . setOutputValueClass ( IntWritable . class );", 
            "title": "Creating Map only jobs"
        }, 
        {
            "location": "/mapreduce/#combiners", 
            "text": "We can introduce an additional step for optimizing bandwidth usage in our MapReduce job.    The combiner is a \u201cmini reduce\u201d process which operates on data generated by one node to reduce the network traffic    To add a combiner in our word count example, add the following line in the driver code:    job . setReducerClass ( WCReducer . class );", 
            "title": "Combiners"
        }, 
        {
            "location": "/mapreduce/#example-2-bigrams-count", 
            "text": "Use the  bible_shakespear  data set again  Write a MapReduce program to count the co-occurrences of pairs of words  For simplicity, you can use this  bigrams_Helper.zip  program structure and build upon it  Monitor the progress of your job  Examine the output  Use the combiner class to optimize your job   Solution Mapper class: First we need to convert the Text value recieved by each map to String[]: public   static   String []   textToString ( Text   value )   { \n     String   text   =   value . toString (); \n     text   =   text . toLowerCase (); \n     text   =   text . replaceAll ( [^a-z]+ ,     ); \n     text   =   text . replaceAll ( ^\\\\s+ ,   ); \n     StringTokenizer   itr   =   new   StringTokenizer ( text ); \n     ArrayList String   result   =   new   ArrayList String (); \n     while   ( itr . hasMoreTokens ()) \n         result . add ( itr . nextToken ()); \n     return   Arrays . copyOf ( result . toArray (), result . size (), String []. class );}  Then we can loop over the String[] object and extract bigrams: for ( int   i   =   0 ;   i     Tokens . length   -   1 ;   i ++)   { \n     word . set ( Tokens [ i ]   +       +    Tokens [ i + 1 ]); \n     context . write ( word ,   one );}  Reducer class: We can use the same reducer as in the word count example, because we need to sum the values of the received keys (in this case the keys emitted by the mappers are bigrams): int   sum   =   0 ;  for   ( IntWritable   value   :   values ) \n     sum   +=   value . get ();  result . set ( sum );  context . write ( key ,   result );", 
            "title": "Example 2 - Bigrams count"
        }, 
        {
            "location": "/mapreduce/#further-exercises", 
            "text": "Length Histogram: Write a MapReduce program to count the occurrences for ranges of lengths of words.", 
            "title": "Further exercises"
        }, 
        {
            "location": "/hive/", 
            "text": "Hive is a \"pluggable software\" running on top of YARN/HDFS. Its primary goal is\nto facilitate reading, writing, and managing large datasets residing in distributed storage using SQL.\n\n\nWith Hive, it is possible to project a structure onto data already in storage and to write\n\nHive Query Language (HQL)\n statements that are similar to standard SQL statements. Of course, Hive implements only a limited subset of SQL commands, but is still quite helpful.\n\n\nUnder the hood, HQL statement are translated into \nMapReduce\n jobs and executed across a Hadoop cluster.\n\n\nResources\n\n\nThis tutorial is heavily inspired from the HortonWorks tutorial and is illustrated in command-line.\n\n\n\n\nhttp://hortonworks.com/hadoop-tutorial/how-to-process-data-with-apache-hive/\n\n\n\n\nData Preparation\n\n\nHere, we will use a CSV file with batting records, i.e. statistics about baseball players for each year.\n\n\nThe headers are:\n\n\nplayerID, yearID, stint, teamID, lgID, G ,G_batting, AB, R, H, 2B, 3B, HR, RBI, SB, CS, BB, SO, IBB, HBP, SH, SF, GIDP, G_old\n\n\nwhich stand for:\n\n\nG=games, AB=at bats, R=runs, H=hits, 2B=doubles, 3B=triples, HR=dinger, RBI=runs batted in, SB=stolen base, CS=caught stealing, BB=base on balls, SO=strikeout, IBB=intentional walks, HBP=hit by pitch, SH=sacrifice hits, SF=sacrifice flys, GIDP=ground into double play\n\n\nIn this program, we will simply output the maximum number of runs made by one player for each year.\n\n\n\n\nLoad the csv file \nBatting.csv\n into your home folder in HDFS:\n\n\n\n\nhdfs dfs -copyFromLocal Batting.csv /user/\n$(\nwhoami\n)\n\n\n\n\n\nCreate a database in Hive\n\n\nNote: In order to have this tutorial to work for everybody, let's create a database prefixed by your username (\n${\nenv\n:\nUSER\n}\n inside hive).\n\n\nType the \"hive\" command to start the hive shell, and continue as follows:\n\n\n$ hive\n\ncreate database \n${\nenv\n:\nUSER\n}\n_test\n;\n\n\n\n\n\nCreate a temp table to load the file into\n\n\n$\n \nhive\n\n\n\nuse\n \n${\nenv\n:\nUSER\n}\n_test\n;\n\n\ncreate\n \ntable\n \ntemp_batting\n \n(\ncol_value\n \nSTRING\n);\n\n\nLOAD\n \nDATA\n \nINPATH\n \n/user/${env:USER}/Batting.csv\n \nOVERWRITE\n \nINTO\n \nTABLE\n \ntemp_batting\n;\n\n\n\n\n\nCreate the final table and insert the data into\n\n\n$\n \nhive\n\n\n\nuse\n \n${\nenv\n:\nUSER\n}\n_test\n;\n\n\ncreate\n \ntable\n \nbatting\n \n(\nplayer_id\n \nSTRING\n,\n \nyear\n \nINT\n,\n \nruns\n \nINT\n);\n\n\ninsert\n \noverwrite\n \ntable\n \nbatting\n\n\nSELECT\n\n\nregexp_extract\n(\ncol_value\n,\n \n^(?:([^,]*)\\,?){1}\n,\n \n1\n)\n \nplayer_id\n,\n\n\nregexp_extract\n(\ncol_value\n,\n \n^(?:([^,]*)\\,?){2}\n,\n \n1\n)\n \nyear\n,\n\n\nregexp_extract\n(\ncol_value\n,\n \n^(?:([^,]*)\\,?){9}\n,\n \n1\n)\n \nrun\n\n\nfrom\n \ntemp_batting\n;\n\n\n\n\n\nRun your first query\n\n\n$\n \nhive\n \n--database ${USER}_test\n\n\n\nSELECT\n \nyear\n,\n \nmax\n(\nruns\n)\n \nFROM\n \nbatting\n \nGROUP\n \nBY\n \nyear\n;\n\n\n\n\n\nRun a more complex query\n\n\n$\n \nhive\n \n--database ${USER}_test\n\n\n\nSELECT\n \na\n.\nyear\n,\n \na\n.\nplayer_id\n,\n \na\n.\nruns\n \nfrom\n \nbatting\n \na\n\n\nJOIN\n \n(\nSELECT\n \nyear\n,\n \nmax\n(\nruns\n)\n \nruns\n \nFROM\n \nbatting\n \nGROUP\n \nBY\n \nyear\n \n)\n \nb\n\n\nON\n \n(\na\n.\nyear\n \n=\n \nb\n.\nyear\n \nAND\n \na\n.\nruns\n \n=\n \nb\n.\nruns\n)\n \n;\n\n\n\n\n\nHive external table\n\n\nAs you might have noticed, with hive \"normal\" tables, you need to upload the data in HDFS,\ncreate a temp table, load the data into this temp table, create another, final,\ntable and eventually copy and format the data from the temp table to the final one.\n\n\nAnother idea is to rely on hive external table, which will read the data directly from the CSV file.\n\n\nLet's create a table mapping the CSV file and run a query on top of it.\n\n\nNote: the external table location (where the data is physically stored)\n\nMUST\n be a folder. We'll change the directory structure to accommodate this requirement.\n\n\nhdfs dfs -mkdir /user/\n$(\nwhoami\n)\n/Batting\nhdfs dfs -copyFromLocal Batting.csv /user/\n$(\nwhoami\n)\n/Batting \n# note here that the file needs to be re-uploaded because the LOAD DATA consumed (and removed the file)\n\n\n\n\n\nAnd then create the external table\n\n\n$\n \nhive\n \n--database ${USER}_test\n\n\n\ncreate\n \nEXTERNAL\n \ntable\n \nbatting_ext\n \n(\nplayer_id\n \nSTRING\n,\n \nyear\n \nINT\n,\n \nstint\n \nSTRING\n,\n \nteam\n \nSTRING\n,\n \nlg\n \nSTRING\n,\n \nG\n \nSTRING\n,\n \nG_batting\n \nSTRING\n,\n \nAB\n \nSTRING\n,\n \nruns\n \nINT\n,\n \nH\n \nSTRING\n,\n \nx2B\n \nSTRING\n,\n \nx3B\n \nSTRING\n,\n \nHR\n \nSTRING\n,\n \nRBI\n \nSTRING\n,\n \nSB\n \nSTRING\n,\n \nCS\n \nSTRING\n,\n \nBB\n \nSTRING\n,\n \nSO\n \nSTRING\n,\n \nIBB\n \nSTRING\n,\n \nHBP\n \nSTRING\n,\n \nSH\n \nSTRING\n,\n \nSF\n \nSTRING\n,\n \nGIDP\n \nSTRING\n,\n \nG_Old\n \nSTRING\n)\n\n\nROW\n \nFORMAT\n\n\nDELIMITED\n \nFIELDS\n \nTERMINATED\n \nBY\n \n,\n\n\nLINES\n \nTERMINATED\n \nBY\n \n\\n\n\n\nSTORED\n \nAS\n \nTEXTFILE\n\n\nLOCATION\n \n/user/${env:USER}/Batting/\n;\n\n\n\n\n\nLet's run the two queries\n\n\n$\n \nhive\n \n--database ${USER}_test\n\n\n\nSELECT\n \nyear\n,\n \nmax\n(\nruns\n)\n \nFROM\n \nbatting_ext\n \nGROUP\n \nBY\n \nyear\n;\n\n\n\nSELECT\n \na\n.\nyear\n,\n \na\n.\nplayer_id\n,\n \na\n.\nruns\n \nfrom\n \nbatting_ext\n \na\n\n\nJOIN\n \n(\nSELECT\n \nyear\n,\n \nmax\n(\nruns\n)\n \nruns\n \nFROM\n \nbatting_ext\n \nGROUP\n \nBY\n \nyear\n \n)\n \nb\n\n\nON\n \n(\na\n.\nyear\n \n=\n \nb\n.\nyear\n \nAND\n \na\n.\nruns\n \n=\n \nb\n.\nruns\n)\n \n;\n\n\n\n\n\nCleanup\n\n\nOnce you're done, you can cleanup your environment, deleting the tables and the database.\n\n\n$\n \nhive\n\n\n\ndrop\n \ntable\n \ntemp_batting\n;\n\n\ndrop\n \ntable\n \nbatting\n;\n\n\ndrop\n \ntable\n \nbatting_ext\n;\n\n\ndrop\n \ndatabase\n \n${\nenv\n:\nUSER\n}\n_test\n;", 
            "title": "Hive"
        }, 
        {
            "location": "/hive/#resources", 
            "text": "This tutorial is heavily inspired from the HortonWorks tutorial and is illustrated in command-line.   http://hortonworks.com/hadoop-tutorial/how-to-process-data-with-apache-hive/", 
            "title": "Resources"
        }, 
        {
            "location": "/hive/#data-preparation", 
            "text": "Here, we will use a CSV file with batting records, i.e. statistics about baseball players for each year.  The headers are:  playerID, yearID, stint, teamID, lgID, G ,G_batting, AB, R, H, 2B, 3B, HR, RBI, SB, CS, BB, SO, IBB, HBP, SH, SF, GIDP, G_old  which stand for:  G=games, AB=at bats, R=runs, H=hits, 2B=doubles, 3B=triples, HR=dinger, RBI=runs batted in, SB=stolen base, CS=caught stealing, BB=base on balls, SO=strikeout, IBB=intentional walks, HBP=hit by pitch, SH=sacrifice hits, SF=sacrifice flys, GIDP=ground into double play  In this program, we will simply output the maximum number of runs made by one player for each year.   Load the csv file  Batting.csv  into your home folder in HDFS:   hdfs dfs -copyFromLocal Batting.csv /user/ $( whoami )", 
            "title": "Data Preparation"
        }, 
        {
            "location": "/hive/#create-a-database-in-hive", 
            "text": "Note: In order to have this tutorial to work for everybody, let's create a database prefixed by your username ( ${ env : USER }  inside hive).  Type the \"hive\" command to start the hive shell, and continue as follows:  $ hive\n\ncreate database  ${ env : USER } _test ;", 
            "title": "Create a database in Hive"
        }, 
        {
            "location": "/hive/#create-a-temp-table-to-load-the-file-into", 
            "text": "$   hive  use   ${ env : USER } _test ;  create   table   temp_batting   ( col_value   STRING );  LOAD   DATA   INPATH   /user/${env:USER}/Batting.csv   OVERWRITE   INTO   TABLE   temp_batting ;", 
            "title": "Create a temp table to load the file into"
        }, 
        {
            "location": "/hive/#create-the-final-table-and-insert-the-data-into", 
            "text": "$   hive  use   ${ env : USER } _test ;  create   table   batting   ( player_id   STRING ,   year   INT ,   runs   INT );  insert   overwrite   table   batting  SELECT  regexp_extract ( col_value ,   ^(?:([^,]*)\\,?){1} ,   1 )   player_id ,  regexp_extract ( col_value ,   ^(?:([^,]*)\\,?){2} ,   1 )   year ,  regexp_extract ( col_value ,   ^(?:([^,]*)\\,?){9} ,   1 )   run  from   temp_batting ;", 
            "title": "Create the final table and insert the data into"
        }, 
        {
            "location": "/hive/#run-your-first-query", 
            "text": "$   hive   --database ${USER}_test  SELECT   year ,   max ( runs )   FROM   batting   GROUP   BY   year ;", 
            "title": "Run your first query"
        }, 
        {
            "location": "/hive/#run-a-more-complex-query", 
            "text": "$   hive   --database ${USER}_test  SELECT   a . year ,   a . player_id ,   a . runs   from   batting   a  JOIN   ( SELECT   year ,   max ( runs )   runs   FROM   batting   GROUP   BY   year   )   b  ON   ( a . year   =   b . year   AND   a . runs   =   b . runs )   ;", 
            "title": "Run a more complex query"
        }, 
        {
            "location": "/hive/#hive-external-table", 
            "text": "As you might have noticed, with hive \"normal\" tables, you need to upload the data in HDFS,\ncreate a temp table, load the data into this temp table, create another, final,\ntable and eventually copy and format the data from the temp table to the final one.  Another idea is to rely on hive external table, which will read the data directly from the CSV file.  Let's create a table mapping the CSV file and run a query on top of it.  Note: the external table location (where the data is physically stored) MUST  be a folder. We'll change the directory structure to accommodate this requirement.  hdfs dfs -mkdir /user/ $( whoami ) /Batting\nhdfs dfs -copyFromLocal Batting.csv /user/ $( whoami ) /Batting  # note here that the file needs to be re-uploaded because the LOAD DATA consumed (and removed the file)   And then create the external table  $   hive   --database ${USER}_test  create   EXTERNAL   table   batting_ext   ( player_id   STRING ,   year   INT ,   stint   STRING ,   team   STRING ,   lg   STRING ,   G   STRING ,   G_batting   STRING ,   AB   STRING ,   runs   INT ,   H   STRING ,   x2B   STRING ,   x3B   STRING ,   HR   STRING ,   RBI   STRING ,   SB   STRING ,   CS   STRING ,   BB   STRING ,   SO   STRING ,   IBB   STRING ,   HBP   STRING ,   SH   STRING ,   SF   STRING ,   GIDP   STRING ,   G_Old   STRING )  ROW   FORMAT  DELIMITED   FIELDS   TERMINATED   BY   ,  LINES   TERMINATED   BY   \\n  STORED   AS   TEXTFILE  LOCATION   /user/${env:USER}/Batting/ ;   Let's run the two queries  $   hive   --database ${USER}_test  SELECT   year ,   max ( runs )   FROM   batting_ext   GROUP   BY   year ;  SELECT   a . year ,   a . player_id ,   a . runs   from   batting_ext   a  JOIN   ( SELECT   year ,   max ( runs )   runs   FROM   batting_ext   GROUP   BY   year   )   b  ON   ( a . year   =   b . year   AND   a . runs   =   b . runs )   ;", 
            "title": "Hive external table"
        }, 
        {
            "location": "/hive/#cleanup", 
            "text": "Once you're done, you can cleanup your environment, deleting the tables and the database.  $   hive  drop   table   temp_batting ;  drop   table   batting ;  drop   table   batting_ext ;  drop   database   ${ env : USER } _test ;", 
            "title": "Cleanup"
        }, 
        {
            "location": "/zeppelin/", 
            "text": "Apache Zeppelin is an online notebook that lets you interact with a HADOOP cluster (or any other hadoop/spark installation) through many languages and technology backends. \n\n\nIn this workshop, we will use Zeppelin to explore data with Spark. \n\n\nEnvironment setup\n\n\nTo simplify, we will use a local Zeppelin running on a Docker container on your local machine. \n\n\nWhat is Docker?\nDocker is a tool designed to make it easier to create, deploy, and run applications by using \ncontainers\n.\nContainers are a way to package software in a format that can run isolated on a shared operating system. Unlike VMs, containers do not bundle a full operating system - only libraries and settings required to make the software work are needed. This makes for efficient, lightweight, self-contained systems and guarantees that software will always run the same, regardless of where it\u2019s deployed.\nBy now, you should have Docker running on your machine as well as snorkel set up. Simply start zeppelin (see \nhow to setup Snorkel ?\n) and navigate to \nhttp://localhost:8080/\n.\n\n\nGetting Started\n\n\nCreating a new notebook\n\n\nOn the home page or on the notebook menu, select \"\ncreate new...\n\". Once the notebook is opened, give it a new name.\n\n\n\n\nCreating folders\n\n\nUsing slashes (\n/\n) in the notebook name will automatically create and/or move the notebook into folders.\n\n\n\n\nBasic concepts\n\n\nA notebook is made of \ncells\n, also called \nparagraphs\n. A cell has an \ninterpreter\n that tells Zeppelin which langage/backend to use to run the cell.\n\n\nThe interpreter is configured by writing \n%\ninterpreter name\n at the top of the cell. Without it, Zeppelin will use the default interpreter, which you can configure by clicking on \n \n interpreters\n at the top right of the notebook (drag-drop to re-order them, the first one being the default).\n\n\nYou can run cells by clicking on the \n icon on the right or using the shortcut \nshift+enter\n. \n\n\n\n\nshortcuts\n\n\nMany useful shortcuts exist in edit mode. Click on \n the at the top right of a notebook to display them.\n\n\n\n\nThe first time you run a cell of a given type, an new \ninstance\n of the interpreter is started (it might take a moment). This instance will then be used for all subsequent run of any cell configured with the same language. This is nice, because it means you can share variables and code between cells. \n\n\n\n\nIn case of trouble\n\n\nIf something goes wrong, you can restart an interpreter any time using the \n \n interpreters\n and then clicking on the \n icon alongside the interpreter name.\n\n\n\n\nList of interpreter prefixes\n\n\nInterpreters and prefixes may vary between the installations. On your local Zeppelin, the following interpreters are available: \n\n\n\n\n\n\n\n\nPrefix\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n%spark\n\n\nSpark (scala)\n\n\n\n\n\n\n%pyspark\n\n\nSpark (python)\n\n\n\n\n\n\n%sql\n\n\nSpark SQL\n\n\n\n\n\n\n%dep\n\n\nspecial syntax to load external dependencies\n\n\n\n\n\n\n%md\n\n\nMarkDown cell\n\n\n\n\n\n\n%sh\n\n\nshell script (bash)\n\n\n\n\n\n\n%python\n\n\n\"regular\" python\n\n\n\n\n\n\n\n\nNote\n: \nspark\n is Spark 1.6, \nspark2\n is Spark 2.1.0.\n\n\nA first example\n\n\nTo test Zeppelin, create a new notebook. \n\n\nShell cell\n: \n\n\nOn the first cell, enter the snippet below:\n\n\n%sh\n\necho\n \nThe working directory is \n$(\npwd\n)\n, it contains:\n\nls\n\n\n\n\nAs you can see, this is a regular shell script that will run inside the docker container. The working directory is \n/zeppelin\n, which contains some folders that are also available from your filesystem:\n\n\n\n\n\n\n\n\nZeppelin path\n\n\nLocal path\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n/zeppelin/data\n\n\nsnorkel/zeppelin/data\n\n\nEmpty directory you can use to store input and output data\n\n\n\n\n\n\n/zeppelin/logs\n\n\nsnorkel/zeppelin/logs\n\n\nZeppelin logs\n\n\n\n\n\n\n/zeppelin/notebooks\n\n\nsnorkel/zeppelin/notebooks\n\n\nWhere Zeppelin stores the notebooks (in a special \n.json\n format)... Don't erase it !\n\n\n\n\n\n\n/zeppelin/spark-warehouse\n\n\nsnorkel/zeppelin/spark-warehouse\n\n\nA special directory used by Spark for storing temp tables and such\n\n\n\n\n\n\n\n\nMarkdown cell\n:\n\n\nOn a new cell, type some markdown and press \nshift+enter\n:\n\n\n%md\n\n#\n Title\n\n##\n Subtitle\nI am testing the \n_Zeppelin_\n functionalities, specifically the \n`%markdown`\n \ninterpreter. I can even do some nice \n__latex math__\n using the \\$\\$ delimiters!\nExample:\n\n$$ \\frac{N}{4} * log(t) $$\n\n\n\n\nLet's do some python:\n\n\nOn a new cell, type the following:\n\n\n%\npython\n\n\nrange\n(\n0\n,\n10\n)\n\n\n\n\n\nYou should have the result \n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n displayed. Now, store the result into a variable:\n\n\n%\npython\n\n\nlst\n \n=\n \nrange\n(\n0\n,\n10\n)\n\n\n\n\n\nAs you can see, no output is shown. This is because the last statement, here an assignment, does not return anything. To still view the result, add \nprint(lst)\n at the end of the cell.\n\n\nInside a new cell, let's define a function:\n\n\n%\npython\n\n\ndef\n \nadd_lists\n(\nlst_a\n,\n \nlst_b\n):\n\n    \n do an addition term by term between two lists \n\n    \nreturn\n \n[\n \na\n \n+\n \nb\n \nfor\n \n(\na\n,\n \nb\n)\n \nin\n \nzip\n(\nlst_a\n,\n \nlst_b\n)\n \n]\n\n\n\n\n\nOnce you ran the cell, the function exists in the current context, so you can use it anywhere in your notebook. In case you need to make a change to the function, simply rerun the cell with the updated code.\n\n\n\n\nWarning\n\n\nWhen you stop the docker container or reload an interpreter, the current context is lost and you need to rerun the cells.\n\n\n\n\n\n\nBattling with pyspark\n\n\nLet's explore the \nbattling.csv\n. It is a csv file containing information on baseball games. The columns are:\n\n\n\n\n\n\n\n\nColumn name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nplayerID\n\n\nPlayer ID code\n\n\n\n\n\n\nyearID\n\n\nYear\n\n\n\n\n\n\nstint\n\n\nplayer's stint (order of appearances within a season)\n\n\n\n\n\n\nteamID\n\n\nTeam\n\n\n\n\n\n\nlgID\n\n\nLeague\n\n\n\n\n\n\nG\n\n\nGames\n\n\n\n\n\n\nG_batting\n\n\nGame as batter\n\n\n\n\n\n\nAB\n\n\nAt Bats\n\n\n\n\n\n\nR\n\n\nRuns\n\n\n\n\n\n\nH\n\n\nHits\n\n\n\n\n\n\n2B\n\n\nDoubles\n\n\n\n\n\n\n3B\n\n\nTriples\n\n\n\n\n\n\nHR\n\n\nHomeruns\n\n\n\n\n\n\nRBI\n\n\nRuns Batted In\n\n\n\n\n\n\nSB\n\n\nStolen Bases\n\n\n\n\n\n\nCS\n\n\nCaught Stealing\n\n\n\n\n\n\nBB\n\n\nBase on Balls\n\n\n\n\n\n\nSO\n\n\nStrikeouts\n\n\n\n\n\n\nIBB\n\n\nIntentional walks\n\n\n\n\n\n\nHBP\n\n\nHit by pitch\n\n\n\n\n\n\nSH\n\n\nSacrifice hits\n\n\n\n\n\n\nSF\n\n\nSacrifice flies\n\n\n\n\n\n\nGIDP\n\n\nGrounded into double plays\n\n\n\n\n\n\n\n\nSpark Context\n\n\nTo interact with Spark, we need a \nSpark Context\n. \n\n\nIn Zeppelin, the context is created for us and available through the \nspark\n variable. Likewise, the \nSpark SQL Context\n is stored in the \nsqlContext\n variable.\n\n\n%\npyspark\n\n\nprint\n(\nspark\n.\nversion\n)\n\n\nprint\n(\nsqlContext\n)\n\n\n\n\n\nLoading the data\n\n\nDownload the \nbattling.csv\n file and save it in \nsnorkel/zeppelin/data\n. \n\n\nTo read CSV data into a \nSpark Dataframe\n, nothing is more easy:\n\n\n%\npysark\n\n\nbattingFile\n \n=\n \ndata/Batting.csv\n\n\nbatting\n \n=\n \nspark\n.\nread\n.\ncsv\n(\nbattingFile\n,\n \n    \nheader\n=\nTrue\n,\n \nmode\n=\nDROPMALFORMED\n,\n \ninferSchema\n=\nTrue\n)\n\n\n\n\n\nVisualising the data\n\n\nA \nSparkDataframe\n has two interesting methods to visualize the content:\n\n\n\n\ndescribe()\n: prints the dataframe \nschema\n, i.e. the columns and their types\n\n\nshow()\n or \nshow(numRows, truncate=truncate)\n: prints the content of dataframe as a table. By default, only the first 20 rows are shown and the content of the cells might be truncated from better readability;\n\n\n\n\n%pyspark\n\n\nbatting\n.\ndescribe\n()\n\n\nbatting\n.\nshow\n()\n\n\n\n\n\nTo have an even better view, we can use the Zeppelin method \nz.show()\n (\nz\n is a global variable used to interact with Zeppelin):\n\n%pyspark\n\n\nz\n.\nshow\n(\nbatting\n)\n\n\n\n\nSimple aggregation\n\n\nA \nSpark Dataframe\n is like a table, with rows and columns. It is possible to do most of the operations you would do on an SQL table.\n\n\nFirst, let's compute some statistics per year:\n\n\n%\npyspark\n\n\n\n# import the sum function \n\n\nfrom\n \npyspark.sql.functions\n \nimport\n \nsum\n \n\n\n# compute aggregations. In SQL, this would be written:\n\n\nstatsPerYear\n \n=\n \nbatting\n\\\n    \n.\ngroupBy\n(\nyearID\n)\n\\\n    \n.\nagg\n(\n\n        \nsum\n(\nR\n)\n.\nalias\n(\ntotal runs\n),\n \n        \nsum\n(\nH\n)\n.\nalias\n(\ntotal hits\n),\n \n        \nsum\n(\nG\n)\n.\nalias\n(\ntotal games\n))\n\\\n    \n.\norderBy\n(\nyearID\n)\n\n\n\nz\n.\nshow\n(\nstatsPerYear\n)\n\n\n\n\n\nSQL equivalent\nIn SQL syntax, this query would look like:\nELECT\n \nR\n \nas\n \ntotal runs\n,\n \nH\n \nas\n \ntotal hits\n,\n \nG\n \nas\n \ntotal games\n\n\nFROM\n \nbatting\n\n\nGROUP\n \nBY\n \nyearID\n\n\nORDER\n \nBY\n \nyearID\n\n\n\n\nOn the interface, select the line chart \n or area chart \n and then click on \nsettings\n. Drag-and-drop the statistics into the \nValues\n area:\n\n\n\n\nInteractive queries\n\n\nZeppelin offers methods to create simple forms. The basic syntax is:\n\n\nz\n.\ninput\n(\ninput name\n,\n \ndefault\n \nvalue\n)\n\n\n\n\n\nLet's use an input form to display the hit by pitch per team for a given year:\n\n\n%\npyspark\n\n\nfrom\n \npyspark.sql.functions\n \nimport\n \navg\n\n\n\n# get user input\n\n\nyear\n \n=\n \nz\n.\ninput\n(\nyear\n,\n \n1894\n)\n\n\n\n# do the query\n\n\nhbp_results\n \n=\n \nbatting\n\\\n    \n.\nfilter\n(\nbatting\n.\nyearID\n \n==\n \nyear\n)\n\\\n    \n.\ngroupBy\n(\nteamID\n)\n\\\n    \n.\nagg\n(\navg\n(\nHBP\n)\n\\\n    \n.\nalias\n(\naverage hit by pitch\n))\n\n\n\n# display the results\n\n\nz\n.\nshow\n(\nhbp_results\n)\n\n\n\n\n\nAs you can see, the query is rerun everytime the input changes.\n\n\nz.input\n creates a simple input text, but you can also use \nz.select(\ninput title\n, labels, values)\n for a dropdown and \nz.checkbox(\ninput title\n, default_value)\n for multiple choices. \n\n\nFor example, we could create a dropdown for all teams like this:\n\n\n%\npyspark\n\n\n# get all team names\n\n\nall_teams\n \n=\n \nbatting\n.\nselect\n(\nteamID\n)\n.\ndistinct\n()\n.\ncollect\n()\n\n\n# we get a list of Row(teamsId). Get rid of the Row wrapper!\n\n\nall_teams\n \n=\n \n[\nr\n[\n0\n]\n \nfor\n \nr\n \nin\n \nall_teams\n]\n\n\n\n# create and show a dropdown form\n\n\nteam\n \n=\n \nz\n.\nselect\n(\nselected team\n,\n \nsorted\n(\nzip\n(\nall_teams\n,\n \nall_teams\n)))\n\n\n\n# go something with the results, for example listing the years when the team played\n\n\nyears_played\n \n=\n \nbatting\n\\\n    \n.\nselect\n(\nbatting\n.\nyearID\n.\nalias\n(\nyears played\n))\n\\\n    \n.\nwhere\n(\nbatting\n.\nteamID\n \n==\n \nteam\n)\n\\\n    \n.\ndistinct\n()\n\\\n    \n.\norderBy\n(\nyears played\n)\n\n\n\nz\n.\nshow\n(\nyears_played\n)\n\n\n\n\n\nHave a look at the \nZeppelin documentation\n for more information and examples.\n\n\n\n\nBattling with Spark SQL\n\n\nFrom the code so far, you might have noticed how similar to SQL queries our code were. What if we could get rid of the difficult python syntax and use a declarative language instead ? Well, we can. \n\n\nRegistering a table\n\n\nTo use the SQL syntax directly, we first need to register our dataframe as a table and give it name:\n\n\n%\npyspark\n\n\nbatting\n.\ncreateOrReplaceTempView\n(\nbatting\n)\n \n\n# or registerTempTable(\nbatting\n) for older spark versions\n\n\n\n\n\nThe name can be anything; we will be used in SQL queries, for example in a \nFROM\n clause, to refer to the \nbatting\n dataframe.\n\n\nSimple aggregation\n\n\nFrom now on, we can run SQL-like queries using the \nsqlContext\n. For example (the triple quotes in python are for multi-line strings):\n\n\n%\npyspark\n\n\nresult\n \n=\n \nsqlContext\n.\nsql\n(\n\n    \n\n\n    SELECT teamID, max(H) as `max hits` \n\n\n    FROM batting \n\n\n    GROUP BY teamID \n\n\n    ORDER BY(`max hits`) \n\n\n    DESC LIMIT 10\n\n\n    \n)\n\n\nz\n.\nshow\n(\nresult\n)\n\n\n\n\n\n\n\nColumn aliases with spaces or special characters must be enclosed in \nbackticks\n and not \nstraight quotes\n !\n\n\n\n\nBut this is still python. Thanks to Zeppelin, it is possible to run Spark SQL queries and display results directly using a \nSpark SQL\n cell. So our query now looks like:\n\n\n%\nsql\n  \n-- tell Zeppelin to use the Spark SQL interpreter\n\n\nSELECT\n \nteamID\n,\n \nmax\n(\nH\n)\n \nas\n \n`\nmax\n \nhits\n`\n \n\nFROM\n \nbatting\n \n\nGROUP\n \nBY\n \nteamID\n \n\nORDER\n \nBY\n(\n`\nmax\n \nhits\n`\n)\n \n\nDESC\n \nLIMIT\n \n10\n\n\n\n\n\nInteractive queries\n\n\nForms fields can also be used in an SQL cell, but the syntax is a bit different:\n\n\n\n\nsimple form: \n${\ninput_name\n=\ndefault_value\n}\n\n\ndropdown: \n${\ninput_name\n=\ndefault_value\n,\nvalue1\n|\nvalue2\n|\nvalueX\n}\n\n\n\n\nFor example, using an input:\n\n\n%\nsql\n\n\nselect\n \nteamID\n,\n \navg\n(\nHBP\n)\n \nas\n \n`\naverage\n \nhit\n \nper\n \npitch\n`\n\n\nfrom\n \nbatting\n\n\nwhere\n \nyearID\n \n=\n \n${\nyear\n=\n1984\n}\n \n-- let the user choose the year\n\n\ngroup\n \nby\n \nteamID\n\n\n\n\n\nusing a dropdown:\n\n\n%\nsql\n\n\nselect\n \ndistinct\n(\nyearID\n)\n \nas\n \n`\nyears\n \nplayed\n`\n\n\nfrom\n \nbatting\n\n\nwhere\n \nteamID\n \n=\n \n${team=OAK,OAK|SFN|SDN|PHI}\n \n-- let the user choose one 4 teams\n\n\norder\n \nby\n \n`\nyears\n \nplayed\n`\n\n\n\n.", 
            "title": "Zeppelin/Spark SQL"
        }, 
        {
            "location": "/zeppelin/#environment-setup", 
            "text": "To simplify, we will use a local Zeppelin running on a Docker container on your local machine.   What is Docker? Docker is a tool designed to make it easier to create, deploy, and run applications by using  containers . Containers are a way to package software in a format that can run isolated on a shared operating system. Unlike VMs, containers do not bundle a full operating system - only libraries and settings required to make the software work are needed. This makes for efficient, lightweight, self-contained systems and guarantees that software will always run the same, regardless of where it\u2019s deployed. By now, you should have Docker running on your machine as well as snorkel set up. Simply start zeppelin (see  how to setup Snorkel ? ) and navigate to  http://localhost:8080/ .", 
            "title": "Environment setup"
        }, 
        {
            "location": "/zeppelin/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/zeppelin/#creating-a-new-notebook", 
            "text": "On the home page or on the notebook menu, select \" create new... \". Once the notebook is opened, give it a new name.   Creating folders  Using slashes ( / ) in the notebook name will automatically create and/or move the notebook into folders.", 
            "title": "Creating a new notebook"
        }, 
        {
            "location": "/zeppelin/#basic-concepts", 
            "text": "A notebook is made of  cells , also called  paragraphs . A cell has an  interpreter  that tells Zeppelin which langage/backend to use to run the cell.  The interpreter is configured by writing  % interpreter name  at the top of the cell. Without it, Zeppelin will use the default interpreter, which you can configure by clicking on     interpreters  at the top right of the notebook (drag-drop to re-order them, the first one being the default).  You can run cells by clicking on the   icon on the right or using the shortcut  shift+enter .    shortcuts  Many useful shortcuts exist in edit mode. Click on   the at the top right of a notebook to display them.   The first time you run a cell of a given type, an new  instance  of the interpreter is started (it might take a moment). This instance will then be used for all subsequent run of any cell configured with the same language. This is nice, because it means you can share variables and code between cells.    In case of trouble  If something goes wrong, you can restart an interpreter any time using the     interpreters  and then clicking on the   icon alongside the interpreter name.", 
            "title": "Basic concepts"
        }, 
        {
            "location": "/zeppelin/#list-of-interpreter-prefixes", 
            "text": "Interpreters and prefixes may vary between the installations. On your local Zeppelin, the following interpreters are available:      Prefix  Description      %spark  Spark (scala)    %pyspark  Spark (python)    %sql  Spark SQL    %dep  special syntax to load external dependencies    %md  MarkDown cell    %sh  shell script (bash)    %python  \"regular\" python     Note :  spark  is Spark 1.6,  spark2  is Spark 2.1.0.", 
            "title": "List of interpreter prefixes"
        }, 
        {
            "location": "/zeppelin/#a-first-example", 
            "text": "To test Zeppelin, create a new notebook.   Shell cell :   On the first cell, enter the snippet below:  %sh echo   The working directory is  $( pwd ) , it contains: \nls  As you can see, this is a regular shell script that will run inside the docker container. The working directory is  /zeppelin , which contains some folders that are also available from your filesystem:     Zeppelin path  Local path  Description      /zeppelin/data  snorkel/zeppelin/data  Empty directory you can use to store input and output data    /zeppelin/logs  snorkel/zeppelin/logs  Zeppelin logs    /zeppelin/notebooks  snorkel/zeppelin/notebooks  Where Zeppelin stores the notebooks (in a special  .json  format)... Don't erase it !    /zeppelin/spark-warehouse  snorkel/zeppelin/spark-warehouse  A special directory used by Spark for storing temp tables and such     Markdown cell :  On a new cell, type some markdown and press  shift+enter :  %md #  Title ##  Subtitle\nI am testing the  _Zeppelin_  functionalities, specifically the  `%markdown`  \ninterpreter. I can even do some nice  __latex math__  using the \\$\\$ delimiters!\nExample:\n\n$$ \\frac{N}{4} * log(t) $$  Let's do some python:  On a new cell, type the following:  % python  range ( 0 , 10 )   You should have the result  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  displayed. Now, store the result into a variable:  % python  lst   =   range ( 0 , 10 )   As you can see, no output is shown. This is because the last statement, here an assignment, does not return anything. To still view the result, add  print(lst)  at the end of the cell.  Inside a new cell, let's define a function:  % python  def   add_lists ( lst_a ,   lst_b ): \n      do an addition term by term between two lists  \n     return   [   a   +   b   for   ( a ,   b )   in   zip ( lst_a ,   lst_b )   ]   Once you ran the cell, the function exists in the current context, so you can use it anywhere in your notebook. In case you need to make a change to the function, simply rerun the cell with the updated code.   Warning  When you stop the docker container or reload an interpreter, the current context is lost and you need to rerun the cells.", 
            "title": "A first example"
        }, 
        {
            "location": "/zeppelin/#battling-with-pyspark", 
            "text": "Let's explore the  battling.csv . It is a csv file containing information on baseball games. The columns are:     Column name  Description      playerID  Player ID code    yearID  Year    stint  player's stint (order of appearances within a season)    teamID  Team    lgID  League    G  Games    G_batting  Game as batter    AB  At Bats    R  Runs    H  Hits    2B  Doubles    3B  Triples    HR  Homeruns    RBI  Runs Batted In    SB  Stolen Bases    CS  Caught Stealing    BB  Base on Balls    SO  Strikeouts    IBB  Intentional walks    HBP  Hit by pitch    SH  Sacrifice hits    SF  Sacrifice flies    GIDP  Grounded into double plays", 
            "title": "Battling with pyspark"
        }, 
        {
            "location": "/zeppelin/#spark-context", 
            "text": "To interact with Spark, we need a  Spark Context .   In Zeppelin, the context is created for us and available through the  spark  variable. Likewise, the  Spark SQL Context  is stored in the  sqlContext  variable.  % pyspark  print ( spark . version )  print ( sqlContext )", 
            "title": "Spark Context"
        }, 
        {
            "location": "/zeppelin/#loading-the-data", 
            "text": "Download the  battling.csv  file and save it in  snorkel/zeppelin/data .   To read CSV data into a  Spark Dataframe , nothing is more easy:  % pysark  battingFile   =   data/Batting.csv  batting   =   spark . read . csv ( battingFile ,  \n     header = True ,   mode = DROPMALFORMED ,   inferSchema = True )", 
            "title": "Loading the data"
        }, 
        {
            "location": "/zeppelin/#visualising-the-data", 
            "text": "A  SparkDataframe  has two interesting methods to visualize the content:   describe() : prints the dataframe  schema , i.e. the columns and their types  show()  or  show(numRows, truncate=truncate) : prints the content of dataframe as a table. By default, only the first 20 rows are shown and the content of the cells might be truncated from better readability;   %pyspark  batting . describe ()  batting . show ()   To have an even better view, we can use the Zeppelin method  z.show()  ( z  is a global variable used to interact with Zeppelin): %pyspark  z . show ( batting )", 
            "title": "Visualising the data"
        }, 
        {
            "location": "/zeppelin/#simple-aggregation", 
            "text": "A  Spark Dataframe  is like a table, with rows and columns. It is possible to do most of the operations you would do on an SQL table.  First, let's compute some statistics per year:  % pyspark  # import the sum function   from   pyspark.sql.functions   import   sum   # compute aggregations. In SQL, this would be written:  statsPerYear   =   batting \\\n     . groupBy ( yearID ) \\\n     . agg ( \n         sum ( R ) . alias ( total runs ),  \n         sum ( H ) . alias ( total hits ),  \n         sum ( G ) . alias ( total games )) \\\n     . orderBy ( yearID )  z . show ( statsPerYear )   SQL equivalent In SQL syntax, this query would look like: ELECT   R   as   total runs ,   H   as   total hits ,   G   as   total games  FROM   batting  GROUP   BY   yearID  ORDER   BY   yearID   On the interface, select the line chart   or area chart   and then click on  settings . Drag-and-drop the statistics into the  Values  area:", 
            "title": "Simple aggregation"
        }, 
        {
            "location": "/zeppelin/#interactive-queries", 
            "text": "Zeppelin offers methods to create simple forms. The basic syntax is:  z . input ( input name ,   default   value )   Let's use an input form to display the hit by pitch per team for a given year:  % pyspark  from   pyspark.sql.functions   import   avg  # get user input  year   =   z . input ( year ,   1894 )  # do the query  hbp_results   =   batting \\\n     . filter ( batting . yearID   ==   year ) \\\n     . groupBy ( teamID ) \\\n     . agg ( avg ( HBP ) \\\n     . alias ( average hit by pitch ))  # display the results  z . show ( hbp_results )   As you can see, the query is rerun everytime the input changes.  z.input  creates a simple input text, but you can also use  z.select( input title , labels, values)  for a dropdown and  z.checkbox( input title , default_value)  for multiple choices.   For example, we could create a dropdown for all teams like this:  % pyspark  # get all team names  all_teams   =   batting . select ( teamID ) . distinct () . collect ()  # we get a list of Row(teamsId). Get rid of the Row wrapper!  all_teams   =   [ r [ 0 ]   for   r   in   all_teams ]  # create and show a dropdown form  team   =   z . select ( selected team ,   sorted ( zip ( all_teams ,   all_teams )))  # go something with the results, for example listing the years when the team played  years_played   =   batting \\\n     . select ( batting . yearID . alias ( years played )) \\\n     . where ( batting . teamID   ==   team ) \\\n     . distinct () \\\n     . orderBy ( years played )  z . show ( years_played )   Have a look at the  Zeppelin documentation  for more information and examples.", 
            "title": "Interactive queries"
        }, 
        {
            "location": "/zeppelin/#battling-with-spark-sql", 
            "text": "From the code so far, you might have noticed how similar to SQL queries our code were. What if we could get rid of the difficult python syntax and use a declarative language instead ? Well, we can.", 
            "title": "Battling with Spark SQL"
        }, 
        {
            "location": "/zeppelin/#registering-a-table", 
            "text": "To use the SQL syntax directly, we first need to register our dataframe as a table and give it name:  % pyspark  batting . createOrReplaceTempView ( batting )   # or registerTempTable( batting ) for older spark versions   The name can be anything; we will be used in SQL queries, for example in a  FROM  clause, to refer to the  batting  dataframe.", 
            "title": "Registering a table"
        }, 
        {
            "location": "/zeppelin/#simple-aggregation_1", 
            "text": "From now on, we can run SQL-like queries using the  sqlContext . For example (the triple quotes in python are for multi-line strings):  % pyspark  result   =   sqlContext . sql ( \n          SELECT teamID, max(H) as `max hits`       FROM batting       GROUP BY teamID       ORDER BY(`max hits`)       DESC LIMIT 10       )  z . show ( result )    Column aliases with spaces or special characters must be enclosed in  backticks  and not  straight quotes  !   But this is still python. Thanks to Zeppelin, it is possible to run Spark SQL queries and display results directly using a  Spark SQL  cell. So our query now looks like:  % sql    -- tell Zeppelin to use the Spark SQL interpreter  SELECT   teamID ,   max ( H )   as   ` max   hits `   FROM   batting   GROUP   BY   teamID   ORDER   BY ( ` max   hits ` )   DESC   LIMIT   10", 
            "title": "Simple aggregation"
        }, 
        {
            "location": "/zeppelin/#interactive-queries_1", 
            "text": "Forms fields can also be used in an SQL cell, but the syntax is a bit different:   simple form:  ${ input_name = default_value }  dropdown:  ${ input_name = default_value , value1 | value2 | valueX }   For example, using an input:  % sql  select   teamID ,   avg ( HBP )   as   ` average   hit   per   pitch `  from   batting  where   yearID   =   ${ year = 1984 }   -- let the user choose the year  group   by   teamID   using a dropdown:  % sql  select   distinct ( yearID )   as   ` years   played `  from   batting  where   teamID   =   ${team=OAK,OAK|SFN|SDN|PHI}   -- let the user choose one 4 teams  order   by   ` years   played `  \n.", 
            "title": "Interactive queries"
        }, 
        {
            "location": "/spark/", 
            "text": "In this workshop, we will create a search engine for BBC articles using pyspark and the Spark ML library. \n\n\n\n\npyspark documentation\n\n\nML guide\n\n\n\n\nIntroduction\n\n\nWhat we will do\n\n\nTo create the search engine, we will do the following steps:\n\n\n\n\nWhat we will use\n\n\nLSA, \nlatent semantic anlysis\n is\n\n\n\n\na technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms\n (source: wikipedia)\n\n\n\n\nIn summary, we want to establish underlying relationships between documents in a collection by linking documents and terms through \nconcepts\n. Those concepts are deduced or constructed through a statistical or mathematical algorithm that forms a \nmodel\n. Once we have the model, we can begin using it for search queries.\n\n\nAvailable algorithms\n\n\nThere are two well-known methods for discovering underlying topics:\n\n\n\n\n\n\nSVD\n, \nSingular Value Decomposition\n: is a mathematical method for decomposing a matrix into a product of smaller matrices. It has the advantage of being \nmathematically correct\n: computing the model, hence  the decomposed matrices, takes only one pass and is deterministic: the same data will always give the same result (as long as the same parameters are used).\n\n\n\n\n\n\nLDA\n, \nLatent Dirichlet Allocation\n, is a generative probabilistic model. Based on statistics, many iterations are necessary to get a good-enough model and every run could give a different result. The result is also highly dependant on the parameters we use for the training. \n\n\n\n\n\n\nIn this workshop, we will use the LDA technique.\n\n\nLDA Basics\n\n\nLDA is like a clustering algorithm where :\n\n\n\n\nthe \ncluster centers\n are the topics;\n\n\nthe \ntraining examples\n are documents;\n\n\nthe \nfeatures\n are the word frequencies;\n\n\nthe distance is \neuclidean\n based on a statistical model (Bayesian inference rules / Dirichlet distribution)\n\n\n\n\nAs in clustering, we need to give some informations (\nparameters\n) to the model. The most important one is \nthe number of topics (k)\n we think there is (exactly as we need to specify the number of clusters to find).\n\n\nDuring training, the model will first assign each document to a random topic. Then, on each iteration, it computes how well the actual topic distribution describes/predicts each document, makes adjustments and try again. Most of the time, the programmer will set in advance the \nmaximum number of iterations\n to do.\n\n\nIn the end, the model outputs a topic distribution over document (how much a document is important to a given topic) and over terms (how much the term describes the topic). Documents with similar topic distributions are likely to be similar, even if they don't use the same words.\n\n\nLDA with pyspark\n\n\nGetting the dataset\n\n\nThe dataset \"\nDataset: BBC\n\" available at: \nhttp://mlg.ucd.ie/datasets/bbc.html\n (we used the raw text files).  It consists of 2225 documents from the BBC news website corresponding to stories in five topical areas (business, entertainment, politics, sport and tech) from 2004-2005.\n\n\nSince the dataset comes in multiple folders and with some duplicates, we already processed it and made it available in csv or json format. If you are interested, you can get \nthe python script\n we used for the parsing.\n\n\nFor the workshop, download \nbbc.json\n and save it in the \nsnorkel/zeppelin/data\n folder.\n\n\nLoading the dataset into Spark\n\n\nCreate a new notebook. On the first cell, ensure that the \nbbc.json\n file is were it should be by running:\n\n\n%sh\nls data/bbc.json\n\n\n\n\nThe file should be listed in the output.\n\n\nNow, let's load those data into a Spark dataframe (\ndocumentation\n):\n\n\n%\npyspark\n\n\ndataset\n \n=\n \nspark\n.\nread\n.\njson\n(\ndata/bbc.json\n)\n\n\n\n\n\nEnsure this worked by inspecting the dataset:\n\n\n%\npyspark\n\n\nprint\n(\nnumber of documents: \n%d\n \n%\n \ndataset\n.\ncount\n())\n\n\nprint\n(\nstructure: \n%s\n \n%\n \ndataset\n.\ndescribe\n())\n\n\ndataset\n.\nshow\n()\n\n\n\n\n\nnumber of documents: 2096\nstructure: DataFrame[summary: string, category: string, content: string, filename: string, title: string]\n...\n\n\n\n\nAdding IDs and creating lookup tables\n\n\nIn order to keep track of our articles, let's give each line of our \"table\" a unique ID:\n\n\n%\npyspark\n\n\n\nfrom\n \npyspark.sql.functions\n \nimport\n \nmonotonically_increasing_id\n\n\nfrom\n \npyspark.sql.types\n \nimport\n \nLongType\n\n\n\n# This will return a new DF with all the columns + id\n\n\ndataset\n \n=\n \ndataset\n.\nwithColumn\n(\nid\n,\n \nmonotonically_increasing_id\n())\n\n\ndataset\n.\nshow\n(\n5\n)\n\n\n\n\n\nThen, it would be nice to be quickly able to get an ID given a title and vice-versa. So let's create lookup dictionaries:\n\n\n%\npyspark\n\n\ndocTitles\n \n=\n  \ndict\n([(\nr\n[\n0\n],\n \nr\n[\n1\n])\n \nfor\n \nr\n \nin\n \ndataset\n.\nselect\n(\nid\n,\n \ntitle\n)\n.\ncollect\n()])\n \n\ndocIds\n \n=\n \ndict\n([(\nr\n[\n1\n],\n \nr\n[\n0\n])\n \nfor\n \nr\n \nin\n \ndataset\n.\nselect\n(\nid\n,\n \ntitle\n)\n.\ncollect\n()])\n\n\n\n\n\nIn those lines, we select two columns from our dataframe: \nid\n and \ntitle\n. Using \ncollect\n, we ask Spark to actually execute the selection (it is an \naction\n) and get the results into a list of \nRow\n. A \nRow\n behaves like an untyped array. We convert our rows into tuples and finally cast the list of tuples into a dictionary for quick lookup.\n\n\nTo ensure it works, try to lookup the title of the document with ID 0: \ndocTitles[0]\n.\n\n\nFeatures extraction\n\n\nIn our case, the feature extraction is applied on the articles body (column \ncontent\n) and contains the following steps:\n\n\n\n\nbreaking content into words (\ntokenisation\n)\n\n\nremoving common words (\nstopwords removal\n)\n\n\ncomputing the frequencies of each word (\ncount vectors\n) and selecting a subset of words (\nvocabulary\n)\n\n\ncomputing the TF-IDF over documents and vocabulary\n\n\n\n\nAs you will see, all those steps are very common and can be performed using Spark ML utilities.\n\n\nTokenisation\n\n\nThe first thing to do is to break our article's content into tokens. \n\n\nHave a look at the Spark ML documentation and try to figure out how to tokenise our \ncontent\n column: \nSpark ML documentation: tokenizer\n.\n\n\nTips\n: discard words of less than 4 characters, save the result into a \ntokens\n column.\n\n\nSolution\nWe use the \nRegexTokenizer\n with the following arguments:\nregex\n: break by white space character(s) \ninputCol\n: name of the input column, here \ncontent\noutputCol\n: name of the new column with tokens, here \ntokens\nminTokenLength\n: discard tokens with length \n 4\n%\npyspark\n\n\n# import the RegexTokenizer class\n\n\nfrom\n \npyspark.ml.feature\n \nimport\n \nRegexTokenizer\n\n\n# create  a new tokenizer instance\n\n\ntokenizer\n \n=\n \nRegexTokenizer\n(\npattern\n=\n[\n\\\\\nW_]+\n,\n \ninputCol\n=\ncontent\n,\n \n    \noutputCol\n=\ntokens\n,\n \nminTokenLength\n=\n4\n)\n\n\n# tokenise (and select only the interesting columns to gain space)\n\n\ntokenized_df\n \n=\n \ntokenizer\n.\ntransform\n(\ndataset\n)\n.\nselect\n(\nid\n,\n \ntitle\n,\n \ncontent\n,\n \ntokens\n)\n\n\n\n\nTo visualize the results, lets print the first 100 characters of the first article and some of the resulting tokens:\n\n%\npyspark\n\n\n\ncontent\n \n=\n \ntokenized_df\n.\nselect\n(\ncontent\n)\n.\nfirst\n()\n\n\ntokens\n \n=\n \ntokenized_df\n.\nselect\n(\ntokens\n)\n.\nfirst\n()\n\n\n\nprint\n(\ncontent: \n%s\n...\n \n%\n \ncontent\n[\n0\n][:\n100\n])\n\n\nprint\n(\ntokens:  \n%s\n..\n \n%\n \n, \n.\njoin\n(\ntokens\n[\n0\n][:\n10\n]))\n\n\n\n\nResult:\n\ncontent\n:\n \nQuarterly\n \nprofits\n \nat\n \nUS\n \nmedia\n \ngiant\n \nTimeWarner\n \njumped\n \n76\n%\n \nto\n \n$1\n.\n13\nbn\n \n(\n\u00a3\n600\nm\n)\n \nfor\n \nthe\n \nthree\n \nmonths\n \nto\n...\n\n\ntokens\n:\n  \nquarterly\n,\n \nprofits\n,\n \nmedia\n,\n \ngiant\n,\n \ntimewarner\n,\n \njumped\n,\n \n13\nbn\n,\n \n600\nm\n,\n \nthree\n,\n \nmonths\n..\n\n\n\n\nThe difficulties of tokenisation\nTokenisation is very important in language processing, but it is not an easy task. Here, we separated words based on spaces and removed small words. But is it always a good idea ? It depends... For example:\nquarterly profit\n \n \nquarterly\n, \nprofit\n seems good. But what about \nNew York\n ? Breaking it down into two tokens might change the way our search engine treats a query about \nNew York\n... Indeed, if it is considered two tokens, a document with a text \"there is a \nnew\n professor at the \nYork\n university\" becomes relevant as well;\nBy removing small words, we might loose informations, such as \nUS\n or \nUSA\n;\nHere, our tokeniser treats \n13bn\n or \n600m\n as tokens. But we can wonder if words with non-letter characters should be kept or not.\nIn real-life applications, the tokenizer is often way more complex and augmented with context-sensitive rules and heuristics.\nStopwords removal\n\n\nWe want to remove common words that are likely to appear in every document, hence not very informative. Here, we will use the following:\n\n\n%\npyspark\n\n\n# stopwords to use: copy-paste it into a cell\n\n\nstopwords\n \n=\n \nbove,abst,accordance,according,accordingly,across,act,actually,added,adj,affected,affecting,affects,after,afterwards,again,against,ah,all,almost,alone,along,already,also,although,always,am,among,amongst,an,and,announce,another,any,anybody,anyhow,anymore,anyone,anything,anyway,anyways,anywhere,apparently,approximately,are,aren,arent,arise,around,as,aside,ask,asking,at,auth,available,away,awfully,b,back,be,became,because,become,becomes,becoming,been,before,beforehand,begin,beginning,beginnings,begins,behind,being,believe,below,beside,besides,between,beyond,biol,both,brief,briefly,but,by,c,ca,came,can,cannot,can\nt,cause,causes,certain,certainly,co,com,come,comes,contain,containing,contains,could,couldnt,d,date,did,didn\nt,different,do,does,doesn\nt,doing,done,don\nt,down,downwards,due,during,e,each,ed,edu,effect,eg,eight,eighty,either,else,elsewhere,end,ending,enough,especially,et,et-al,etc,even,ever,every,everybody,everyone,everything,everywhere,ex,except,f,far,few,ff,fifth,first,five,fix,followed,following,follows,for,former,formerly,forth,found,four,from,further,furthermore,g,gave,get,gets,getting,give,given,gives,giving,go,goes,gone,got,gotten,h,had,happens,hardly,has,hasn\nt,have,haven\nt,having,he,hed,hence,her,here,hereafter,hereby,herein,heres,hereupon,hers,herself,hes,hi,hid,him,himself,his,hither,home,how,howbeit,however,hundred,i,id,ie,if,i\nll,im,immediate,immediately,importance,important,in,inc,indeed,index,information,instead,into,invention,inward,is,isn\nt,it,itd,it\nll,its,itself,i\nve,j,just,k,keep,keeps,kept,kg,km,know,known,knows,l,largely,last,lately,later,latter,latterly,least,less,lest,let,lets,like,liked,likely,line,little,\nll,look,looking,looks,ltd,m,made,mainly,make,makes,many,may,maybe,me,mean,means,meantime,meanwhile,merely,mg,might,million,miss,ml,more,moreover,most,mostly,mr,mrs,much,mug,must,my,myself,n,na,name,namely,nay,nd,near,nearly,necessarily,necessary,need,needs,neither,never,nevertheless,new,next,nine,ninety,no,nobody,non,none,nonetheless,noone,nor,normally,nos,not,noted,nothing,now,nowhere,o,obtain,obtained,obviously,of,off,often,oh,ok,okay,old,omitted,on,once,one,ones,only,onto,or,ord,other,others,otherwise,ought,our,ours,ourselves,out,outside,over,overall,owing,own,p,page,pages,part,particular,particularly,past,per,perhaps,placed,please,plus,poorly,possible,possibly,potentially,pp,predominantly,present,previously,primarily,probably,promptly,proud,provides,put,q,que,quickly,quite,qv,r,ran,rather,rd,re,readily,really,recent,recently,ref,refs,regarding,regardless,regards,related,relatively,research,respectively,resulted,resulting,results,right,run,s,said,same,saw,say,saying,says,sec,section,see,seeing,seem,seemed,seeming,seems,seen,self,selves,sent,seven,several,shall,she,shed,she\nll,shes,should,shouldn\nt,show,showed,shown,showns,shows,significant,significantly,similar,similarly,since,six,slightly,so,some,somebody,somehow,someone,somethan,something,sometime,sometimes,somewhat,somewhere,soon,sorry,specifically,specified,specify,specifying,still,stop,strongly,sub,substantially,successfully,such,sufficiently,suggest,sup,sure,t,take,taken,taking,tell,tends,th,than,thank,thanks,thanx,that,that\nll,thats,that\nve,the,their,theirs,them,themselves,then,thence,there,thereafter,thereby,thered,therefore,therein,there\nll,thereof,therere,theres,thereto,thereupon,there\nve,these,they,theyd,they\nll,theyre,they\nve,think,this,those,thou,though,thoughh,thousand,throug,through,throughout,thru,thus,til,tip,to,together,too,took,toward,towards,tried,tries,truly,try,trying,ts,twice,two,u,un,under,unfortunately,unless,unlike,unlikely,until,unto,up,upon,ups,us,use,used,useful,usefully,usefulness,uses,using,usually,v,value,various,\nve,very,via,viz,vol,vols,vs,w,want,wants,was,wasnt,way,we,wed,welcome,we\nll,went,were,werent,we\nve,what,whatever,what\nll,whats,when,whence,whenever,where,whereafter,whereas,whereby,wherein,wheres,whereupon,wherever,whether,which,while,whim,whither,who,whod,whoever,whole,who\nll,whom,whomever,whos,whose,why,widely,willing,wish,with,within,without,wont,words,world,would,wouldnt,www,x,y,yes,yet,you,youd,you\nll,your,youre,yours,yourself,yourselves,you\nve,z,zero,article,about,writes,entry,well,will,newsgroup\n.\nsplit\n(\n,\n)\n\n\n\n\n\nLook at the \nSpark ML documentation\n to remove all the stopwords from our \ntokens\n column using a \nStopWordsRemover\n.\n\n\nSolution\nThe code is very similar to the tokenisation one:\n%\npyspark\n\n\nfrom\n \npyspark.ml.feature\n \nimport\n \nStopWordsRemover\n\n\n\nremover\n \n=\n \nStopWordsRemover\n(\ninputCol\n=\ntokens\n,\n \noutputCol\n=\nfiltered\n,\n \nstopWords\n=\nstopwords\n)\n\n\nfiltered_df\n \n=\n \nremover\n.\ntransform\n(\ntokenized_df\n)\n\n\n\n\nTo visualise the results, let's print the first 18 tokens of a document, with and without stopwords:\n\n\n%\npyspark\n\n\ntokens\n \n=\n \nfiltered_df\n.\nselect\n(\ntokens\n)\n.\nfirst\n()\n\n\nfiltered_tokens\n \n=\n \nfiltered_df\n.\nselect\n(\nfiltered\n)\n.\nfirst\n()\n\n\n\nprint\n(\n  tokens: \n%s\n \n%\n \n, \n.\njoin\n(\ntokens\n[\n0\n][:\n18\n]))\n\n\nprint\n(\nfiltered: \n%s\n \n%\n \n, \n.\njoin\n(\nfiltered_tokens\n[\n0\n][:\n18\n]))\n\n\n\n\n\nResult:\n\n\n  tokens: quarterly, profits, media, giant, timewarner, jumped, 13bn, 600m, three, months, december, from, 639m, year, earlier, firm, which, biggest\nfiltered: quarterly, profits, media, giant, timewarner, jumped, 13bn, 600m, three, months, december, 639m, year, earlier, firm, biggest, investors, google\n\n\nHere, \nfrom\n and \nwhich\n have been removed.\n\n\nWord frequencies and vocabulary\n\n\nUsing the \nSpark ML CountVectorizer\n, try to find how to create a model with a vocabulary of 3000 words and a minimum document frequency of 5. Then, use the model over our \nfiltered_df\n dataset to create a new column named \ntoken_counts\n.\n\n\nSolution\n%\npyspark\n\n\nfrom\n \npyspark.ml.feature\n \nimport\n \nCountVectorizer\n\n\n\nvocabSize\n \n=\n \n3000\n \n# the maximum number of term to retain\n\n\nminDF\n \n=\n \n5\n        \n# the minimum number of different documents a \n\n                 \n# term must appear in to be included in the vocabulary.\n\n\n\nvectorizer\n \n=\n \nCountVectorizer\n(\ninputCol\n \n=\n \nfiltered\n,\n \noutputCol\n \n=\n \ntoken_counts\n,\n \nvocabSize\n=\nvocabSize\n,\n \nminDF\n=\nminDF\n)\n\n\n\ncount_model\n \n=\n \nvectorizer\n.\nfit\n(\nfiltered_df\n)\n       \n# create model\n\n\ncounted_df\n \n=\n \ncount_model\n.\ntransform\n(\nfiltered_df\n)\n \n# apply model to our df\n\n\nvocabulary\n \n=\n \ncount_model\n.\nvocabulary\n             \n# extract the vocabulary\n\n\n\n\nIf you look at our \ntoken_counts\n column, here is what we get:\n\n\n%\npyspark\n\n\nfirst_doc_freqs\n \n=\n \ncounted_df\n.\nselect\n(\ntoken_counts\n)\n.\nfirst\n()\n\n\nfirst_doc_freqs\n[\n0\n]\n\n\n\n\n\nSparseVector(3000, {0: 4.0, 2: 2.0, 8: 2.0, 9: 2.0, 11: 1.0, 13: 1.0, 14: 2.0, 25: 1.0, 32: 2.0, 40: 3.0, 43: 1.0, 44: 1.0, 47: 1.0, 55: 1.0, 56: 1.0, 57: 2.0, 65: 2.0, 67: 1.0, 68: 1.0, 69: 2.0, 71: 1.0, 85: 1.0, 89: 1.0, 102: 1.0, 109: 1.0, 115: 1.0, 118: 1.0, 119: 1.0, 123: 1.0, 132: 1.0, 137: 2.0, 140: 4.0, 149: 1.0, 150: 1.0, 156: 1.0, 159: 1.0, 160: 1.0, 161: 1.0, 170: 2.0, 179: 3.0, 189: 1.0, 190: 1.0, 202: 1.0, 205: 1.0, 209: 2.0, 227: 1.0, 248: 1.0, 249: 1.0, 261: 1.0, 283: 1.0, 300: 2.0, 327: 1.0, 331: 2.0, 333: 1.0, 340: 3.0, 342: 1.0, 360: 5.0, 364: 2.0, 410: 1.0, 440: 1.0, 464: 1.0, 467: 1.0, 489: 1.0, 498: 1.0, 504: 1.0, 544: 2.0, 549: 1.0, 560: 1.0, 562: 1.0, 589: 2.0, 632: 1.0, 687: 4.0, 742: 2.0, 779: 1.0, 780: 1.0, 797: 1.0, 806: 3.0, 828: 1.0, 890: 1.0, 931: 1.0, 938: 1.0, 951: 1.0, 960: 1.0, 971: 1.0, 1017: 2.0, 1066: 1.0, 1129: 1.0, 1177: 2.0, 1290: 1.0, 1311: 1.0, 1360: 1.0, 1437: 1.0, 1451: 1.0, 1462: 1.0, 1597: 2.0, 1639: 1.0, 1674: 1.0, 1719: 1.0, 1732: 1.0, 1733: 1.0, 1906: 1.0, 1987: 1.0, 2034: 1.0, 2090: 1.0, 2120: 1.0, 2188: 1.0, 2199: 1.0, 2265: 1.0, 2305: 2.0, 2306: 1.0, 2392: 1.0, 2443: 1.0, 2467: 1.0, 2670: 1.0, 2742: 1.0})\n\n\n\n\nAbout SparseVector\nA sparse vector is just a way to compress a vector when it is filled mainly with zeroes. The idea is to only keep track of the length of the vector and the cells with a non-zero value. So in this \nSparseVector\n, we have 3000 entries. Index 0 has value \n4\n, index 1 has value 0, index 2 has value \n2\n, etc. Each index corresponds to the token at the same index in the \nvocabulary\n. \nYou can convert a \nSparseVector\n back into a \"normal\" vector (called a \nDenseVector\n) using:\n\nfrom\n \npyspark.ml.linalg\n \nimport\n \nVectors\n\n\nVectors\n.\nDense\n(\nmySparseVector\n)\n\n\nThe \ntoken_counts\n column contains a vector of length \nlen(vocabulary)\n. The value at index \nx\n corresponds to the frequency of the token at index \nx\n in the \nvocabulary\n:\n\n\nfirst_doc_freqs\n \n=\n \ncounted_df\n.\nselect\n(\ntoken_counts\n)\n.\nfirst\n()[\n0\n]\n\n\nprint\n(\nfrequencies of the first ten words of the vocabulary in the first document:\n)\n\n\nfor\n \ni\n \nin\n \nrange\n(\n10\n):\n\n    \nprint\n(\n%-10s\n: \n%d\n \n%\n \n(\nvocabulary\n[\ni\n],\n \nfirst_doc_freqs\n[\ni\n]))\n\n\n\n\n\nResult:\n\nfrequencies of the first ten words of the vocabulary in the first document:\nyear      : 4\npeople    : 0\ntime      : 2\ngovernment: 0\nyears     : 0\nbest      : 0\ntold      : 0\ngame      : 0\nthree     : 2\nfilm      : 2\n\n\n\nTF-IDF\n\n\nThe \nterm frequency\u2013inverse document frequency\n (TF-IDF) is often used in information retrieval and search engine. The basic idea is:\n\n\n\n\nthe more often a searched term appears in a document, the more relevant the document is relative to the search (\nterm frequency\n);\n\n\nthe more often a term appears in a corpus of document, the less informative/discriminative it is (\ninverse document frequency\n).\n\n\n\n\nSo the TF-IDF is a way to give different weights to search terms and results, in order to get more relevant results.\n\n\nWe already computed the term frequencies using the \nCountVectorizer\n. The only thing left to do is to compute the IDF: \n$$ \nidf_t = \\frac{N}{df_t}\n$$\nwhere \nN\n is the number of documents in the corpus and df\nt\n is the number of documents in which \nt\n appears.\n\n\nIn Spark ML, the \nIDF\n class takes care of everything:\n\n\n\n\nIDF is an Estimator which is fit on a dataset and produces an IDFModel. The IDFModel takes feature vectors (generally created from HashingTF or CountVectorizer) and scales each column. Intuitively, it down-weights columns which appear frequently in a corpus.\n\n\n\n\n%\npyspark\n\n\nfrom\n \npyspark.ml.feature\n \nimport\n \nIDF\n\n\n\nidf\n \n=\n \nIDF\n(\ninputCol\n \n=\n \ntoken_counts\n,\n \noutputCol\n \n=\n \nfeatures\n)\n\n\nidf_model\n \n=\n \nidf\n.\nfit\n(\ncounted_df\n)\n\n\n# apply the model and keep only the relevant columns\n\n\nfeatures_df\n \n=\n \nidf_model\n.\ntransform\n(\ncounted_df\n)\n.\nselect\n(\nid\n,\n \ntitle\n,\n \ncontent\n,\n \nfeatures\n)\n\n\n\n\n\nCreating the model\n\n\nNow, that we have our features, let's train an LDA model.\n\n\nThere are many parameters to fine-tune an \nLDA model\n. Right now, we will just set two of them:\n\n\n\n\nk\n: the number of topics (or clusters) to find;\n\n\nmaxIter\n: the maximum number of iterations to do.\n\n\n\n\nBased on the \nLDA example\n, we create the model with:\n\n\n%\npyspark\n\n\nfrom\n \npyspark.ml.clustering\n \nimport\n \nLDA\n\n\n\n# set parameters\n\n\nk\n \n=\n \n10\n        \n# number of topics\n\n\nmaxIter\n \n=\n \n10\n \n# low boundary: might not be incredible, but it will be quick\n\n\n\n# train a LDA model.\n\n\nlda\n \n=\n \nLDA\n(\nk\n=\nk\n,\n \nmaxIter\n=\nmaxIter\n,\n \nfeaturesCol\n=\nfeatures\n)\n\n\nmodel\n \n=\n \nlda\n.\nfit\n(\nfeatures_df\n)\n\n\n\n# Add the result to the dataframe\n\n\ntransformed\n \n=\n \nmodel\n.\ntransform\n(\nfeatures_df\n)\n\n\n\n\n\nThe LDA model gives us two important things:\n\n\n\n\nmodel.topicsMatrix()\n: the inferred topics, where each topic is represented by a distribution over terms. It is a matrix of size \nlen(vocabulary)\n x \nk\n, where each column is a topic: value at row 1 and column 2 tells the importance of term 1 for topic 2. \n\n\nthe column \ntopicDistribution\n in the \ntransformed\n dataframe: it is a vector of size \nk\n whose values are the importance of the document for each topic;\n\n\n\n\nHaving the relations \nterm\n \n \ntopic\n and \ntopic\n \n \ndocument\n, we can also deduce the relations \nterm\n \n \ndocument\n (transitivity). \n\n\nDescribing the topics\n\n\nOk, now let's look at our topics. The model is not able to tell us exactly what they are, but we can list the most relevant terms of the vocabulary for each topic:\n\n\n%\npyspark\n\n\n# This returns a DataFrame[topic: int, termIndices: array\nint\n, termWeights: array\ndouble\n]\n\n\ntopics\n \n=\n \nmodel\n.\ndescribeTopics\n(\n5\n)\n \n# get the top 5 words\n\n\n\n# Let\ns replace the termIndices by the actual terms\n\n\ntopics_descr\n \n=\n \ntopics\n.\nrdd\n.\nmap\n(\nlambda\n \nr\n:\n \n(\nr\n[\n0\n],\n  \n[\nvocabulary\n[\ni\n]\n \nfor\n \ni\n \nin\n \nr\n[\n1\n]]))\n.\ncollect\n()\n\n\n\n# use a zeppelin trick to display a nice table:\n\n\nprint\n(\nThe topics described by their top-weighted terms:\n)\n\n\nprint\n(\n%table\n\\n\nid\n\\t\nterms\n)\n\n\nfor\n \nr\n \nin\n \ntopics_descr\n:\n\n    \nprint\n(\n%s\n\\t\n%s\n \n%\n \n(\nr\n[\n0\n],\n \n, \n.\njoin\n(\nr\n[\n1\n])))\n\n\n\n\n\nPrinting tables with Zeppelin\nIf you output the \n%table\n \nshebang\n followed by lines (ending with \n\\n\n) with tabs (\n\\t\n), Zeppelin automagically convert it to a table. Example:\n\n%pyspark\n\n\nprint\n(\n\n\n%table\n\n\nheader\n \n1\n    \nheader\n \n2\n\n\ncell\n \n1\n,\n1\n    \ncell\n \n1\n,\n2\n\n\ncell\n \n2\n,\n1\n    \ncell\n \n2\n,\n1\n\n\n)\n\n\nQueryEngine\n\n\nTo simplify, here is \na python script\n defining a \nQueryEngine\n class. \n\n\nCreating a QueryEngine\n\n\nEither copy-paste its content to a new cell or save it to \nzeppelin/data\n and import it to the notebook using the following snippet:\n\n\n%\npyspark\n\n\nexec\n(\nopen\n(\ndata/QueryEngine.py\n)\n.\nread\n())\n\n\n\n\n\nQueryEngine methods\nYou can type \nhelp(QueryEngine)\n in a new cell to get the list of available methods. Here is a summary:\nshowDoc(docId)\n: returns the title and content of a document as a string;\ntopDocsForDoc(docId, top=10)\n: returns the most relevant documents for a given document;\ntopDocsForTerm(termId, top=10)\n: returns the most relevant documents for a given term;\ntopDocsForTermQuery(termIds, top=10)\n: returns the most relevant document for a given list of terms;\ntopTermsForTerm(termId, top=10)\n: returns the most relevant terms for a given term.\nCreate a new \nQueryEngine\n instance:\n\n\n%\npyspark\n\n\nq\n \n=\n  \nQueryEngine\n(\nmodel\n,\n \ntransformed\n,\n \nvocabulary\n,\n \nidf_model\n)\n\n\n\n\n\nUsing the QueryEngine\n\n\nType \nhelp(q)\n to list the \nQueryEngine\n method and try the different search functions.\n\n\n\n\nYou might notice that the \nQueryEngine\n works with term and document \nIDs\n. \n\n\nTo work with term ids, use the \nvocabulary\n:\n\n\n\n\nvocabulary.index(\ncomputer\n)\n: returns the id of the term \ncomputer\n, or \n-1\n if the term is not in the vocabulary;\n\n\nvocabulary[10]\n: returns the \nterm\n at id \n10\n;\n\n\n\n\nFor documents, use the \nlookup tables\n created at the beginning:\n\n\n\n\ndocTitles[0]\n: returns the title of document \n0\n;\n\n\ndocIds[\nAwesome title\n]\n: returns the ID of the document with title \nAwesome title\n;\n\n\n\n\n\n\n\n\nTo display the results nicely, you can use the \nresult2tables(result)\n method, or simply call \ntoTable()\n on the results. \n\n\n\n\nHere is an example:\n\n%\npyspark\n\n\n# looking for documents related to term \ncomputer\n\n\ntermId\n \n=\n \nvocabulary\n.\nindex\n(\ncomputer\n)\n\n\nq\n.\ntopDocsForTerm\n(\ntermId\n)\n.\ntoTable\n()\n \n# or: results2table(q.topDocsForTerm(termId))\n\n\n\n\nInteractive queries\n\n\nSearch in titles\n\n\nFirst, let's have an interactive cell to search in the document titles. \n\n\nThe \nz.input(name, default_value)\n let's you create an interactive form. \n\n\nThe \nfilter\n method on a dataframe accepts an SQL like clause. For example \nid \n 300\n, \ncontent LIKE \n%computer%\n or \nlower(title) = \nweak dollar hits reuters\n.\n\n\nUsing this, we can create an interactive form for searching for strings in the title:\n\n\n%\npyspark\n\n\nterms\n \n=\n \nz\n.\ninput\n(\nsearch in title:\n,\n \ncomputer\n)\n \n# get input from the user\n\n\n\n# do the query \n\n\nresults\n \n=\n \ntransformed\n\\\n    \n.\nselect\n(\nid\n,\n \ntitle\n)\n\\\n    \n.\nfilter\n(\nlower(title) LIKE \n%\n \n+\n \nterms\n \n+\n \n%\n)\n\\\n    \n.\ncollect\n()\n\n\n\n# print the results\n\n\nprint\n(\n%table\n)\n\n\nprint\n(\nid\n\\t\ntitle\n)\n\n\nfor\n \nrow\n \nin\n \nresults\n:\n\n    \nprint\n(\n%d\n\\t\n%s\n \n%\n \n(\nrow\n[\n0\n],\n \nrow\n[\n1\n]))\n\n\n\n\n\nTop docs for term\n\n\nTry to write an interactive query where the user enters a term and the system displays the most relevant documents.\n\n\n\n\nBe careful: you need to ensure the term is part of the vocabulary (and retrieve its ID) before calling the \nQueryEngine\n.\n\n\n\n\nSolution\nHere is one way to do it:\n\n%\npyspark\n\n\nimport\n \nrandom\n\n\n\n# get the input\n\n\nterm\n \n=\n \nz\n.\ninput\n(\nterm:\n)\n\n\n\nif\n \nterm\n \n!=\n \n \nand\n \nterm\n \nin\n \nvocabulary\n:\n\n    \n# the term is in the vocabulary, we can proceed\n\n    \ntermId\n \n=\n \nvocabulary\n.\nindex\n(\nterm\n)\n\n    \nprint\n(\nresults for term: \n%s\n (\n%d\n)\n \n%\n \n(\nterm\n,\n \ntermId\n))\n\n    \nq\n.\ntopDocsForTerm\n(\ntermId\n)\n.\ntoTable\n()\n\n\nelse\n:\n\n    \n# oups: not in the vocabulary. List some random terms instead.\n\n    \nprint\n(\n%s\n is not in the vocabulary. Try: \n%s\n...\n \n%\n \n(\nterm\n,\n \n, \n.\njoin\n(\nrandom\n.\nsample\n(\nvocabulary\n,\n \n10\n))\n \n))\n\n\nTop terms for term\n\n\nUsing nearly the same code as above, we can also list the most relevant terms for a term.\n\n\nSolution\nSimply replace \nq.topDocsForTerm(termId)\n with \nq.topTermsForTerm(termId)\n in the code above.\nTop docs for terms\n\n\nHere, we do quite the same as above, but let the user enter multiple terms (the relevant \nQueryEngine\n method is \ntopDocsForTermQuery\n).\n\n\nSolution\n%\npyspark\n\n\nimport\n \nrandom\n\n\n\n# get the input\n\n\nterms\n \n=\n \nz\n.\ninput\n(\nterms, space-separated:\n)\n\n\n\n# prepare random terms\n\n\nrandom_terms\n \n=\n \n, \n.\njoin\n(\nrandom\n.\nsample\n(\nvocabulary\n,\n \n10\n))\n\n\n\nif\n \nterm\n \n!=\n \n:\n\n    \n# the term is in the vocabulary, we can proceed\n\n    \ntermIds\n \n=\n \n[\nvocabulary\n.\nindex\n(\nt\n)\n \nfor\n \nt\n \nin\n \nterms\n.\nsplit\n(\n \n)\n \nif\n \nt\n \nin\n \nvocabulary\n]\n\n    \nif\n \nlen\n(\ntermIds\n)\n \n \n0\n:\n\n        \nprint\n(\nlooking for documents matching \n%s\n \n%\n \n[\nvocabulary\n[\nid\n]\n \nfor\n \nid\n \nin\n \ntermIds\n])\n\n        \nq\n.\ntopDocsForTermQuery\n(\ntermIds\n)\n.\ntoTable\n()\n\n    \nelse\n:\n\n        \nprint\n(\nsorry, those terms are not in the vocabulary. Try: \n%s\n \n%\n \nrandom_terms\n)\n\n\nelse\n:\n\n    \n# no input. Suggest some terms\n\n    \nprint\n(\nTry: \n%s\n...\n \n%\n \nrandom_terms\n)\n\n\n\n\nTop docs for doc\n\n\nHere is a code for creating a dropdown with document titles:\n\n\n%\npyspark\n\n\n# tip: for better performances, move the next line to its own cell and execute it once only\n\n\ndocSelection\n \n=\n \n[(\nr\n[\n0\n],\n \n%d\n - \n%s\n \n%\n \n(\nr\n[\n0\n],\n \nr\n[\n1\n]))\n \nfor\n \nr\n \nin\n \ntransformed\n.\nselect\n(\nid\n,\n \ntitle\n)\n.\ncollect\n()]\n\n\n# display the dropdown\n\n\ndocId\n \n=\n \nz\n.\nselect\n(\ndocument\n,\n \ndocSelection\n,\n \n1\n)\n\n\n\n\n\nThe \ndocId\n is the ID of the selected document. Add the code necessary to list relevant documents.\n\n\nSolution\n%\npyspark\n\n\ndocSelection\n \n=\n \n[(\nr\n[\n0\n],\n \n%d\n - \n%s\n \n%\n \n(\nr\n[\n0\n],\n \nr\n[\n1\n]))\n \nfor\n \nr\n \nin\n \ntransformed\n.\nselect\n(\nid\n,\n \ntitle\n)\n.\ncollect\n()]\n\n\ndocId\n \n=\n \nz\n.\nselect\n(\ndocument\n,\n \ndocSelection\n,\n \n1\n)\n\n\n\nprint\n(\nTop documents for doc #\n%d\n \n%\n \ndocId\n)\n\n\nq\n.\ntopDocsForDoc\n(\ndocId\n)\n.\ntoTable\n()\n\n\n\n\nWhere to go from here\n\n\nConsidering our application, there are many things we could change/enhance. For example:\n\n\n\n\nwe chose a bit randomly \nk = 10\n. From the information we have on our data, another value for \nk\n could yield better results. The same holds for the size of the vocabulary (set to \n3000\n);\n\n\nconcerning the preprocessing, we could:\n\n\nuse different stopwords or even skip the stopwords removal step;\n\n\nuse \nlemmatization\n or \nstemming\n to group words from the same family/root into \ntoken classes\n;\n\n\nchange the TF-IDF formula to finetune our scoring system;\n\n\n...\n\n\n\n\n\n\n\n\nThere are also other techniques for LSA, like the \nSVD\n algorithm we mentioned in the introduction. Spark ML offers an implementation of \nSVD\n that can yield the same kind of results (let me know if you are interested, I can give you some pointers \n).", 
            "title": "LSA with Spark ML"
        }, 
        {
            "location": "/spark/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/spark/#what-we-will-do", 
            "text": "To create the search engine, we will do the following steps:", 
            "title": "What we will do"
        }, 
        {
            "location": "/spark/#what-we-will-use", 
            "text": "LSA,  latent semantic anlysis  is   a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms  (source: wikipedia)   In summary, we want to establish underlying relationships between documents in a collection by linking documents and terms through  concepts . Those concepts are deduced or constructed through a statistical or mathematical algorithm that forms a  model . Once we have the model, we can begin using it for search queries.", 
            "title": "What we will use"
        }, 
        {
            "location": "/spark/#available-algorithms", 
            "text": "There are two well-known methods for discovering underlying topics:    SVD ,  Singular Value Decomposition : is a mathematical method for decomposing a matrix into a product of smaller matrices. It has the advantage of being  mathematically correct : computing the model, hence  the decomposed matrices, takes only one pass and is deterministic: the same data will always give the same result (as long as the same parameters are used).    LDA ,  Latent Dirichlet Allocation , is a generative probabilistic model. Based on statistics, many iterations are necessary to get a good-enough model and every run could give a different result. The result is also highly dependant on the parameters we use for the training.     In this workshop, we will use the LDA technique.", 
            "title": "Available algorithms"
        }, 
        {
            "location": "/spark/#lda-basics", 
            "text": "LDA is like a clustering algorithm where :   the  cluster centers  are the topics;  the  training examples  are documents;  the  features  are the word frequencies;  the distance is  euclidean  based on a statistical model (Bayesian inference rules / Dirichlet distribution)   As in clustering, we need to give some informations ( parameters ) to the model. The most important one is  the number of topics (k)  we think there is (exactly as we need to specify the number of clusters to find).  During training, the model will first assign each document to a random topic. Then, on each iteration, it computes how well the actual topic distribution describes/predicts each document, makes adjustments and try again. Most of the time, the programmer will set in advance the  maximum number of iterations  to do.  In the end, the model outputs a topic distribution over document (how much a document is important to a given topic) and over terms (how much the term describes the topic). Documents with similar topic distributions are likely to be similar, even if they don't use the same words.", 
            "title": "LDA Basics"
        }, 
        {
            "location": "/spark/#lda-with-pyspark", 
            "text": "", 
            "title": "LDA with pyspark"
        }, 
        {
            "location": "/spark/#getting-the-dataset", 
            "text": "The dataset \" Dataset: BBC \" available at:  http://mlg.ucd.ie/datasets/bbc.html  (we used the raw text files).  It consists of 2225 documents from the BBC news website corresponding to stories in five topical areas (business, entertainment, politics, sport and tech) from 2004-2005.  Since the dataset comes in multiple folders and with some duplicates, we already processed it and made it available in csv or json format. If you are interested, you can get  the python script  we used for the parsing.  For the workshop, download  bbc.json  and save it in the  snorkel/zeppelin/data  folder.", 
            "title": "Getting the dataset"
        }, 
        {
            "location": "/spark/#loading-the-dataset-into-spark", 
            "text": "Create a new notebook. On the first cell, ensure that the  bbc.json  file is were it should be by running:  %sh\nls data/bbc.json  The file should be listed in the output.  Now, let's load those data into a Spark dataframe ( documentation ):  % pyspark  dataset   =   spark . read . json ( data/bbc.json )   Ensure this worked by inspecting the dataset:  % pyspark  print ( number of documents:  %d   %   dataset . count ())  print ( structure:  %s   %   dataset . describe ())  dataset . show ()   number of documents: 2096\nstructure: DataFrame[summary: string, category: string, content: string, filename: string, title: string]\n...", 
            "title": "Loading the dataset into Spark"
        }, 
        {
            "location": "/spark/#adding-ids-and-creating-lookup-tables", 
            "text": "In order to keep track of our articles, let's give each line of our \"table\" a unique ID:  % pyspark  from   pyspark.sql.functions   import   monotonically_increasing_id  from   pyspark.sql.types   import   LongType  # This will return a new DF with all the columns + id  dataset   =   dataset . withColumn ( id ,   monotonically_increasing_id ())  dataset . show ( 5 )   Then, it would be nice to be quickly able to get an ID given a title and vice-versa. So let's create lookup dictionaries:  % pyspark  docTitles   =    dict ([( r [ 0 ],   r [ 1 ])   for   r   in   dataset . select ( id ,   title ) . collect ()])   docIds   =   dict ([( r [ 1 ],   r [ 0 ])   for   r   in   dataset . select ( id ,   title ) . collect ()])   In those lines, we select two columns from our dataframe:  id  and  title . Using  collect , we ask Spark to actually execute the selection (it is an  action ) and get the results into a list of  Row . A  Row  behaves like an untyped array. We convert our rows into tuples and finally cast the list of tuples into a dictionary for quick lookup.  To ensure it works, try to lookup the title of the document with ID 0:  docTitles[0] .", 
            "title": "Adding IDs and creating lookup tables"
        }, 
        {
            "location": "/spark/#features-extraction", 
            "text": "In our case, the feature extraction is applied on the articles body (column  content ) and contains the following steps:   breaking content into words ( tokenisation )  removing common words ( stopwords removal )  computing the frequencies of each word ( count vectors ) and selecting a subset of words ( vocabulary )  computing the TF-IDF over documents and vocabulary   As you will see, all those steps are very common and can be performed using Spark ML utilities.", 
            "title": "Features extraction"
        }, 
        {
            "location": "/spark/#tokenisation", 
            "text": "The first thing to do is to break our article's content into tokens.   Have a look at the Spark ML documentation and try to figure out how to tokenise our  content  column:  Spark ML documentation: tokenizer .  Tips : discard words of less than 4 characters, save the result into a  tokens  column.  Solution We use the  RegexTokenizer  with the following arguments: regex : break by white space character(s)  inputCol : name of the input column, here  content outputCol : name of the new column with tokens, here  tokens minTokenLength : discard tokens with length   4 % pyspark  # import the RegexTokenizer class  from   pyspark.ml.feature   import   RegexTokenizer  # create  a new tokenizer instance  tokenizer   =   RegexTokenizer ( pattern = [ \\\\ W_]+ ,   inputCol = content ,  \n     outputCol = tokens ,   minTokenLength = 4 )  # tokenise (and select only the interesting columns to gain space)  tokenized_df   =   tokenizer . transform ( dataset ) . select ( id ,   title ,   content ,   tokens )   To visualize the results, lets print the first 100 characters of the first article and some of the resulting tokens: % pyspark  content   =   tokenized_df . select ( content ) . first ()  tokens   =   tokenized_df . select ( tokens ) . first ()  print ( content:  %s ...   %   content [ 0 ][: 100 ])  print ( tokens:   %s ..   %   ,  . join ( tokens [ 0 ][: 10 ]))   Result: content :   Quarterly   profits   at   US   media   giant   TimeWarner   jumped   76 %   to   $1 . 13 bn   ( \u00a3 600 m )   for   the   three   months   to ...  tokens :    quarterly ,   profits ,   media ,   giant ,   timewarner ,   jumped ,   13 bn ,   600 m ,   three ,   months ..   The difficulties of tokenisation Tokenisation is very important in language processing, but it is not an easy task. Here, we separated words based on spaces and removed small words. But is it always a good idea ? It depends... For example: quarterly profit     quarterly ,  profit  seems good. But what about  New York  ? Breaking it down into two tokens might change the way our search engine treats a query about  New York ... Indeed, if it is considered two tokens, a document with a text \"there is a  new  professor at the  York  university\" becomes relevant as well; By removing small words, we might loose informations, such as  US  or  USA ; Here, our tokeniser treats  13bn  or  600m  as tokens. But we can wonder if words with non-letter characters should be kept or not. In real-life applications, the tokenizer is often way more complex and augmented with context-sensitive rules and heuristics.", 
            "title": "Tokenisation"
        }, 
        {
            "location": "/spark/#stopwords-removal", 
            "text": "We want to remove common words that are likely to appear in every document, hence not very informative. Here, we will use the following:  % pyspark  # stopwords to use: copy-paste it into a cell  stopwords   =   bove,abst,accordance,according,accordingly,across,act,actually,added,adj,affected,affecting,affects,after,afterwards,again,against,ah,all,almost,alone,along,already,also,although,always,am,among,amongst,an,and,announce,another,any,anybody,anyhow,anymore,anyone,anything,anyway,anyways,anywhere,apparently,approximately,are,aren,arent,arise,around,as,aside,ask,asking,at,auth,available,away,awfully,b,back,be,became,because,become,becomes,becoming,been,before,beforehand,begin,beginning,beginnings,begins,behind,being,believe,below,beside,besides,between,beyond,biol,both,brief,briefly,but,by,c,ca,came,can,cannot,can t,cause,causes,certain,certainly,co,com,come,comes,contain,containing,contains,could,couldnt,d,date,did,didn t,different,do,does,doesn t,doing,done,don t,down,downwards,due,during,e,each,ed,edu,effect,eg,eight,eighty,either,else,elsewhere,end,ending,enough,especially,et,et-al,etc,even,ever,every,everybody,everyone,everything,everywhere,ex,except,f,far,few,ff,fifth,first,five,fix,followed,following,follows,for,former,formerly,forth,found,four,from,further,furthermore,g,gave,get,gets,getting,give,given,gives,giving,go,goes,gone,got,gotten,h,had,happens,hardly,has,hasn t,have,haven t,having,he,hed,hence,her,here,hereafter,hereby,herein,heres,hereupon,hers,herself,hes,hi,hid,him,himself,his,hither,home,how,howbeit,however,hundred,i,id,ie,if,i ll,im,immediate,immediately,importance,important,in,inc,indeed,index,information,instead,into,invention,inward,is,isn t,it,itd,it ll,its,itself,i ve,j,just,k,keep,keeps,kept,kg,km,know,known,knows,l,largely,last,lately,later,latter,latterly,least,less,lest,let,lets,like,liked,likely,line,little, ll,look,looking,looks,ltd,m,made,mainly,make,makes,many,may,maybe,me,mean,means,meantime,meanwhile,merely,mg,might,million,miss,ml,more,moreover,most,mostly,mr,mrs,much,mug,must,my,myself,n,na,name,namely,nay,nd,near,nearly,necessarily,necessary,need,needs,neither,never,nevertheless,new,next,nine,ninety,no,nobody,non,none,nonetheless,noone,nor,normally,nos,not,noted,nothing,now,nowhere,o,obtain,obtained,obviously,of,off,often,oh,ok,okay,old,omitted,on,once,one,ones,only,onto,or,ord,other,others,otherwise,ought,our,ours,ourselves,out,outside,over,overall,owing,own,p,page,pages,part,particular,particularly,past,per,perhaps,placed,please,plus,poorly,possible,possibly,potentially,pp,predominantly,present,previously,primarily,probably,promptly,proud,provides,put,q,que,quickly,quite,qv,r,ran,rather,rd,re,readily,really,recent,recently,ref,refs,regarding,regardless,regards,related,relatively,research,respectively,resulted,resulting,results,right,run,s,said,same,saw,say,saying,says,sec,section,see,seeing,seem,seemed,seeming,seems,seen,self,selves,sent,seven,several,shall,she,shed,she ll,shes,should,shouldn t,show,showed,shown,showns,shows,significant,significantly,similar,similarly,since,six,slightly,so,some,somebody,somehow,someone,somethan,something,sometime,sometimes,somewhat,somewhere,soon,sorry,specifically,specified,specify,specifying,still,stop,strongly,sub,substantially,successfully,such,sufficiently,suggest,sup,sure,t,take,taken,taking,tell,tends,th,than,thank,thanks,thanx,that,that ll,thats,that ve,the,their,theirs,them,themselves,then,thence,there,thereafter,thereby,thered,therefore,therein,there ll,thereof,therere,theres,thereto,thereupon,there ve,these,they,theyd,they ll,theyre,they ve,think,this,those,thou,though,thoughh,thousand,throug,through,throughout,thru,thus,til,tip,to,together,too,took,toward,towards,tried,tries,truly,try,trying,ts,twice,two,u,un,under,unfortunately,unless,unlike,unlikely,until,unto,up,upon,ups,us,use,used,useful,usefully,usefulness,uses,using,usually,v,value,various, ve,very,via,viz,vol,vols,vs,w,want,wants,was,wasnt,way,we,wed,welcome,we ll,went,were,werent,we ve,what,whatever,what ll,whats,when,whence,whenever,where,whereafter,whereas,whereby,wherein,wheres,whereupon,wherever,whether,which,while,whim,whither,who,whod,whoever,whole,who ll,whom,whomever,whos,whose,why,widely,willing,wish,with,within,without,wont,words,world,would,wouldnt,www,x,y,yes,yet,you,youd,you ll,your,youre,yours,yourself,yourselves,you ve,z,zero,article,about,writes,entry,well,will,newsgroup . split ( , )   Look at the  Spark ML documentation  to remove all the stopwords from our  tokens  column using a  StopWordsRemover .  Solution The code is very similar to the tokenisation one: % pyspark  from   pyspark.ml.feature   import   StopWordsRemover  remover   =   StopWordsRemover ( inputCol = tokens ,   outputCol = filtered ,   stopWords = stopwords )  filtered_df   =   remover . transform ( tokenized_df )   To visualise the results, let's print the first 18 tokens of a document, with and without stopwords:  % pyspark  tokens   =   filtered_df . select ( tokens ) . first ()  filtered_tokens   =   filtered_df . select ( filtered ) . first ()  print (   tokens:  %s   %   ,  . join ( tokens [ 0 ][: 18 ]))  print ( filtered:  %s   %   ,  . join ( filtered_tokens [ 0 ][: 18 ]))   Result:    tokens: quarterly, profits, media, giant, timewarner, jumped, 13bn, 600m, three, months, december, from, 639m, year, earlier, firm, which, biggest\nfiltered: quarterly, profits, media, giant, timewarner, jumped, 13bn, 600m, three, months, december, 639m, year, earlier, firm, biggest, investors, google \nHere,  from  and  which  have been removed.", 
            "title": "Stopwords removal"
        }, 
        {
            "location": "/spark/#word-frequencies-and-vocabulary", 
            "text": "Using the  Spark ML CountVectorizer , try to find how to create a model with a vocabulary of 3000 words and a minimum document frequency of 5. Then, use the model over our  filtered_df  dataset to create a new column named  token_counts .  Solution % pyspark  from   pyspark.ml.feature   import   CountVectorizer  vocabSize   =   3000   # the maximum number of term to retain  minDF   =   5          # the minimum number of different documents a  \n                  # term must appear in to be included in the vocabulary.  vectorizer   =   CountVectorizer ( inputCol   =   filtered ,   outputCol   =   token_counts ,   vocabSize = vocabSize ,   minDF = minDF )  count_model   =   vectorizer . fit ( filtered_df )         # create model  counted_df   =   count_model . transform ( filtered_df )   # apply model to our df  vocabulary   =   count_model . vocabulary               # extract the vocabulary   If you look at our  token_counts  column, here is what we get:  % pyspark  first_doc_freqs   =   counted_df . select ( token_counts ) . first ()  first_doc_freqs [ 0 ]   SparseVector(3000, {0: 4.0, 2: 2.0, 8: 2.0, 9: 2.0, 11: 1.0, 13: 1.0, 14: 2.0, 25: 1.0, 32: 2.0, 40: 3.0, 43: 1.0, 44: 1.0, 47: 1.0, 55: 1.0, 56: 1.0, 57: 2.0, 65: 2.0, 67: 1.0, 68: 1.0, 69: 2.0, 71: 1.0, 85: 1.0, 89: 1.0, 102: 1.0, 109: 1.0, 115: 1.0, 118: 1.0, 119: 1.0, 123: 1.0, 132: 1.0, 137: 2.0, 140: 4.0, 149: 1.0, 150: 1.0, 156: 1.0, 159: 1.0, 160: 1.0, 161: 1.0, 170: 2.0, 179: 3.0, 189: 1.0, 190: 1.0, 202: 1.0, 205: 1.0, 209: 2.0, 227: 1.0, 248: 1.0, 249: 1.0, 261: 1.0, 283: 1.0, 300: 2.0, 327: 1.0, 331: 2.0, 333: 1.0, 340: 3.0, 342: 1.0, 360: 5.0, 364: 2.0, 410: 1.0, 440: 1.0, 464: 1.0, 467: 1.0, 489: 1.0, 498: 1.0, 504: 1.0, 544: 2.0, 549: 1.0, 560: 1.0, 562: 1.0, 589: 2.0, 632: 1.0, 687: 4.0, 742: 2.0, 779: 1.0, 780: 1.0, 797: 1.0, 806: 3.0, 828: 1.0, 890: 1.0, 931: 1.0, 938: 1.0, 951: 1.0, 960: 1.0, 971: 1.0, 1017: 2.0, 1066: 1.0, 1129: 1.0, 1177: 2.0, 1290: 1.0, 1311: 1.0, 1360: 1.0, 1437: 1.0, 1451: 1.0, 1462: 1.0, 1597: 2.0, 1639: 1.0, 1674: 1.0, 1719: 1.0, 1732: 1.0, 1733: 1.0, 1906: 1.0, 1987: 1.0, 2034: 1.0, 2090: 1.0, 2120: 1.0, 2188: 1.0, 2199: 1.0, 2265: 1.0, 2305: 2.0, 2306: 1.0, 2392: 1.0, 2443: 1.0, 2467: 1.0, 2670: 1.0, 2742: 1.0})  About SparseVector A sparse vector is just a way to compress a vector when it is filled mainly with zeroes. The idea is to only keep track of the length of the vector and the cells with a non-zero value. So in this  SparseVector , we have 3000 entries. Index 0 has value  4 , index 1 has value 0, index 2 has value  2 , etc. Each index corresponds to the token at the same index in the  vocabulary .  You can convert a  SparseVector  back into a \"normal\" vector (called a  DenseVector ) using: from   pyspark.ml.linalg   import   Vectors  Vectors . Dense ( mySparseVector )  The  token_counts  column contains a vector of length  len(vocabulary) . The value at index  x  corresponds to the frequency of the token at index  x  in the  vocabulary :  first_doc_freqs   =   counted_df . select ( token_counts ) . first ()[ 0 ]  print ( frequencies of the first ten words of the vocabulary in the first document: )  for   i   in   range ( 10 ): \n     print ( %-10s :  %d   %   ( vocabulary [ i ],   first_doc_freqs [ i ]))   Result: frequencies of the first ten words of the vocabulary in the first document:\nyear      : 4\npeople    : 0\ntime      : 2\ngovernment: 0\nyears     : 0\nbest      : 0\ntold      : 0\ngame      : 0\nthree     : 2\nfilm      : 2", 
            "title": "Word frequencies and vocabulary"
        }, 
        {
            "location": "/spark/#tf-idf", 
            "text": "The  term frequency\u2013inverse document frequency  (TF-IDF) is often used in information retrieval and search engine. The basic idea is:   the more often a searched term appears in a document, the more relevant the document is relative to the search ( term frequency );  the more often a term appears in a corpus of document, the less informative/discriminative it is ( inverse document frequency ).   So the TF-IDF is a way to give different weights to search terms and results, in order to get more relevant results.  We already computed the term frequencies using the  CountVectorizer . The only thing left to do is to compute the IDF: \n$$ \nidf_t = \\frac{N}{df_t}\n$$\nwhere  N  is the number of documents in the corpus and df t  is the number of documents in which  t  appears.  In Spark ML, the  IDF  class takes care of everything:   IDF is an Estimator which is fit on a dataset and produces an IDFModel. The IDFModel takes feature vectors (generally created from HashingTF or CountVectorizer) and scales each column. Intuitively, it down-weights columns which appear frequently in a corpus.   % pyspark  from   pyspark.ml.feature   import   IDF  idf   =   IDF ( inputCol   =   token_counts ,   outputCol   =   features )  idf_model   =   idf . fit ( counted_df )  # apply the model and keep only the relevant columns  features_df   =   idf_model . transform ( counted_df ) . select ( id ,   title ,   content ,   features )", 
            "title": "TF-IDF"
        }, 
        {
            "location": "/spark/#creating-the-model", 
            "text": "Now, that we have our features, let's train an LDA model.  There are many parameters to fine-tune an  LDA model . Right now, we will just set two of them:   k : the number of topics (or clusters) to find;  maxIter : the maximum number of iterations to do.   Based on the  LDA example , we create the model with:  % pyspark  from   pyspark.ml.clustering   import   LDA  # set parameters  k   =   10          # number of topics  maxIter   =   10   # low boundary: might not be incredible, but it will be quick  # train a LDA model.  lda   =   LDA ( k = k ,   maxIter = maxIter ,   featuresCol = features )  model   =   lda . fit ( features_df )  # Add the result to the dataframe  transformed   =   model . transform ( features_df )   The LDA model gives us two important things:   model.topicsMatrix() : the inferred topics, where each topic is represented by a distribution over terms. It is a matrix of size  len(vocabulary)  x  k , where each column is a topic: value at row 1 and column 2 tells the importance of term 1 for topic 2.   the column  topicDistribution  in the  transformed  dataframe: it is a vector of size  k  whose values are the importance of the document for each topic;   Having the relations  term     topic  and  topic     document , we can also deduce the relations  term     document  (transitivity).", 
            "title": "Creating the model"
        }, 
        {
            "location": "/spark/#describing-the-topics", 
            "text": "Ok, now let's look at our topics. The model is not able to tell us exactly what they are, but we can list the most relevant terms of the vocabulary for each topic:  % pyspark  # This returns a DataFrame[topic: int, termIndices: array int , termWeights: array double ]  topics   =   model . describeTopics ( 5 )   # get the top 5 words  # Let s replace the termIndices by the actual terms  topics_descr   =   topics . rdd . map ( lambda   r :   ( r [ 0 ],    [ vocabulary [ i ]   for   i   in   r [ 1 ]])) . collect ()  # use a zeppelin trick to display a nice table:  print ( The topics described by their top-weighted terms: )  print ( %table \\n id \\t terms )  for   r   in   topics_descr : \n     print ( %s \\t %s   %   ( r [ 0 ],   ,  . join ( r [ 1 ])))   Printing tables with Zeppelin If you output the  %table   shebang  followed by lines (ending with  \\n ) with tabs ( \\t ), Zeppelin automagically convert it to a table. Example: %pyspark  print (  %table  header   1      header   2  cell   1 , 1      cell   1 , 2  cell   2 , 1      cell   2 , 1  )", 
            "title": "Describing the topics"
        }, 
        {
            "location": "/spark/#queryengine", 
            "text": "To simplify, here is  a python script  defining a  QueryEngine  class.", 
            "title": "QueryEngine"
        }, 
        {
            "location": "/spark/#creating-a-queryengine", 
            "text": "Either copy-paste its content to a new cell or save it to  zeppelin/data  and import it to the notebook using the following snippet:  % pyspark  exec ( open ( data/QueryEngine.py ) . read ())   QueryEngine methods You can type  help(QueryEngine)  in a new cell to get the list of available methods. Here is a summary: showDoc(docId) : returns the title and content of a document as a string; topDocsForDoc(docId, top=10) : returns the most relevant documents for a given document; topDocsForTerm(termId, top=10) : returns the most relevant documents for a given term; topDocsForTermQuery(termIds, top=10) : returns the most relevant document for a given list of terms; topTermsForTerm(termId, top=10) : returns the most relevant terms for a given term. Create a new  QueryEngine  instance:  % pyspark  q   =    QueryEngine ( model ,   transformed ,   vocabulary ,   idf_model )", 
            "title": "Creating a QueryEngine"
        }, 
        {
            "location": "/spark/#using-the-queryengine", 
            "text": "Type  help(q)  to list the  QueryEngine  method and try the different search functions.   You might notice that the  QueryEngine  works with term and document  IDs .   To work with term ids, use the  vocabulary :   vocabulary.index( computer ) : returns the id of the term  computer , or  -1  if the term is not in the vocabulary;  vocabulary[10] : returns the  term  at id  10 ;   For documents, use the  lookup tables  created at the beginning:   docTitles[0] : returns the title of document  0 ;  docIds[ Awesome title ] : returns the ID of the document with title  Awesome title ;     To display the results nicely, you can use the  result2tables(result)  method, or simply call  toTable()  on the results.    Here is an example: % pyspark  # looking for documents related to term  computer  termId   =   vocabulary . index ( computer )  q . topDocsForTerm ( termId ) . toTable ()   # or: results2table(q.topDocsForTerm(termId))", 
            "title": "Using the QueryEngine"
        }, 
        {
            "location": "/spark/#interactive-queries", 
            "text": "", 
            "title": "Interactive queries"
        }, 
        {
            "location": "/spark/#search-in-titles", 
            "text": "First, let's have an interactive cell to search in the document titles.   The  z.input(name, default_value)  let's you create an interactive form.   The  filter  method on a dataframe accepts an SQL like clause. For example  id   300 ,  content LIKE  %computer%  or  lower(title) =  weak dollar hits reuters .  Using this, we can create an interactive form for searching for strings in the title:  % pyspark  terms   =   z . input ( search in title: ,   computer )   # get input from the user  # do the query   results   =   transformed \\\n     . select ( id ,   title ) \\\n     . filter ( lower(title) LIKE  %   +   terms   +   % ) \\\n     . collect ()  # print the results  print ( %table )  print ( id \\t title )  for   row   in   results : \n     print ( %d \\t %s   %   ( row [ 0 ],   row [ 1 ]))", 
            "title": "Search in titles"
        }, 
        {
            "location": "/spark/#top-docs-for-term", 
            "text": "Try to write an interactive query where the user enters a term and the system displays the most relevant documents.   Be careful: you need to ensure the term is part of the vocabulary (and retrieve its ID) before calling the  QueryEngine .   Solution Here is one way to do it: % pyspark  import   random  # get the input  term   =   z . input ( term: )  if   term   !=     and   term   in   vocabulary : \n     # the term is in the vocabulary, we can proceed \n     termId   =   vocabulary . index ( term ) \n     print ( results for term:  %s  ( %d )   %   ( term ,   termId )) \n     q . topDocsForTerm ( termId ) . toTable ()  else : \n     # oups: not in the vocabulary. List some random terms instead. \n     print ( %s  is not in the vocabulary. Try:  %s ...   %   ( term ,   ,  . join ( random . sample ( vocabulary ,   10 ))   ))", 
            "title": "Top docs for term"
        }, 
        {
            "location": "/spark/#top-terms-for-term", 
            "text": "Using nearly the same code as above, we can also list the most relevant terms for a term.  Solution Simply replace  q.topDocsForTerm(termId)  with  q.topTermsForTerm(termId)  in the code above.", 
            "title": "Top terms for term"
        }, 
        {
            "location": "/spark/#top-docs-for-terms", 
            "text": "Here, we do quite the same as above, but let the user enter multiple terms (the relevant  QueryEngine  method is  topDocsForTermQuery ).  Solution % pyspark  import   random  # get the input  terms   =   z . input ( terms, space-separated: )  # prepare random terms  random_terms   =   ,  . join ( random . sample ( vocabulary ,   10 ))  if   term   !=   : \n     # the term is in the vocabulary, we can proceed \n     termIds   =   [ vocabulary . index ( t )   for   t   in   terms . split (   )   if   t   in   vocabulary ] \n     if   len ( termIds )     0 : \n         print ( looking for documents matching  %s   %   [ vocabulary [ id ]   for   id   in   termIds ]) \n         q . topDocsForTermQuery ( termIds ) . toTable () \n     else : \n         print ( sorry, those terms are not in the vocabulary. Try:  %s   %   random_terms )  else : \n     # no input. Suggest some terms \n     print ( Try:  %s ...   %   random_terms )", 
            "title": "Top docs for terms"
        }, 
        {
            "location": "/spark/#top-docs-for-doc", 
            "text": "Here is a code for creating a dropdown with document titles:  % pyspark  # tip: for better performances, move the next line to its own cell and execute it once only  docSelection   =   [( r [ 0 ],   %d  -  %s   %   ( r [ 0 ],   r [ 1 ]))   for   r   in   transformed . select ( id ,   title ) . collect ()]  # display the dropdown  docId   =   z . select ( document ,   docSelection ,   1 )   The  docId  is the ID of the selected document. Add the code necessary to list relevant documents.  Solution % pyspark  docSelection   =   [( r [ 0 ],   %d  -  %s   %   ( r [ 0 ],   r [ 1 ]))   for   r   in   transformed . select ( id ,   title ) . collect ()]  docId   =   z . select ( document ,   docSelection ,   1 )  print ( Top documents for doc # %d   %   docId )  q . topDocsForDoc ( docId ) . toTable ()", 
            "title": "Top docs for doc"
        }, 
        {
            "location": "/spark/#where-to-go-from-here", 
            "text": "Considering our application, there are many things we could change/enhance. For example:   we chose a bit randomly  k = 10 . From the information we have on our data, another value for  k  could yield better results. The same holds for the size of the vocabulary (set to  3000 );  concerning the preprocessing, we could:  use different stopwords or even skip the stopwords removal step;  use  lemmatization  or  stemming  to group words from the same family/root into  token classes ;  change the TF-IDF formula to finetune our scoring system;  ...     There are also other techniques for LSA, like the  SVD  algorithm we mentioned in the introduction. Spark ML offers an implementation of  SVD  that can yield the same kind of results (let me know if you are interested, I can give you some pointers  ).", 
            "title": "Where to go from here"
        }
    ]
}