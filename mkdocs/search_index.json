{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the datasciences workshop !",
            "title": "Home"
        },
        {
            "location": "/hdfs/",
            "text": "In order to use HADOOP, it is crucial that you understand the basic functioning of HDFS, as well as some of its constraints.\nAfter a brief introduction of core HDFS concepts, this page presents \ncopy-paste\n-like tutorial to familiarize with\n\nHDFS commands\n.\nIt mainly focuses on user commands (uploading and downloading data into HDFS).\n\n\nResources\n\n\nWhile the source of truth for HDFS commands is the code source, the \ndocumentation page describing the \nhdfs dfs\n commands\n is really useful.\n\n\nA good and simpler cheat sheet is also available \nhere\n.\n\n\nIntroduction\n\n\nHDFS (\nHadoop Distributed File System\n) is one of the core components of HADOOP.\n\n\nThe HDFS is a distributed file system designed to run on commodity hardware. Very powerful,\nit should ensure that data are replicated across a wide variety of nodes, making the system\nfault tolerant and suitable for large data sets and gives high throughput.\n\n\n\n\nTip\n\n\nTo have a better understanding of how HDFS works, we strongly encourage you to check out \nthe HDFS Architecture Guide\n.\n\n\n\n\nSome remarks on HDFS\n\n\nHDFS uses a \nsimple coherency model\n: applications mostly need a \nwrite-once-read-many\n access model for files. As a result, a file once created, written to and closed becomes read-only. It is possible to append to an HDFS file only if the system was explicitly configured to.\n\n\nHDFS is tuned to deal with \nlarge files\n. A typical file in HDFS is gigabytes to terabytes in size. As a result, try to avoid scattering your data in numerous small files.\n\n\nHDFS is designed more for \nbatch processing\n rather than interactive use (high throughput versus low latency), and provides only sequential access of data. If your application has other needs, check out tools like \nHBase\n, \nHive\n, \nApache Spark\n, etc.\n\n\n\n\n\u201cMoving Computation is Cheaper than Moving Data\u201d\n\n\n\n\nHDFS architecture\n\n\nAs the \nthe HDFS Architecture Guide\n explains, HDFS has a \nmaster/slave architecture\n.\n\n\nAn HDFS cluster consists of a single \nNameNode\n, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of \nDataNodes\n, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system\u2019s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.\n\n\n\n\n\n\nBasic Manipulations\n\n\n\n\nInfo\n\n\nIn HDFS, user's folders are stored in \n/user\n and not \n/home\n like traditional Unix/Linux filesystems.\n\n\n\n\nListing your home folder\n\n\n $ hdfs dfs -ls\n Found \n28\n items\n ...\n -rw-r--r--   \n3\n llinder  daplab_users   \n6398990\n \n2015\n-03-13 \n11\n:01 data.csv\n ...\n ^^^^^^^^^^   ^ ^^^^^^^^ ^^^^^^^^^^^^   ^^^^^^^ ^^^^^^^^^^ ^^^^^ ^^^^^^^^\n          \n1\n   \n2\n        \n3\n            \n4\n         \n5\n          \n6\n     \n7\n        \n8\n\n\n\n\n\nColumns, as numbered below, represent:\n\n\n\n\nPermissions, in \ntraditional unix permission\n syntax;\n\n\nReplication factor, \nRF\n in short. The RF default to 3 for a file and 0 for a directory; \n\n\nOwner (you!);\n\n\nGroup owning the file;\n\n\nSize of the file, in bytes. Note that to compute the physical space used, this number should be multiplied by the RF;\n\n\nModification date. As HDFS being mostly a ''\nwrite-once-read-many\n'' filesystem,\nthis date often means creation date;\n\n\nModification time. Same as date;\n\n\nFilename, within the listed folder.\n\n\n\n\nListing the \n/tmp\n folder\n\n\nThe only change is that you specify an absolute path from the root of the filesystem (\n/\n):\n\nhdfs dfs -ls /tmp\n\n\n\nUploading a resource\n\n\nTo put a file to HDFS, you have two choices. You can use \nhdfs\n with the \n-put\n option or with the \n-copyFromLocal\n option:\n\n\n# uploading a file\n\nhdfs dfs -put localfile.txt /tmp\nhdfs dfs -copyFromLocal localfile.txt /tmp/\n\n# uploading a directory \n\nhdfs dfs -put localdir /tmp\nhdfs dfs -copyFromLocal localdir /tmp/\n\n\n\n\nThe first arguments after \n-copyFromLocal\n or \n-put\n point to local files or folders, while the last argument is a file (if only one file listed as source) or directory in HDFS. Note that you can use wildcards and also rename files and folders when copying, exactly as you would do in a linux shell:\n\n\n# uploading all files in the current directory with the .txt extension\n\nhdfs dfs -put *.txt /tmp\nhdfs dfs -copyFromLocal *.txt /tmp/\n\n# uploading a directory and renaming it hdfsdir\n\nhdfs dfs -put localdir /tmp/hdfsdir\nhdfs dfs -copyFromLocal localdir /tmp/hdfsdir\n\n\n\n\nBoth options are doing about the same thing, but \n-copyFromLocal\n is more explicit when you're uploading a local file and thus preferred.\n\n\nDownloading a resource\n\n\nDownload is the same as uploading, but \n-put\n becomes \n-get\n and \n-copyFromLocal\n becomes \n-copyToLocal\n:\n\n\nhdfs dfs -get /tmp/remotefile.txt .\nhdfs dfs -copyToLocal /tmp/remotefile.txt .\n\n\n\n\nCreating a folder\n\n\nTo create a folder, use \n-mkdir\n\n\n# create a folder in your hdfs home\n\nhdfs dfs -mkdir dummy-folder\n\n# create a folder in /tmp\n\nhdfs dfs -mkdir /tmp/dummy-folder\n\n\n\n\nNote that relative paths points to your home folder,  \n/user/llinder\n for instance.\n\n\nRemoving resources\n\n\nTo remove individual files, use the \n-rm\n option:\n\n\nhdfs dfs -rm /tmp/somefile.txt\n\n\n\n\nTo remove a folder, the option is \n-rmdir\n for an empty directory and \n-rm -r\n for a non-empty one. The \n-r\n in \n-rm -r\n means \nrecursive\n: it removes the folder and all its children recursively:\n\n\n# remove the dummy-folder in your home\n\nhdfs dfs -rmdir dummy-folder\nrmdir: \n'/tmp/lala'\n: Directory is not empty\n\n# oups, the directory is not empty... use -rm -r\n\nhdfs dfs -rm -r dummy-folder\n\n\n\n\nAdvanced Manipulations\n\n\nthe \nhdfs dfs\n command support several actions that any linux user is already familiar with. Most of their parameters are the same, but note that the collapsing of options (\n-rf\n instead of \n-r -f\n for example) are not supported. Here is a non-exhaustive list:\n\n\n\n\n-rm [-r] [-f]\n: remove a file or directory;\n\n\n-cp [-r]\n: copy a file or directory;\n\n\n-mv\n: move/rename a file or directory;\n\n\n-cat\n: display the content of a file;\n\n\n-chmod\n: manipulate file permissions;\n\n\n-chown\n: manipulate file ownership;\n\n\n-tail|-touch|\netc.\n\n\n\n\nOther useful commands include:\n\n\n\n\n-moveFromLocal|-moveToLocal\n: same as \n-copyFromLocal|-copyToLocal\n, but remove the source;\n\n\n-stat\n: display information about the specified path;\n\n\n-count\n: counts the number of directories, files, and bytes under the paths;\n\n\n-du\n: display the size of the specified file, or the sizes of files and directories that are contained in the specified directory;\n\n\n-dus\n: display a summary of the file sizes;\n\n\n-getmerge\n: concatenate the files in src and writes the result to the specified local destination file. To add a newline character at the end of each file, specify the \naddnl\n option: \nhdfs dfs -getmerge <src> <localdst> [addnl]\n\n\n-setrep [-R]\n: change the replication factor for a specified file or directory;",
            "title": "HDFS"
        },
        {
            "location": "/hdfs/#resources",
            "text": "While the source of truth for HDFS commands is the code source, the  documentation page describing the  hdfs dfs  commands  is really useful.  A good and simpler cheat sheet is also available  here .",
            "title": "Resources"
        },
        {
            "location": "/hdfs/#introduction",
            "text": "HDFS ( Hadoop Distributed File System ) is one of the core components of HADOOP.  The HDFS is a distributed file system designed to run on commodity hardware. Very powerful,\nit should ensure that data are replicated across a wide variety of nodes, making the system\nfault tolerant and suitable for large data sets and gives high throughput.   Tip  To have a better understanding of how HDFS works, we strongly encourage you to check out  the HDFS Architecture Guide .",
            "title": "Introduction"
        },
        {
            "location": "/hdfs/#some-remarks-on-hdfs",
            "text": "HDFS uses a  simple coherency model : applications mostly need a  write-once-read-many  access model for files. As a result, a file once created, written to and closed becomes read-only. It is possible to append to an HDFS file only if the system was explicitly configured to.  HDFS is tuned to deal with  large files . A typical file in HDFS is gigabytes to terabytes in size. As a result, try to avoid scattering your data in numerous small files.  HDFS is designed more for  batch processing  rather than interactive use (high throughput versus low latency), and provides only sequential access of data. If your application has other needs, check out tools like  HBase ,  Hive ,  Apache Spark , etc.   \u201cMoving Computation is Cheaper than Moving Data\u201d",
            "title": "Some remarks on HDFS"
        },
        {
            "location": "/hdfs/#hdfs-architecture",
            "text": "As the  the HDFS Architecture Guide  explains, HDFS has a  master/slave architecture .  An HDFS cluster consists of a single  NameNode , a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of  DataNodes , usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system\u2019s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.",
            "title": "HDFS architecture"
        },
        {
            "location": "/hdfs/#basic-manipulations",
            "text": "Info  In HDFS, user's folders are stored in  /user  and not  /home  like traditional Unix/Linux filesystems.",
            "title": "Basic Manipulations"
        },
        {
            "location": "/hdfs/#listing-your-home-folder",
            "text": "$ hdfs dfs -ls\n Found  28  items\n ...\n -rw-r--r--    3  llinder  daplab_users    6398990   2015 -03-13  11 :01 data.csv\n ...\n ^^^^^^^^^^   ^ ^^^^^^^^ ^^^^^^^^^^^^   ^^^^^^^ ^^^^^^^^^^ ^^^^^ ^^^^^^^^\n           1     2          3              4           5            6       7          8   Columns, as numbered below, represent:   Permissions, in  traditional unix permission  syntax;  Replication factor,  RF  in short. The RF default to 3 for a file and 0 for a directory;   Owner (you!);  Group owning the file;  Size of the file, in bytes. Note that to compute the physical space used, this number should be multiplied by the RF;  Modification date. As HDFS being mostly a '' write-once-read-many '' filesystem,\nthis date often means creation date;  Modification time. Same as date;  Filename, within the listed folder.",
            "title": "Listing your home folder"
        },
        {
            "location": "/hdfs/#listing-the-tmp-folder",
            "text": "The only change is that you specify an absolute path from the root of the filesystem ( / ): hdfs dfs -ls /tmp",
            "title": "Listing the /tmp folder"
        },
        {
            "location": "/hdfs/#uploading-a-resource",
            "text": "To put a file to HDFS, you have two choices. You can use  hdfs  with the  -put  option or with the  -copyFromLocal  option:  # uploading a file \nhdfs dfs -put localfile.txt /tmp\nhdfs dfs -copyFromLocal localfile.txt /tmp/ # uploading a directory  \nhdfs dfs -put localdir /tmp\nhdfs dfs -copyFromLocal localdir /tmp/  The first arguments after  -copyFromLocal  or  -put  point to local files or folders, while the last argument is a file (if only one file listed as source) or directory in HDFS. Note that you can use wildcards and also rename files and folders when copying, exactly as you would do in a linux shell:  # uploading all files in the current directory with the .txt extension \nhdfs dfs -put *.txt /tmp\nhdfs dfs -copyFromLocal *.txt /tmp/ # uploading a directory and renaming it hdfsdir \nhdfs dfs -put localdir /tmp/hdfsdir\nhdfs dfs -copyFromLocal localdir /tmp/hdfsdir  Both options are doing about the same thing, but  -copyFromLocal  is more explicit when you're uploading a local file and thus preferred.",
            "title": "Uploading a resource"
        },
        {
            "location": "/hdfs/#downloading-a-resource",
            "text": "Download is the same as uploading, but  -put  becomes  -get  and  -copyFromLocal  becomes  -copyToLocal :  hdfs dfs -get /tmp/remotefile.txt .\nhdfs dfs -copyToLocal /tmp/remotefile.txt .",
            "title": "Downloading a resource"
        },
        {
            "location": "/hdfs/#creating-a-folder",
            "text": "To create a folder, use  -mkdir  # create a folder in your hdfs home \nhdfs dfs -mkdir dummy-folder # create a folder in /tmp \nhdfs dfs -mkdir /tmp/dummy-folder  Note that relative paths points to your home folder,   /user/llinder  for instance.",
            "title": "Creating a folder"
        },
        {
            "location": "/hdfs/#removing-resources",
            "text": "To remove individual files, use the  -rm  option:  hdfs dfs -rm /tmp/somefile.txt  To remove a folder, the option is  -rmdir  for an empty directory and  -rm -r  for a non-empty one. The  -r  in  -rm -r  means  recursive : it removes the folder and all its children recursively:  # remove the dummy-folder in your home \nhdfs dfs -rmdir dummy-folder\nrmdir:  '/tmp/lala' : Directory is not empty # oups, the directory is not empty... use -rm -r \nhdfs dfs -rm -r dummy-folder",
            "title": "Removing resources"
        },
        {
            "location": "/hdfs/#advanced-manipulations",
            "text": "the  hdfs dfs  command support several actions that any linux user is already familiar with. Most of their parameters are the same, but note that the collapsing of options ( -rf  instead of  -r -f  for example) are not supported. Here is a non-exhaustive list:   -rm [-r] [-f] : remove a file or directory;  -cp [-r] : copy a file or directory;  -mv : move/rename a file or directory;  -cat : display the content of a file;  -chmod : manipulate file permissions;  -chown : manipulate file ownership;  -tail|-touch| etc.   Other useful commands include:   -moveFromLocal|-moveToLocal : same as  -copyFromLocal|-copyToLocal , but remove the source;  -stat : display information about the specified path;  -count : counts the number of directories, files, and bytes under the paths;  -du : display the size of the specified file, or the sizes of files and directories that are contained in the specified directory;  -dus : display a summary of the file sizes;  -getmerge : concatenate the files in src and writes the result to the specified local destination file. To add a newline character at the end of each file, specify the  addnl  option:  hdfs dfs -getmerge <src> <localdst> [addnl]  -setrep [-R] : change the replication factor for a specified file or directory;",
            "title": "Advanced Manipulations"
        },
        {
            "location": "/mapreduce/",
            "text": "Coming soon",
            "title": "MapReduce"
        },
        {
            "location": "/hive/",
            "text": "Coming soon",
            "title": "Hive"
        },
        {
            "location": "/zeppelin/",
            "text": "Apache Zeppelin is an online notebook that let you interact with the DAPLAB cluster (or any other hadoop/spark installation) through many languages and technology backends. \n\n\nCurrently, Zeppelin on the DAPLAB supports:\n\n\n\n\nSpark 1.6 and Spark 2.1.0 using python, scala or R\n\n\nHive\n\n\nMarkDown\n\n\nShell\n\n\n\n\nGetting Started\n\n\nLogin\n\n\n\n\nInfo\n\n\nIf you are running a dockerized version of zeppelin, you can stay anonymous\nand thus skip this step.\n\n\n\n\n\n\nGo to \nZeppelin\n\n\nLogin using your daplab credentials\n\n\n\n\nCreate a new notebook\n\n\nOn the home page or on the notebook menu, select \"\ncreate new...\n\". Once the notebook is opened, give it a new name.\n\n\n\n\nCreating folders\n\n\nUsing slashes (\n/\n) in the notebook name will automatically create and/or move the notebook into folders.\n\n\n\n\nBasics\n\n\nA notebook is made of \ncells\n, also called \nparagraphs\n. A cell has an \ninterpreter\n that tells Zeppelin which langage/backend to use to run the cell.\n\n\nThe interpreter is configured by writing \n%<interpreter name>\n at the top of the cell. Without it, Zeppelin will use the default interpreter, which you can configure by clicking on \n \n> interpreters\n at the top right of the notebook (drag-drop to re-order them, the first one being the default).\n\n\nYou can run cells by clicking on the \n icon on the right or using \nshift+enter\n. \n\n\nMany useful shortcuts exist in edit mode. Click on \n the at the top right of a notebook to display them.\n\n\nList of interpreter prefixes\n\n\n\n\n\n\n\n\nPrefix\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n%spark\n, \n%spark2\n\n\nSpark with Scala\n\n\n\n\n\n\n%spark\n.\nsql\n, \n%spark2\n.\nsql\n\n\nSpark SQL syntax\n\n\n\n\n\n\n%spark\n.\ndep\n, \n%spark2\n.\ndep\n\n\nLoad dependencies for use within Spark cells\n\n\n\n\n\n\n%spark\n.\npyspark\n, \n%spark2\n.\npyspark\n\n\nSpark with Python\n\n\n\n\n\n\n%md\n\n\nMarkDown cell\n\n\n\n\n\n\n%sh\n\n\nshell scripts\n\n\n\n\n\n\n%jdbc\n(\nhive\n)\n\n\nHive\n\n\n\n\n\n\n\n\nNote\n: \nspark\n is Spark 1.6, \nspark2\n is Spark 2.1.0.\n\n\nBattling with spark\n\n\nLet's use our \nbattling.csv\n example from the \nhive\n and \npig\n  tutorials. \n\n\nInclude Spark CSV\n\n\nFirst, we need an external library to easily read the CSV. On the first cell, enter and run:\n\n\n%dep\n\n\nz\n.\nload\n(\n\"com.databricks:spark-csv_2.11:1.5.0\"\n)\n\n\n\n\n\n%dep\n is used to manage dependencies. Have a look \nhere\n for more information. \n\n\nIf you run into the error:\n\n\nMust be used before SparkInterpreter (%spark) initialized\nHint: put this paragraph before any Spark code and restart Zeppelin/Interpreter\n\n\n\n\nOpen the interpretor's list (\n \n> interpreters\n on the top right) and click on the \n icon on the left of the \nspark2\n interpreter.\n\n\nLoad Data\n\n\n%spark2\n\n\nval\n \nbattingFile\n \n=\n \n\"hdfs:///shared/seanlahman/2011/Batting/Batting.csv\"\n\n\nval\n \nbatting\n \n=\n \nsqlContext\n.\nread\n\n    \n.\nformat\n(\n\"com.databricks.spark.csv\"\n)\n\n    \n.\noption\n(\n\"header\"\n,\n \n\"true\"\n)\n \n// Use first line of all files as header\n\n    \n.\noption\n(\n\"inferSchema\"\n,\n \n\"true\"\n)\n \n// Automatically infer data types\n\n    \n.\nload\n(\nbattingFile\n)\n\n\n\n\n\nVisualizing\n\n\nFirst, have a \nlook at the data:\n\n\n%spark2\n\n\nz\n.\nshow\n(\nbatting\n)\n\n\n\n\n\nNOTE\n : \nz.show\n is a zeppelin builtin that allows you to display values inside a variable. The interface let's you switch between views, such as table, piechart, etc.\n\n\nCompute some statistics per year\n: \n\n\nval statsPerYear = batting\n    .groupBy($\"yearID\")\n    .agg(\n        sum(\"R\").alias(\"total runs\"),\n        sum(\"H\").alias(\"total hits\"), \n        sum(\"G\").alias(\"total games\"))\n\nz.show(statsPerYear)\n\n\n\n\nOn the interface, select the line chart \n or area chart \n and then click on \nsettings\n. Drag-and-drop the statistics into the \nValues\n area:\n\n\n\n\nUse an input form\n to display the hit by pitch per team for a given year:\n\n\n#\n \nz\n.\ninput\n(<\ninput\n \nname\n>,\n \n<\ndefault\n \nvalue\n>)\n\n\nval\n \nyear\n \n=\n \nz\n.\ninput\n(\n\"year\"\n,\n \n1894\n)\n\n\nval\n \nhbp1894\n \n=\n \nbatting\n\n    \n.\nfilter\n(\n$\n\"yearID\"\n \n===\n \nyear\n)\n\n    \n.\ngroupBy\n(\n\"teamID\"\n)\n\n    \n.\nagg\n(\nsum\n(\n\"HBP\"\n)\n\n    \n.\nalias\n(\n\"hit by pitch\"\n))\n\n\nz\n.\nshow\n(\nhbp1894\n)\n\n\n\n\n\nz.input\n creates a simple input text. Use \nz.select\n for a dropdown and \nz.checkbox\n for multiple choices. For example, a dropdown for all teams would be:\n\n\n// get all team names\n\n\nval\n \nall_teams\n \n=\n \nbatting\n.\nselect\n(\n\"teamID\"\n)\n\n    \n.\ndistinct\n()\n\n    \n.\nmap\n(\n_\n.\ngetAs\n[\nString\n](\n0\n))\n\n    \n.\ncollect\n()\n\n\n// create and show a dropdown form\n\n\nval\n \nteam\n \n=\n \nz\n.\nselect\n(\n\"selected team\"\n,\n \nall_teams\n.\nzip\n(\nall_teams\n).\nsorted\n)\n\n\n\n\n\nMore dynamic forms \nin the documentation\n.",
            "title": "Hello Zeppelin"
        },
        {
            "location": "/zeppelin/#getting-started",
            "text": "",
            "title": "Getting Started"
        },
        {
            "location": "/zeppelin/#login",
            "text": "Info  If you are running a dockerized version of zeppelin, you can stay anonymous\nand thus skip this step.    Go to  Zeppelin  Login using your daplab credentials",
            "title": "Login"
        },
        {
            "location": "/zeppelin/#create-a-new-notebook",
            "text": "On the home page or on the notebook menu, select \" create new... \". Once the notebook is opened, give it a new name.   Creating folders  Using slashes ( / ) in the notebook name will automatically create and/or move the notebook into folders.",
            "title": "Create a new notebook"
        },
        {
            "location": "/zeppelin/#basics",
            "text": "A notebook is made of  cells , also called  paragraphs . A cell has an  interpreter  that tells Zeppelin which langage/backend to use to run the cell.  The interpreter is configured by writing  %<interpreter name>  at the top of the cell. Without it, Zeppelin will use the default interpreter, which you can configure by clicking on    > interpreters  at the top right of the notebook (drag-drop to re-order them, the first one being the default).  You can run cells by clicking on the   icon on the right or using  shift+enter .   Many useful shortcuts exist in edit mode. Click on   the at the top right of a notebook to display them.",
            "title": "Basics"
        },
        {
            "location": "/zeppelin/#list-of-interpreter-prefixes",
            "text": "Prefix  Description      %spark ,  %spark2  Spark with Scala    %spark . sql ,  %spark2 . sql  Spark SQL syntax    %spark . dep ,  %spark2 . dep  Load dependencies for use within Spark cells    %spark . pyspark ,  %spark2 . pyspark  Spark with Python    %md  MarkDown cell    %sh  shell scripts    %jdbc ( hive )  Hive     Note :  spark  is Spark 1.6,  spark2  is Spark 2.1.0.",
            "title": "List of interpreter prefixes"
        },
        {
            "location": "/zeppelin/#battling-with-spark",
            "text": "Let's use our  battling.csv  example from the  hive  and  pig   tutorials.",
            "title": "Battling with spark"
        },
        {
            "location": "/zeppelin/#include-spark-csv",
            "text": "First, we need an external library to easily read the CSV. On the first cell, enter and run:  %dep  z . load ( \"com.databricks:spark-csv_2.11:1.5.0\" )   %dep  is used to manage dependencies. Have a look  here  for more information.   If you run into the error:  Must be used before SparkInterpreter (%spark) initialized\nHint: put this paragraph before any Spark code and restart Zeppelin/Interpreter  Open the interpretor's list (   > interpreters  on the top right) and click on the   icon on the left of the  spark2  interpreter.",
            "title": "Include Spark CSV"
        },
        {
            "location": "/zeppelin/#load-data",
            "text": "%spark2  val   battingFile   =   \"hdfs:///shared/seanlahman/2011/Batting/Batting.csv\"  val   batting   =   sqlContext . read \n     . format ( \"com.databricks.spark.csv\" ) \n     . option ( \"header\" ,   \"true\" )   // Use first line of all files as header \n     . option ( \"inferSchema\" ,   \"true\" )   // Automatically infer data types \n     . load ( battingFile )",
            "title": "Load Data"
        },
        {
            "location": "/zeppelin/#visualizing",
            "text": "First, have a  look at the data:  %spark2  z . show ( batting )   NOTE  :  z.show  is a zeppelin builtin that allows you to display values inside a variable. The interface let's you switch between views, such as table, piechart, etc.  Compute some statistics per year :   val statsPerYear = batting\n    .groupBy($\"yearID\")\n    .agg(\n        sum(\"R\").alias(\"total runs\"),\n        sum(\"H\").alias(\"total hits\"), \n        sum(\"G\").alias(\"total games\"))\n\nz.show(statsPerYear)  On the interface, select the line chart   or area chart   and then click on  settings . Drag-and-drop the statistics into the  Values  area:   Use an input form  to display the hit by pitch per team for a given year:  #   z . input (< input   name >,   < default   value >)  val   year   =   z . input ( \"year\" ,   1894 )  val   hbp1894   =   batting \n     . filter ( $ \"yearID\"   ===   year ) \n     . groupBy ( \"teamID\" ) \n     . agg ( sum ( \"HBP\" ) \n     . alias ( \"hit by pitch\" ))  z . show ( hbp1894 )   z.input  creates a simple input text. Use  z.select  for a dropdown and  z.checkbox  for multiple choices. For example, a dropdown for all teams would be:  // get all team names  val   all_teams   =   batting . select ( \"teamID\" ) \n     . distinct () \n     . map ( _ . getAs [ String ]( 0 )) \n     . collect ()  // create and show a dropdown form  val   team   =   z . select ( \"selected team\" ,   all_teams . zip ( all_teams ). sorted )   More dynamic forms  in the documentation .",
            "title": "Visualizing"
        },
        {
            "location": "/spark/",
            "text": "Coming soon",
            "title": "Spark"
        },
        {
            "location": "/lsa/",
            "text": "Coming soon",
            "title": "LSA"
        }
    ]
}